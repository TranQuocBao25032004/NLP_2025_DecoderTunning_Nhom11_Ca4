{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.13"},"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%cd /kaggle/working/","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12,"status":"ok","timestamp":1752231232005,"user":{"displayName":"Gà Lê văn","userId":"04852251220989386905"},"user_tz":-420},"id":"v3wPOa1yGrDX","outputId":"02003492-55cf-4848-a2ff-3f01ed73f6a1","trusted":true,"execution":{"iopub.status.busy":"2025-07-30T05:11:29.218192Z","iopub.execute_input":"2025-07-30T05:11:29.218571Z","iopub.status.idle":"2025-07-30T05:11:29.225822Z","shell.execute_reply.started":"2025-07-30T05:11:29.218550Z","shell.execute_reply":"2025-07-30T05:11:29.225095Z"}},"outputs":[{"name":"stdout","text":"/kaggle/working\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!git clone https://github.com/OpenBMB/DecT.git\n%cd DecT","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2025-07-30T05:11:29.226468Z","iopub.execute_input":"2025-07-30T05:11:29.226640Z","iopub.status.idle":"2025-07-30T05:11:29.974120Z","shell.execute_reply.started":"2025-07-30T05:11:29.226624Z","shell.execute_reply":"2025-07-30T05:11:29.973330Z"},"executionInfo":{"elapsed":1218,"status":"ok","timestamp":1752231233225,"user":{"displayName":"Gà Lê văn","userId":"04852251220989386905"},"user_tz":-420},"id":"oJY9c2Vnga7h","outputId":"59faf821-f504-49f1-fe4d-7ffe8dab8a15","trusted":true},"outputs":[{"name":"stdout","text":"Cloning into 'DecT'...\nremote: Enumerating objects: 109, done.\u001b[K\nremote: Counting objects: 100% (109/109), done.\u001b[K\nremote: Compressing objects: 100% (76/76), done.\u001b[K\nremote: Total 109 (delta 32), reused 102 (delta 27), pack-reused 0 (from 0)\u001b[K\nReceiving objects: 100% (109/109), 29.66 KiB | 5.93 MiB/s, done.\nResolving deltas: 100% (32/32), done.\n/kaggle/working/DecT\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"!pip install -r requirements.txt","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2025-07-30T05:11:29.976229Z","iopub.execute_input":"2025-07-30T05:11:29.976444Z","iopub.status.idle":"2025-07-30T05:13:23.831826Z","shell.execute_reply.started":"2025-07-30T05:11:29.976424Z","shell.execute_reply":"2025-07-30T05:13:23.830794Z"},"executionInfo":{"elapsed":176938,"status":"ok","timestamp":1752231410165,"user":{"displayName":"Gà Lê văn","userId":"04852251220989386905"},"user_tz":-420},"id":"ZSbGBhnWgcJx","outputId":"e52962a8-8f8d-4d7d-a765-78a51c4220a2","trusted":true},"outputs":[{"name":"stdout","text":"Collecting openprompt (from -r requirements.txt (line 1))\n  Downloading openprompt-1.0.1-py3-none-any.whl.metadata (10 kB)\nRequirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 2)) (2.6.0+cu124)\nRequirement already satisfied: transformers>=4.27.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 3)) (4.52.4)\nCollecting sentencepiece==0.1.96 (from openprompt->-r requirements.txt (line 1))\n  Downloading sentencepiece-0.1.96.tar.gz (508 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m508.6/508.6 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: tqdm>=4.62.2 in /usr/local/lib/python3.11/dist-packages (from openprompt->-r requirements.txt (line 1)) (4.67.1)\nCollecting tensorboardX (from openprompt->-r requirements.txt (line 1))\n  Downloading tensorboardx-2.6.4-py3-none-any.whl.metadata (6.2 kB)\nRequirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from openprompt->-r requirements.txt (line 1)) (3.9.1)\nCollecting yacs (from openprompt->-r requirements.txt (line 1))\n  Downloading yacs-0.1.8-py3-none-any.whl.metadata (639 bytes)\nRequirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from openprompt->-r requirements.txt (line 1)) (0.3.8)\nRequirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (from openprompt->-r requirements.txt (line 1)) (3.6.0)\nCollecting rouge==1.0.0 (from openprompt->-r requirements.txt (line 1))\n  Downloading rouge-1.0.0-py3-none-any.whl.metadata (4.0 kB)\nRequirement already satisfied: pyarrow in /usr/local/lib/python3.11/dist-packages (from openprompt->-r requirements.txt (line 1)) (19.0.1)\nRequirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from openprompt->-r requirements.txt (line 1)) (1.15.3)\nRequirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from rouge==1.0.0->openprompt->-r requirements.txt (line 1)) (1.17.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 2)) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 2)) (4.14.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 2)) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 2)) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 2)) (2025.5.1)\nCollecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch->-r requirements.txt (line 2))\n  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.4.127 (from torch->-r requirements.txt (line 2))\n  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.4.127 (from torch->-r requirements.txt (line 2))\n  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch->-r requirements.txt (line 2))\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch->-r requirements.txt (line 2))\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch->-r requirements.txt (line 2))\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch->-r requirements.txt (line 2))\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch->-r requirements.txt (line 2))\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch->-r requirements.txt (line 2))\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 2)) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 2)) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 2)) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch->-r requirements.txt (line 2))\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 2)) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 2)) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->-r requirements.txt (line 2)) (1.3.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.27.1->-r requirements.txt (line 3)) (0.33.1)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.27.1->-r requirements.txt (line 3)) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.27.1->-r requirements.txt (line 3)) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.27.1->-r requirements.txt (line 3)) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.27.1->-r requirements.txt (line 3)) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers>=4.27.1->-r requirements.txt (line 3)) (2.32.4)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.27.1->-r requirements.txt (line 3)) (0.21.2)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.27.1->-r requirements.txt (line 3)) (0.5.3)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers>=4.27.1->-r requirements.txt (line 3)) (1.1.5)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers>=4.27.1->-r requirements.txt (line 3)) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers>=4.27.1->-r requirements.txt (line 3)) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers>=4.27.1->-r requirements.txt (line 3)) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers>=4.27.1->-r requirements.txt (line 3)) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers>=4.27.1->-r requirements.txt (line 3)) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers>=4.27.1->-r requirements.txt (line 3)) (2.4.1)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets->openprompt->-r requirements.txt (line 1)) (2.2.3)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets->openprompt->-r requirements.txt (line 1)) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets->openprompt->-r requirements.txt (line 1)) (0.70.16)\nCollecting fsspec (from torch->-r requirements.txt (line 2))\n  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.27.1->-r requirements.txt (line 3)) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.27.1->-r requirements.txt (line 3)) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.27.1->-r requirements.txt (line 3)) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.27.1->-r requirements.txt (line 3)) (2025.6.15)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->-r requirements.txt (line 2)) (3.0.2)\nRequirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->openprompt->-r requirements.txt (line 1)) (8.2.1)\nRequirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->openprompt->-r requirements.txt (line 1)) (1.5.1)\nRequirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.11/dist-packages (from tensorboardX->openprompt->-r requirements.txt (line 1)) (3.20.3)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets->openprompt->-r requirements.txt (line 1)) (3.12.13)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers>=4.27.1->-r requirements.txt (line 3)) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers>=4.27.1->-r requirements.txt (line 3)) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers>=4.27.1->-r requirements.txt (line 3)) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers>=4.27.1->-r requirements.txt (line 3)) (2024.2.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets->openprompt->-r requirements.txt (line 1)) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets->openprompt->-r requirements.txt (line 1)) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets->openprompt->-r requirements.txt (line 1)) (2025.2)\nRequirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->openprompt->-r requirements.txt (line 1)) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->openprompt->-r requirements.txt (line 1)) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->openprompt->-r requirements.txt (line 1)) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->openprompt->-r requirements.txt (line 1)) (1.7.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->openprompt->-r requirements.txt (line 1)) (6.6.3)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->openprompt->-r requirements.txt (line 1)) (0.3.2)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->openprompt->-r requirements.txt (line 1)) (1.20.1)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers>=4.27.1->-r requirements.txt (line 3)) (2024.2.0)\nDownloading openprompt-1.0.1-py3-none-any.whl (146 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m146.4/146.4 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading rouge-1.0.0-py3-none-any.whl (14 kB)\nDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m100.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m82.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m44.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m31.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m68.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading tensorboardx-2.6.4-py3-none-any.whl (87 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.2/87.2 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading yacs-0.1.8-py3-none-any.whl (14 kB)\nBuilding wheels for collected packages: sentencepiece\n  Building wheel for sentencepiece (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for sentencepiece: filename=sentencepiece-0.1.96-cp311-cp311-linux_x86_64.whl size=1545731 sha256=829b4f9e4eb8feba54e909bd292b878c39a8c4bb43881f5b7697e96a06d1605a\n  Stored in directory: /root/.cache/pip/wheels/47/db/8c/025aab435b87512164c5771f0f9acb8d6b719e2214db801e66\nSuccessfully built sentencepiece\nInstalling collected packages: sentencepiece, yacs, rouge, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, fsspec, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, tensorboardX, openprompt\n  Attempting uninstall: sentencepiece\n    Found existing installation: sentencepiece 0.2.0\n    Uninstalling sentencepiece-0.2.0:\n      Successfully uninstalled sentencepiece-0.2.0\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.6.82\n    Uninstalling nvidia-curand-cu12-10.3.6.82:\n      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n  Attempting uninstall: nvidia-cuda-runtime-cu12\n    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-cupti-cu12\n    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n  Attempting uninstall: fsspec\n    Found existing installation: fsspec 2025.5.1\n    Uninstalling fsspec-2025.5.1:\n      Successfully uninstalled fsspec-2025.5.1\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.8.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\ngcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\nbigframes 2.8.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\nbigframes 2.8.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed fsspec-2025.3.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 openprompt-1.0.1 rouge-1.0.0 sentencepiece-0.1.96 tensorboardX-2.6.4 yacs-0.1.8\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"!pip install datasets","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2025-07-30T05:13:23.832884Z","iopub.execute_input":"2025-07-30T05:13:23.833193Z","iopub.status.idle":"2025-07-30T05:13:27.139312Z","shell.execute_reply.started":"2025-07-30T05:13:23.833157Z","shell.execute_reply":"2025-07-30T05:13:27.138597Z"},"executionInfo":{"elapsed":8423,"status":"ok","timestamp":1752231418595,"user":{"displayName":"Gà Lê văn","userId":"04852251220989386905"},"user_tz":-420},"id":"fJihp21Ne1fQ","outputId":"322eb6ab-db45-4e2f-febc-89393798e3bf","trusted":true},"outputs":[{"name":"stdout","text":"Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.6.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.4)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (19.0.1)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.3)\nRequirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.4)\nRequirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\nRequirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.33.1)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.13)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.14.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.1.5)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (2.4.1)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.6.15)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.3)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->datasets) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->datasets) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->datasets) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->datasets) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->datasets) (2024.2.0)\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"!cat /kaggle/working/DecT/datasets/download_glue_data.py | grep TASKS -A 10","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2025-07-30T05:13:27.140408Z","iopub.execute_input":"2025-07-30T05:13:27.140693Z","iopub.status.idle":"2025-07-30T05:13:27.496954Z","shell.execute_reply.started":"2025-07-30T05:13:27.140656Z","shell.execute_reply":"2025-07-30T05:13:27.496186Z"},"executionInfo":{"elapsed":141,"status":"ok","timestamp":1752231418737,"user":{"displayName":"Gà Lê văn","userId":"04852251220989386905"},"user_tz":-420},"id":"y-wJQ31_heeO","outputId":"0d3b817a-c4ed-4d1f-f8c7-3505e6a2573e","trusted":true},"outputs":[{"name":"stdout","text":"TASKS = [\"CoLA\", \"SST\", \"MRPC\", \"QQP\", \"STS\", \"MNLI\", \"QNLI\", \"RTE\", \"WNLI\", \"diagnostic\"]\nTASK2PATH = {\"CoLA\":'https://dl.fbaipublicfiles.com/glue/data/CoLA.zip',\n             \"SST\":'https://dl.fbaipublicfiles.com/glue/data/SST-2.zip',\n             \"QQP\":'https://dl.fbaipublicfiles.com/glue/data/STS-B.zip',\n             \"STS\":'https://dl.fbaipublicfiles.com/glue/data/QQP-clean.zip',\n             \"MNLI\":'https://dl.fbaipublicfiles.com/glue/data/MNLI.zip',\n             \"QNLI\":'https://dl.fbaipublicfiles.com/glue/data/QNLIv2.zip',\n             \"RTE\":'https://dl.fbaipublicfiles.com/glue/data/RTE.zip',\n             \"WNLI\":'https://dl.fbaipublicfiles.com/glue/data/WNLI.zip',\n             \"diagnostic\":'https://dl.fbaipublicfiles.com/glue/data/AX.tsv'}\n\n--\n        tasks = TASKS\n    else:\n        tasks = []\n        for task_name in task_names:\n            assert task_name in TASKS, \"Task %s not found!\" % task_name\n            tasks.append(task_name)\n    return tasks\n\ndef main(arguments):\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--data_dir', help='directory to save data to', type=str, default='glue_data')\n    parser.add_argument('--tasks', help='tasks to download data for as a comma separated string',\n                        type=str, default='all')\n    parser.add_argument('--path_to_mrpc', help='path to directory containing extracted MRPC data, msr_paraphrase_train.txt and msr_paraphrase_text.txt',\n                        type=str, default='')\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"!pip install transformers==4.31.0","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2025-07-30T05:13:27.498035Z","iopub.execute_input":"2025-07-30T05:13:27.498318Z","iopub.status.idle":"2025-07-30T05:13:38.493832Z","shell.execute_reply.started":"2025-07-30T05:13:27.498291Z","shell.execute_reply":"2025-07-30T05:13:38.492912Z"},"executionInfo":{"elapsed":12604,"status":"ok","timestamp":1752231431340,"user":{"displayName":"Gà Lê văn","userId":"04852251220989386905"},"user_tz":-420},"id":"LBjUxQwZh1h5","outputId":"4cc30009-2cfe-4ddf-8667-5618856e4b1c","trusted":true},"outputs":[{"name":"stdout","text":"Collecting transformers==4.31.0\n  Downloading transformers-4.31.0-py3-none-any.whl.metadata (116 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.9/116.9 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.31.0) (3.18.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.31.0) (0.33.1)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.31.0) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.31.0) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.31.0) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.31.0) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.31.0) (2.32.4)\nCollecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers==4.31.0)\n  Downloading tokenizers-0.13.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\nRequirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.31.0) (0.5.3)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.31.0) (4.67.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.31.0) (2025.3.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.31.0) (4.14.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.31.0) (1.1.5)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.31.0) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.31.0) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.31.0) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.31.0) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.31.0) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.31.0) (2.4.1)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.31.0) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.31.0) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.31.0) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.31.0) (2025.6.15)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers==4.31.0) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers==4.31.0) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers==4.31.0) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers==4.31.0) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers==4.31.0) (2024.2.0)\nDownloading transformers-4.31.0-py3-none-any.whl (7.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m75.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading tokenizers-0.13.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m72.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: tokenizers, transformers\n  Attempting uninstall: tokenizers\n    Found existing installation: tokenizers 0.21.2\n    Uninstalling tokenizers-0.21.2:\n      Successfully uninstalled tokenizers-0.21.2\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.52.4\n    Uninstalling transformers-4.52.4:\n      Successfully uninstalled transformers-4.52.4\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nkaggle-environments 1.17.6 requires transformers>=4.33.1, but you have transformers 4.31.0 which is incompatible.\nsentence-transformers 4.1.0 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.31.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed tokenizers-0.13.3 transformers-4.31.0\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"!pip install sentencepiece accelerate","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4393,"status":"ok","timestamp":1752231435736,"user":{"displayName":"Gà Lê văn","userId":"04852251220989386905"},"user_tz":-420},"id":"T3LNWlAXiOGU","outputId":"a7b33a9f-81e9-4d54-cc14-4a81415c02b6","trusted":true,"execution":{"iopub.status.busy":"2025-07-30T05:13:38.498689Z","iopub.execute_input":"2025-07-30T05:13:38.498904Z","iopub.status.idle":"2025-07-30T05:13:41.660374Z","shell.execute_reply.started":"2025-07-30T05:13:38.498881Z","shell.execute_reply":"2025-07-30T05:13:41.659637Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (0.1.96)\nRequirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.8.1)\nRequirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.11/dist-packages (from accelerate) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (25.0)\nRequirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (7.0.0)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from accelerate) (6.0.2)\nRequirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.6.0+cu124)\nRequirement already satisfied: huggingface_hub>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (0.33.1)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate) (0.5.3)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.21.0->accelerate) (3.18.0)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.21.0->accelerate) (2025.3.0)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.21.0->accelerate) (2.32.4)\nRequirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.21.0->accelerate) (4.67.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.21.0->accelerate) (4.14.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.21.0->accelerate) (1.1.5)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy<3.0.0,>=1.17->accelerate) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy<3.0.0,>=1.17->accelerate) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy<3.0.0,>=1.17->accelerate) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy<3.0.0,>=1.17->accelerate) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy<3.0.0,>=1.17->accelerate) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy<3.0.0,>=1.17->accelerate) (2.4.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.3.1.170)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3.0.0,>=1.17->accelerate) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3.0.0,>=1.17->accelerate) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy<3.0.0,>=1.17->accelerate) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy<3.0.0,>=1.17->accelerate) (2024.2.0)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2025.6.15)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy<3.0.0,>=1.17->accelerate) (2024.2.0)\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"!mkdir -p datasets/agnews datasets/dbpedia datasets/fewnerd datasets/imdb datasets/sst2 datasets/yahoo datasets/yelp","metadata":{"executionInfo":{"elapsed":144,"status":"ok","timestamp":1752231435879,"user":{"displayName":"Gà Lê văn","userId":"04852251220989386905"},"user_tz":-420},"id":"6tIC2RqBGrDf","trusted":true,"execution":{"iopub.status.busy":"2025-07-30T05:13:41.661318Z","iopub.execute_input":"2025-07-30T05:13:41.661583Z","iopub.status.idle":"2025-07-30T05:13:41.779181Z","shell.execute_reply.started":"2025-07-30T05:13:41.661550Z","shell.execute_reply":"2025-07-30T05:13:41.778220Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"import pandas as pd\nimport os\nimport json\nimport csv\nfrom abc import ABC, abstractmethod\nfrom collections import defaultdict, Counter\nfrom typing import *\nimport pandas as pd\nfrom openprompt.utils.logging import logger\nfrom openprompt.data_utils.utils import InputExample\nfrom openprompt.data_utils.data_processor import DataProcessor","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15693,"status":"ok","timestamp":1752231451569,"user":{"displayName":"Gà Lê văn","userId":"04852251220989386905"},"user_tz":-420},"id":"pOrDNhNiGrDf","outputId":"48ac6ad9-77f8-4d5b-f4da-478bceb7e2a6","trusted":true,"execution":{"iopub.status.busy":"2025-07-30T05:13:41.780642Z","iopub.execute_input":"2025-07-30T05:13:41.780933Z","iopub.status.idle":"2025-07-30T05:14:24.205105Z","shell.execute_reply.started":"2025-07-30T05:13:41.780909Z","shell.execute_reply":"2025-07-30T05:14:24.204459Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.\n  warnings.warn(\n2025-07-30 05:13:53.404301: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1753852433.619127      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1753852433.680532      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"# Tải các dataset","metadata":{"id":"JsX5kYol3Obh"}},{"cell_type":"markdown","source":"## AGNews(chạy được)","metadata":{"id":"WnkJfq5nz7mk"}},{"cell_type":"code","source":"%cd /kaggle/working/DecT/datasets/agnews\n!wget -q -O ag_news_csv.tar.gz https://s3.amazonaws.com/fast-ai-nlp/ag_news_csv.tgz\n!tar -xzf ag_news_csv.tar.gz --strip-components=1\n!rm -r /kaggle/working/DecT/datasets/agnews/ag_news_csv.tar.gz","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12953,"status":"ok","timestamp":1752231464528,"user":{"displayName":"Gà Lê văn","userId":"04852251220989386905"},"user_tz":-420},"id":"k1-t6U7wxxAz","outputId":"91433e75-bfa1-4da0-e439-2f9900d6e024","trusted":true,"execution":{"iopub.status.busy":"2025-07-30T05:14:24.205866Z","iopub.execute_input":"2025-07-30T05:14:24.206482Z","iopub.status.idle":"2025-07-30T05:14:25.937418Z","shell.execute_reply.started":"2025-07-30T05:14:24.206452Z","shell.execute_reply":"2025-07-30T05:14:25.936581Z"}},"outputs":[{"name":"stdout","text":"/kaggle/working/DecT/datasets/agnews\n","output_type":"stream"}],"execution_count":10},{"cell_type":"markdown","source":"## DBpedia(không chạy được)","metadata":{"id":"XqbC1cTzz9o5"}},{"cell_type":"code","source":"!mkdir -p /kaggle/working/DecT/datasets/dbpedia_raw\n%cd /kaggle/working/DecT/datasets/dbpedia_raw\n\n# Tải dữ liệu zip từ FastText (chuẩn DBpedia version)\n!wget https://data.deepai.org/dbpedia.zip\n\n# Giải nén\n!unzip dbpedia.zip","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":37937,"status":"ok","timestamp":1752231502468,"user":{"displayName":"Gà Lê văn","userId":"04852251220989386905"},"user_tz":-420},"id":"ihoIEwylsYyJ","outputId":"698ac9dc-8089-49ed-eae7-fa5ad2acb7a3","trusted":true,"execution":{"iopub.status.busy":"2025-07-30T05:14:25.938422Z","iopub.execute_input":"2025-07-30T05:14:25.938636Z","iopub.status.idle":"2025-07-30T05:15:04.267287Z","shell.execute_reply.started":"2025-07-30T05:14:25.938615Z","shell.execute_reply":"2025-07-30T05:15:04.266547Z"}},"outputs":[{"name":"stdout","text":"/kaggle/working/DecT/datasets/dbpedia_raw\n--2025-07-30 05:14:26--  https://data.deepai.org/dbpedia.zip\nResolving data.deepai.org (data.deepai.org)... 143.244.50.214, 2400:52e0:1a01::1001:1\nConnecting to data.deepai.org (data.deepai.org)|143.244.50.214|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 174050111 (166M) [application/zip]\nSaving to: ‘dbpedia.zip’\n\ndbpedia.zip         100%[===================>] 165.99M  5.11MB/s    in 33s     \n\n2025-07-30 05:14:59 (4.97 MB/s) - ‘dbpedia.zip’ saved [174050111/174050111]\n\nArchive:  dbpedia.zip\n  inflating: DBPEDIA_test.csv        \n  inflating: DBPEDIA_train.csv       \n  inflating: DBPEDIA_val.csv         \n  inflating: DBP_wiki_data.csv       \n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"# Đọc file để biết các nhãn duy nhất trong l1\n%cd /kaggle/working/DecT/datasets\ndf_train = pd.read_csv(\"dbpedia_raw/DBPEDIA_train.csv\")\nunique_labels = sorted(df_train['l1'].dropna().unique())\nlabel2id = {label: idx for idx, label in enumerate(unique_labels)}\n\nprint(\"Số class:\", len(label2id))\nprint(\"Label mapping (ví dụ):\", list(label2id.items())[:5])\n\n# Tạo thư mục đích\noutput_dir = \"dbpedia\"\nos.makedirs(output_dir, exist_ok=True)\n\ndef convert_with_label_map(df, split_name):\n    text_path = f\"{output_dir}/{split_name}.txt\"\n    label_path = f\"{output_dir}/{split_name}_labels.txt\"\n\n    with open(text_path, \"w\", encoding=\"utf-8\") as f_txt, open(label_path, \"w\", encoding=\"utf-8\") as f_lbl:\n        for row in df.itertuples():\n            text = getattr(row, \"text\").strip().replace(\"\\t\", \" \")\n            label_str = getattr(row, \"l1\")\n            if pd.isna(label_str) or label_str not in label2id:\n                continue\n            label_id = label2id[label_str]\n            f_txt.write(text + \"\\n\")\n            f_lbl.write(str(label_id) + \"\\n\")\n\n# Convert train và test\nconvert_with_label_map(df_train, \"train\")\n\ndf_test = pd.read_csv(\"dbpedia_raw/DBPEDIA_test.csv\")\nconvert_with_label_map(df_test, \"test\")\n\n!rm -r dbpedia_raw","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4497,"status":"ok","timestamp":1752231509892,"user":{"displayName":"Gà Lê văn","userId":"04852251220989386905"},"user_tz":-420},"id":"MPjTJ8Vwx9u_","outputId":"421ec7e8-7aaf-4f58-db5b-707bfb9d47c6","trusted":true,"execution":{"iopub.status.busy":"2025-07-30T05:15:04.268474Z","iopub.execute_input":"2025-07-30T05:15:04.268832Z","iopub.status.idle":"2025-07-30T05:15:08.032166Z","shell.execute_reply.started":"2025-07-30T05:15:04.268795Z","shell.execute_reply":"2025-07-30T05:15:08.031082Z"}},"outputs":[{"name":"stdout","text":"/kaggle/working/DecT/datasets\nSố class: 9\nLabel mapping (ví dụ): [('Agent', 0), ('Device', 1), ('Event', 2), ('Place', 3), ('Species', 4)]\n","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":"## FewNERD(chạy được)","metadata":{"id":"yaUvHjyGGrDl"}},{"cell_type":"code","source":"!mkdir -p /kaggle/working/DecT/datasets/fewnerd_raw\n%cd /kaggle/working/DecT/datasets/fewnerd_raw\n\n# # Tải supervised.zip từ Hugging Face repo (LFS hỗ trợ)\n!wget https://huggingface.co/datasets/DFKI-SLT/few-nerd/resolve/main/data/supervised.zip\n\n# # Giải nén\n!unzip -q supervised.zip","metadata":{"execution":{"iopub.status.busy":"2025-07-30T05:15:08.033493Z","iopub.execute_input":"2025-07-30T05:15:08.033830Z","iopub.status.idle":"2025-07-30T05:15:09.245276Z","shell.execute_reply.started":"2025-07-30T05:15:08.033795Z","shell.execute_reply":"2025-07-30T05:15:09.244280Z"},"executionInfo":{"elapsed":272998,"status":"aborted","timestamp":1752231504859,"user":{"displayName":"Gà Lê văn","userId":"04852251220989386905"},"user_tz":-420},"id":"L2nTZMA-GrDl","trusted":true},"outputs":[{"name":"stdout","text":"/kaggle/working/DecT/datasets/fewnerd_raw\n--2025-07-30 05:15:08--  https://huggingface.co/datasets/DFKI-SLT/few-nerd/resolve/main/data/supervised.zip\nResolving huggingface.co (huggingface.co)... 3.165.160.59, 3.165.160.11, 3.165.160.61, ...\nConnecting to huggingface.co (huggingface.co)|3.165.160.59|:443... connected.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://cdn-lfs.hf.co/datasets/dfki-nlp/few-nerd/6b38b77701bd50b7615c299988180595c02cca4b4060777fd399039b6336f8ad?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27supervised.zip%3B+filename%3D%22supervised.zip%22%3B&response-content-type=application%2Fzip&Expires=1753856108&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc1Mzg1NjEwOH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5oZi5jby9kYXRhc2V0cy9kZmtpLW5scC9mZXctbmVyZC82YjM4Yjc3NzAxYmQ1MGI3NjE1YzI5OTk4ODE4MDU5NWMwMmNjYTRiNDA2MDc3N2ZkMzk5MDM5YjYzMzZmOGFkP3Jlc3BvbnNlLWNvbnRlbnQtZGlzcG9zaXRpb249KiZyZXNwb25zZS1jb250ZW50LXR5cGU9KiJ9XX0_&Signature=BjiJImM8af4u2Rlfa8QbmdUxjYoYwqRJs%7EVI8eAysajNt4c7atDQnU2%7En5L-pYjchaXFu5esCqakcaupdwVbm%7Eg9ol1n7s0OFXfaljRecggiDzFQkPu3FsLxbUF3SkT8sawI7V8iutLzjSmMqmujEXFw9gWX%7EBPESTfFXt9mlr9zFHKnh-NlJbx%7Em-xrnm1KcZvYpRJ%7EKQdfmn2ERnkMwGVjSnm4svIERGTNs6Boq63XpGSH6V8cDPEuUrrumKPXKgo3UykhtyRY9h0s1vSMC4aKqkP-E0FhrastBZDnrUGSAa92b8oRSirlgMx7KgNYy%7Enwr-nB6uYoPNd%7E-wlOqg__&Key-Pair-Id=K3RPWS32NSSJCE [following]\n--2025-07-30 05:15:08--  https://cdn-lfs.hf.co/datasets/dfki-nlp/few-nerd/6b38b77701bd50b7615c299988180595c02cca4b4060777fd399039b6336f8ad?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27supervised.zip%3B+filename%3D%22supervised.zip%22%3B&response-content-type=application%2Fzip&Expires=1753856108&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc1Mzg1NjEwOH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5oZi5jby9kYXRhc2V0cy9kZmtpLW5scC9mZXctbmVyZC82YjM4Yjc3NzAxYmQ1MGI3NjE1YzI5OTk4ODE4MDU5NWMwMmNjYTRiNDA2MDc3N2ZkMzk5MDM5YjYzMzZmOGFkP3Jlc3BvbnNlLWNvbnRlbnQtZGlzcG9zaXRpb249KiZyZXNwb25zZS1jb250ZW50LXR5cGU9KiJ9XX0_&Signature=BjiJImM8af4u2Rlfa8QbmdUxjYoYwqRJs%7EVI8eAysajNt4c7atDQnU2%7En5L-pYjchaXFu5esCqakcaupdwVbm%7Eg9ol1n7s0OFXfaljRecggiDzFQkPu3FsLxbUF3SkT8sawI7V8iutLzjSmMqmujEXFw9gWX%7EBPESTfFXt9mlr9zFHKnh-NlJbx%7Em-xrnm1KcZvYpRJ%7EKQdfmn2ERnkMwGVjSnm4svIERGTNs6Boq63XpGSH6V8cDPEuUrrumKPXKgo3UykhtyRY9h0s1vSMC4aKqkP-E0FhrastBZDnrUGSAa92b8oRSirlgMx7KgNYy%7Enwr-nB6uYoPNd%7E-wlOqg__&Key-Pair-Id=K3RPWS32NSSJCE\nResolving cdn-lfs.hf.co (cdn-lfs.hf.co)... 18.238.217.120, 18.238.217.113, 18.238.217.81, ...\nConnecting to cdn-lfs.hf.co (cdn-lfs.hf.co)|18.238.217.120|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 14577157 (14M) [application/zip]\nSaving to: ‘supervised.zip’\n\nsupervised.zip      100%[===================>]  13.90M  --.-KB/s    in 0.1s    \n\n2025-07-30 05:15:08 (104 MB/s) - ‘supervised.zip’ saved [14577157/14577157]\n\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"%cd /kaggle/working/DecT/datasets\n!mkdir -p fewnerd/supervised\n!cp fewnerd_raw/supervised/train.txt fewnerd/supervised/train.txt\n!cp fewnerd_raw/supervised/dev.txt   fewnerd/supervised/val.txt\n!cp fewnerd_raw/supervised/test.txt  fewnerd/supervised/test.txt\n!rm -r fewnerd_raw","metadata":{"execution":{"iopub.status.busy":"2025-07-30T05:15:09.246426Z","iopub.execute_input":"2025-07-30T05:15:09.246743Z","iopub.status.idle":"2025-07-30T05:15:10.075114Z","shell.execute_reply.started":"2025-07-30T05:15:09.246704Z","shell.execute_reply":"2025-07-30T05:15:10.074097Z"},"executionInfo":{"elapsed":273000,"status":"aborted","timestamp":1752231504864,"user":{"displayName":"Gà Lê văn","userId":"04852251220989386905"},"user_tz":-420},"id":"4VNGbR8GGrDn","trusted":true},"outputs":[{"name":"stdout","text":"/kaggle/working/DecT/datasets\n","output_type":"stream"}],"execution_count":14},{"cell_type":"markdown","source":"## IMDB(chạy được)","metadata":{"id":"aXKWrE6z0A4J"}},{"cell_type":"code","source":"# Tạo thư mục chứa IMDB\n!mkdir -p /kaggle/working/DecT/datasets/imdb_raw\n%cd /kaggle/working/DecT/datasets/imdb_raw\n\n# Tải dữ liệu từ Stanford\n!wget https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n\n# Giải nén\n!tar -xzf aclImdb_v1.tar.gz","metadata":{"execution":{"iopub.status.busy":"2025-07-30T05:15:10.076425Z","iopub.execute_input":"2025-07-30T05:15:10.076654Z","iopub.status.idle":"2025-07-30T05:15:17.110300Z","shell.execute_reply.started":"2025-07-30T05:15:10.076633Z","shell.execute_reply":"2025-07-30T05:15:17.109291Z"},"executionInfo":{"elapsed":273009,"status":"aborted","timestamp":1752231504876,"user":{"displayName":"Gà Lê văn","userId":"04852251220989386905"},"user_tz":-420},"id":"HNavzv-kFkvA","trusted":true},"outputs":[{"name":"stdout","text":"/kaggle/working/DecT/datasets/imdb_raw\n--2025-07-30 05:15:10--  https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\nResolving ai.stanford.edu (ai.stanford.edu)... 171.64.68.10\nConnecting to ai.stanford.edu (ai.stanford.edu)|171.64.68.10|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 84125825 (80M) [application/x-gzip]\nSaving to: ‘aclImdb_v1.tar.gz’\n\naclImdb_v1.tar.gz   100%[===================>]  80.23M  46.1MB/s    in 1.7s    \n\n2025-07-30 05:15:12 (46.1 MB/s) - ‘aclImdb_v1.tar.gz’ saved [84125825/84125825]\n\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"def extract_imdb_split(src_dir, label_value):\n    texts = []\n    labels = []\n    for filename in os.listdir(src_dir):\n        filepath = os.path.join(src_dir, filename)\n        if os.path.isfile(filepath):\n            with open(filepath, encoding='utf-8') as f:\n                texts.append(f.read().replace('\\n', ' '))\n                labels.append(str(label_value))\n    return texts, labels\n\n%cd /kaggle/working/DecT/datasets\n\n# Train data\npos_texts, pos_labels = extract_imdb_split(\"imdb_raw/aclImdb/train/pos\", 1)\nneg_texts, neg_labels = extract_imdb_split(\"imdb_raw/aclImdb/train/neg\", 0)\ntrain_texts = pos_texts + neg_texts\ntrain_labels = pos_labels + neg_labels\n\nwith open(\"imdb/train.txt\", \"w\", encoding=\"utf-8\") as f:\n    f.write(\"\\n\".join(train_texts))\nwith open(\"imdb/train_labels.txt\", \"w\", encoding=\"utf-8\") as f:\n    f.write(\"\\n\".join(train_labels))\n\n# Test data\npos_texts, pos_labels = extract_imdb_split(\"imdb_raw/aclImdb/test/pos\", 1)\nneg_texts, neg_labels = extract_imdb_split(\"imdb_raw/aclImdb/test/neg\", 0)\ntest_texts = pos_texts + neg_texts\ntest_labels = pos_labels + neg_labels\n\nwith open(\"imdb/test.txt\", \"w\", encoding=\"utf-8\") as f:\n    f.write(\"\\n\".join(test_texts))\nwith open(\"imdb/test_labels.txt\", \"w\", encoding=\"utf-8\") as f:\n    f.write(\"\\n\".join(test_labels))\n\n!rm -r imdb_raw","metadata":{"execution":{"iopub.status.busy":"2025-07-30T05:15:17.111625Z","iopub.execute_input":"2025-07-30T05:15:17.111955Z","iopub.status.idle":"2025-07-30T05:15:20.689923Z","shell.execute_reply.started":"2025-07-30T05:15:17.111921Z","shell.execute_reply":"2025-07-30T05:15:20.689066Z"},"executionInfo":{"elapsed":273010,"status":"aborted","timestamp":1752231504879,"user":{"displayName":"Gà Lê văn","userId":"04852251220989386905"},"user_tz":-420},"id":"JWibEczlIFr0","trusted":true},"outputs":[{"name":"stdout","text":"/kaggle/working/DecT/datasets\n","output_type":"stream"}],"execution_count":16},{"cell_type":"markdown","source":"## MNLI(chạy được)","metadata":{"id":"T0rqx1vd0JBh"}},{"cell_type":"code","source":"!mkdir -p /kaggle/working/DecT/datasets/mnli_temp\n%cd /kaggle/working/DecT/datasets/mnli_temp\n!wget https://nlp.stanford.edu/projects/snli/snli_1.0.zip\n!wget https://cims.nyu.edu/~sbowman/multinli/multinli_1.0.zip\n!unzip -q -o snli_1.0.zip\n!unzip -q -o multinli_1.0.zip","metadata":{"execution":{"iopub.status.busy":"2025-07-30T05:15:20.691001Z","iopub.execute_input":"2025-07-30T05:15:20.691302Z","iopub.status.idle":"2025-07-30T05:15:45.222122Z","shell.execute_reply.started":"2025-07-30T05:15:20.691265Z","shell.execute_reply":"2025-07-30T05:15:45.221205Z"},"executionInfo":{"elapsed":273009,"status":"aborted","timestamp":1752231504881,"user":{"displayName":"Gà Lê văn","userId":"04852251220989386905"},"user_tz":-420},"id":"lqPCkvsD7utP","trusted":true},"outputs":[{"name":"stdout","text":"/kaggle/working/DecT/datasets/mnli_temp\n--2025-07-30 05:15:20--  https://nlp.stanford.edu/projects/snli/snli_1.0.zip\nResolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\nConnecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 94550081 (90M) [application/zip]\nSaving to: ‘snli_1.0.zip’\n\nsnli_1.0.zip        100%[===================>]  90.17M  13.6MB/s    in 6.7s    \n\n2025-07-30 05:15:27 (13.5 MB/s) - ‘snli_1.0.zip’ saved [94550081/94550081]\n\n--2025-07-30 05:15:27--  https://cims.nyu.edu/~sbowman/multinli/multinli_1.0.zip\nResolving cims.nyu.edu (cims.nyu.edu)... 216.165.22.202\nConnecting to cims.nyu.edu (cims.nyu.edu)|216.165.22.202|:443... connected.\nHTTP request sent, awaiting response... 200 \nLength: 226850426 (216M) [application/zip]\nSaving to: ‘multinli_1.0.zip’\n\nmultinli_1.0.zip    100%[===================>] 216.34M  47.8MB/s    in 5.0s    \n\n2025-07-30 05:15:33 (43.6 MB/s) - ‘multinli_1.0.zip’ saved [226850426/226850426]\n\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"%cd /kaggle/working/DecT/datasets\nos.makedirs(\"snli\", exist_ok=True)\nos.makedirs(\"mnli-m\", exist_ok=True)\nos.makedirs(\"mnli-mm\", exist_ok=True)","metadata":{"execution":{"iopub.status.busy":"2025-07-30T05:15:45.223512Z","iopub.execute_input":"2025-07-30T05:15:45.224149Z","iopub.status.idle":"2025-07-30T05:15:45.230268Z","shell.execute_reply.started":"2025-07-30T05:15:45.224122Z","shell.execute_reply":"2025-07-30T05:15:45.229590Z"},"executionInfo":{"elapsed":4,"status":"aborted","timestamp":1752231504902,"user":{"displayName":"Gà Lê văn","userId":"04852251220989386905"},"user_tz":-420},"id":"iH5xyuOq7Qcm","trusted":true},"outputs":[{"name":"stdout","text":"/kaggle/working/DecT/datasets\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"def convert_to_jsonl(input_path, output_path):\n    with open(input_path, 'r', encoding='utf-8') as fin, open(output_path, 'w', encoding='utf-8') as fout:\n        header = fin.readline().strip().split('\\t')\n        for line in fin:\n            values = line.strip().split('\\t')\n            if len(values) != len(header):\n                continue\n            item = dict(zip(header, values))\n            # Lọc ra nhãn hợp lệ\n            if item.get(\"gold_label\") not in {\"entailment\", \"neutral\", \"contradiction\"}:\n                continue\n            example = {\n                \"sentence1\": item[\"sentence1\"],\n                \"sentence2\": item[\"sentence2\"],\n                \"label\": item[\"gold_label\"]\n            }\n            fout.write(json.dumps(example) + '\\n')","metadata":{"execution":{"iopub.status.busy":"2025-07-30T05:15:45.231087Z","iopub.execute_input":"2025-07-30T05:15:45.231414Z","iopub.status.idle":"2025-07-30T05:15:45.248475Z","shell.execute_reply.started":"2025-07-30T05:15:45.231386Z","shell.execute_reply":"2025-07-30T05:15:45.247935Z"},"executionInfo":{"elapsed":5,"status":"aborted","timestamp":1752231504905,"user":{"displayName":"Gà Lê văn","userId":"04852251220989386905"},"user_tz":-420},"id":"6sqSp87M-bun","trusted":true},"outputs":[],"execution_count":19},{"cell_type":"code","source":"def snli_txt_to_jsonl(input_txt_path, output_jsonl_path):\n    with open(input_txt_path, 'r', encoding='utf-8') as fin, \\\n         open(output_jsonl_path, 'w', encoding='utf-8') as fout:\n        header = fin.readline().strip().split('\\t')\n        for line in fin:\n            row = line.strip().split('\\t')\n            if len(row) != len(header):\n                continue\n            data = dict(zip(header, row))\n            if data[\"gold_label\"] not in [\"entailment\", \"neutral\", \"contradiction\"]:\n                continue\n            obj = {\n                \"sentence1\": data[\"sentence1\"],\n                \"sentence2\": data[\"sentence2\"],\n                \"gold_label\": data[\"gold_label\"]\n            }\n            fout.write(json.dumps(obj) + \"\\n\")\n\n# Chuyển SNLI về đúng định dạng jsonl mà SnliProcessor yêu cầu\nsnli_txt_to_jsonl(\"mnli_temp/snli_1.0/snli_1.0_train.txt\", \"snli/snli_1.0_train.jsonl\")\nsnli_txt_to_jsonl(\"mnli_temp/snli_1.0/snli_1.0_dev.txt\", \"snli/snli_1.0_dev.jsonl\")","metadata":{"execution":{"iopub.status.busy":"2025-07-30T05:15:45.249327Z","iopub.execute_input":"2025-07-30T05:15:45.249559Z","iopub.status.idle":"2025-07-30T05:15:46.290795Z","shell.execute_reply.started":"2025-07-30T05:15:45.249534Z","shell.execute_reply":"2025-07-30T05:15:46.290210Z"},"executionInfo":{"elapsed":14,"status":"aborted","timestamp":1752231504915,"user":{"displayName":"Gà Lê văn","userId":"04852251220989386905"},"user_tz":-420},"id":"EgV3PErwi2Sq","trusted":true},"outputs":[],"execution_count":20},{"cell_type":"code","source":"convert_to_jsonl(\"mnli_temp/snli_1.0/snli_1.0_train.txt\", \"snli/train.jsonl\")\nconvert_to_jsonl(\"mnli_temp/snli_1.0/snli_1.0_dev.txt\", \"snli/val.jsonl\")\nconvert_to_jsonl(\"mnli_temp/snli_1.0/snli_1.0_test.txt\", \"snli/test.jsonl\")","metadata":{"execution":{"iopub.status.busy":"2025-07-30T05:15:46.295108Z","iopub.execute_input":"2025-07-30T05:15:46.295343Z","iopub.status.idle":"2025-07-30T05:15:47.479982Z","shell.execute_reply.started":"2025-07-30T05:15:46.295318Z","shell.execute_reply":"2025-07-30T05:15:47.479368Z"},"executionInfo":{"elapsed":2,"status":"aborted","timestamp":1752231504917,"user":{"displayName":"Gà Lê văn","userId":"04852251220989386905"},"user_tz":-420},"id":"sM8UoApbADNa","trusted":true},"outputs":[],"execution_count":21},{"cell_type":"code","source":"convert_to_jsonl(\"mnli_temp/multinli_1.0/multinli_1.0_train.txt\", \"mnli-m/train.jsonl\")\nconvert_to_jsonl(\"mnli_temp/multinli_1.0/multinli_1.0_dev_matched.txt\", \"mnli-m/val.jsonl\")\nconvert_to_jsonl(\"mnli_temp/multinli_1.0/multinli_1.0_dev_matched.txt\", \"mnli-m/test.jsonl\")","metadata":{"execution":{"iopub.status.busy":"2025-07-30T05:15:47.480656Z","iopub.execute_input":"2025-07-30T05:15:47.480922Z","iopub.status.idle":"2025-07-30T05:15:48.474968Z","shell.execute_reply.started":"2025-07-30T05:15:47.480903Z","shell.execute_reply":"2025-07-30T05:15:48.474340Z"},"executionInfo":{"elapsed":273044,"status":"aborted","timestamp":1752231504919,"user":{"displayName":"Gà Lê văn","userId":"04852251220989386905"},"user_tz":-420},"id":"KT0cVvyYADnX","trusted":true},"outputs":[],"execution_count":22},{"cell_type":"code","source":"convert_to_jsonl(\"mnli_temp/multinli_1.0/multinli_1.0_train.txt\", \"mnli-mm/train.jsonl\")\nconvert_to_jsonl(\"mnli_temp/multinli_1.0/multinli_1.0_dev_mismatched.txt\", \"mnli-mm/val.jsonl\")\nconvert_to_jsonl(\"mnli_temp/multinli_1.0/multinli_1.0_dev_mismatched.txt\", \"mnli-mm/test.jsonl\")","metadata":{"execution":{"iopub.status.busy":"2025-07-30T05:15:48.475724Z","iopub.execute_input":"2025-07-30T05:15:48.475945Z","iopub.status.idle":"2025-07-30T05:15:50.085447Z","shell.execute_reply.started":"2025-07-30T05:15:48.475928Z","shell.execute_reply":"2025-07-30T05:15:50.084867Z"},"executionInfo":{"elapsed":273045,"status":"aborted","timestamp":1752231504920,"user":{"displayName":"Gà Lê văn","userId":"04852251220989386905"},"user_tz":-420},"id":"ZmWJ4sD-AGWr","trusted":true},"outputs":[],"execution_count":23},{"cell_type":"code","source":"def jsonl_to_mnli_tsv(jsonl_path, tsv_path):\n    with open(jsonl_path, 'r', encoding='utf-8') as fin, \\\n         open(tsv_path, 'w', newline='', encoding='utf-8') as fout:\n        writer = csv.writer(fout, delimiter='\\t', quoting=csv.QUOTE_NONE, escapechar='\\\\')\n\n        # Write header row (fake, just to skip with next())\n        writer.writerow(['col0', 'col1', 'col2', 'col3', 'col4', 'col5', 'col6', 'col7', 'sentence1', 'sentence2', 'label'])\n\n        for line in fin:\n            data = json.loads(line)\n            if data[\"label\"] not in [\"entailment\", \"neutral\", \"contradiction\"]:\n                continue\n            row = ['_'] * 8 + [data[\"sentence1\"], data[\"sentence2\"], data[\"label\"]]\n            writer.writerow(row)","metadata":{"execution":{"iopub.status.busy":"2025-07-30T05:15:50.086195Z","iopub.execute_input":"2025-07-30T05:15:50.086453Z","iopub.status.idle":"2025-07-30T05:15:50.544716Z","shell.execute_reply.started":"2025-07-30T05:15:50.086429Z","shell.execute_reply":"2025-07-30T05:15:50.543848Z"},"executionInfo":{"elapsed":273046,"status":"aborted","timestamp":1752231504921,"user":{"displayName":"Gà Lê văn","userId":"04852251220989386905"},"user_tz":-420},"id":"ZBXApqHJDGef","trusted":true},"outputs":[],"execution_count":24},{"cell_type":"code","source":"# Tạo các file cho mnli-m\njsonl_to_mnli_tsv(\"mnli-m/train.jsonl\", \"mnli-m/train.tsv\")\njsonl_to_mnli_tsv(\"mnli-m/val.jsonl\", \"mnli-m/train.tsv\")  # vì dev -> train\njsonl_to_mnli_tsv(\"mnli-m/test.jsonl\", \"mnli-m/dev_matched.tsv\")\n\n# Tạo các file cho mnli-mm\njsonl_to_mnli_tsv(\"mnli-mm/train.jsonl\", \"mnli-mm/train.tsv\")\njsonl_to_mnli_tsv(\"mnli-mm/val.jsonl\", \"mnli-mm/train.tsv\")  # vì dev -> train\njsonl_to_mnli_tsv(\"mnli-mm/test.jsonl\", \"mnli-mm/dev_mismatched.tsv\")\n","metadata":{"execution":{"iopub.status.busy":"2025-07-30T05:15:50.545628Z","iopub.execute_input":"2025-07-30T05:15:50.546165Z","iopub.status.idle":"2025-07-30T05:15:51.362378Z","shell.execute_reply.started":"2025-07-30T05:15:50.546138Z","shell.execute_reply":"2025-07-30T05:15:51.361582Z"},"executionInfo":{"elapsed":273048,"status":"aborted","timestamp":1752231504923,"user":{"displayName":"Gà Lê văn","userId":"04852251220989386905"},"user_tz":-420},"id":"2WpbidBjDxq0","trusted":true},"outputs":[],"execution_count":25},{"cell_type":"code","source":"!rm -r mnli_temp","metadata":{"execution":{"iopub.status.busy":"2025-07-30T05:15:51.363342Z","iopub.execute_input":"2025-07-30T05:15:51.363630Z","iopub.status.idle":"2025-07-30T05:15:51.964315Z","shell.execute_reply.started":"2025-07-30T05:15:51.363608Z","shell.execute_reply":"2025-07-30T05:15:51.963364Z"},"executionInfo":{"elapsed":273048,"status":"aborted","timestamp":1752231504924,"user":{"displayName":"Gà Lê văn","userId":"04852251220989386905"},"user_tz":-420},"id":"BUsu02FFGrDr","trusted":true},"outputs":[],"execution_count":26},{"cell_type":"markdown","source":"## RTE ( chạy được)","metadata":{"id":"Fy75kSqfGrDr"}},{"cell_type":"code","source":"!mkdir -p /kaggle/working/DecT/datasets/rte\n%cd /kaggle/working/DecT/datasets/rte\n\n!wget https://huggingface.co/datasets/SetFit/rte/resolve/main/train.jsonl\n!wget https://huggingface.co/datasets/SetFit/rte/resolve/main/validation.jsonl\n!wget https://huggingface.co/datasets/SetFit/rte/resolve/main/test.jsonl","metadata":{"execution":{"iopub.status.busy":"2025-07-30T05:15:51.965463Z","iopub.execute_input":"2025-07-30T05:15:51.965826Z","iopub.status.idle":"2025-07-30T05:15:53.247613Z","shell.execute_reply.started":"2025-07-30T05:15:51.965798Z","shell.execute_reply":"2025-07-30T05:15:53.246715Z"},"executionInfo":{"elapsed":273047,"status":"aborted","timestamp":1752231504926,"user":{"displayName":"Gà Lê văn","userId":"04852251220989386905"},"user_tz":-420},"id":"yVV18v2fGrDr","trusted":true},"outputs":[{"name":"stdout","text":"/kaggle/working/DecT/datasets/rte\n--2025-07-30 05:15:52--  https://huggingface.co/datasets/SetFit/rte/resolve/main/train.jsonl\nResolving huggingface.co (huggingface.co)... 3.165.160.12, 3.165.160.11, 3.165.160.61, ...\nConnecting to huggingface.co (huggingface.co)|3.165.160.12|:443... connected.\nHTTP request sent, awaiting response... 307 Temporary Redirect\nLocation: /api/resolve-cache/datasets/SetFit/rte/23f2a468b9bc13030f5595a2e5f9307cb165280c/train.jsonl?%2Fdatasets%2FSetFit%2Frte%2Fresolve%2Fmain%2Ftrain.jsonl=&etag=%222b40fef8de4beb31e0412e63cf29c2a4a6a99dd4%22 [following]\n--2025-07-30 05:15:52--  https://huggingface.co/api/resolve-cache/datasets/SetFit/rte/23f2a468b9bc13030f5595a2e5f9307cb165280c/train.jsonl?%2Fdatasets%2FSetFit%2Frte%2Fresolve%2Fmain%2Ftrain.jsonl=&etag=%222b40fef8de4beb31e0412e63cf29c2a4a6a99dd4%22\nReusing existing connection to huggingface.co:443.\nHTTP request sent, awaiting response... 200 OK\nLength: 980383 (957K) [text/plain]\nSaving to: ‘train.jsonl’\n\ntrain.jsonl         100%[===================>] 957.41K  --.-KB/s    in 0.03s   \n\n2025-07-30 05:15:52 (28.4 MB/s) - ‘train.jsonl’ saved [980383/980383]\n\n--2025-07-30 05:15:52--  https://huggingface.co/datasets/SetFit/rte/resolve/main/validation.jsonl\nResolving huggingface.co (huggingface.co)... 3.165.160.61, 3.165.160.12, 3.165.160.59, ...\nConnecting to huggingface.co (huggingface.co)|3.165.160.61|:443... connected.\nHTTP request sent, awaiting response... 307 Temporary Redirect\nLocation: /api/resolve-cache/datasets/SetFit/rte/23f2a468b9bc13030f5595a2e5f9307cb165280c/validation.jsonl?%2Fdatasets%2FSetFit%2Frte%2Fresolve%2Fmain%2Fvalidation.jsonl=&etag=%227d6cb5af81484e44249b14105c7685b832f665fa%22 [following]\n--2025-07-30 05:15:52--  https://huggingface.co/api/resolve-cache/datasets/SetFit/rte/23f2a468b9bc13030f5595a2e5f9307cb165280c/validation.jsonl?%2Fdatasets%2FSetFit%2Frte%2Fresolve%2Fmain%2Fvalidation.jsonl=&etag=%227d6cb5af81484e44249b14105c7685b832f665fa%22\nReusing existing connection to huggingface.co:443.\nHTTP request sent, awaiting response... 200 OK\nLength: 105200 (103K) [text/plain]\nSaving to: ‘validation.jsonl’\n\nvalidation.jsonl    100%[===================>] 102.73K  --.-KB/s    in 0.01s   \n\n2025-07-30 05:15:52 (9.77 MB/s) - ‘validation.jsonl’ saved [105200/105200]\n\n--2025-07-30 05:15:52--  https://huggingface.co/datasets/SetFit/rte/resolve/main/test.jsonl\nResolving huggingface.co (huggingface.co)... 3.165.160.59, 3.165.160.61, 3.165.160.11, ...\nConnecting to huggingface.co (huggingface.co)|3.165.160.59|:443... connected.\nHTTP request sent, awaiting response... 307 Temporary Redirect\nLocation: /api/resolve-cache/datasets/SetFit/rte/23f2a468b9bc13030f5595a2e5f9307cb165280c/test.jsonl?%2Fdatasets%2FSetFit%2Frte%2Fresolve%2Fmain%2Ftest.jsonl=&etag=%22d588b0ba3a93cba05440ca462e6a5dd110d1684f%22 [following]\n--2025-07-30 05:15:53--  https://huggingface.co/api/resolve-cache/datasets/SetFit/rte/23f2a468b9bc13030f5595a2e5f9307cb165280c/test.jsonl?%2Fdatasets%2FSetFit%2Frte%2Fresolve%2Fmain%2Ftest.jsonl=&etag=%22d588b0ba3a93cba05440ca462e6a5dd110d1684f%22\nReusing existing connection to huggingface.co:443.\nHTTP request sent, awaiting response... 200 OK\nLength: 1128373 (1.1M) [text/plain]\nSaving to: ‘test.jsonl’\n\ntest.jsonl          100%[===================>]   1.08M  --.-KB/s    in 0.03s   \n\n2025-07-30 05:15:53 (38.0 MB/s) - ‘test.jsonl’ saved [1128373/1128373]\n\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"%cd /kaggle/working/DecT/datasets\ndata_dir = \"rte\"\nlabel_map = {0: \"not_entailment\", 1: \"entailment\"}\n\ndef convert_jsonl_to_tsv(jsonl_path, tsv_path):\n    with open(jsonl_path, 'r', encoding='utf-8') as fin, open(tsv_path, 'w', encoding='utf-8') as fout:\n        fout.write(\"id\\tsentence1\\tsentence2\\tlabel\\n\")\n        for idx, line in enumerate(fin):\n            obj = json.loads(line)\n            label = obj['label']\n            if label == -1:\n                # Bỏ qua hoặc gán nhãn mặc định\n                # continue  # nếu muốn bỏ dòng này\n                label_str = \"not_entailment\"  # hoặc \"entailment\"\n            else:\n                label_str = label_map[label]\n            fout.write(f\"{idx}\\t{obj['text1']}\\t{obj['text2']}\\t{label_str}\\n\")\n\n\nconvert_jsonl_to_tsv(os.path.join(data_dir, \"train.jsonl\"), os.path.join(data_dir, \"train.tsv\"))\nconvert_jsonl_to_tsv(os.path.join(data_dir, \"validation.jsonl\"), os.path.join(data_dir, \"dev.tsv\"))\nconvert_jsonl_to_tsv(os.path.join(data_dir, \"test.jsonl\"), os.path.join(data_dir, \"test.tsv\"))","metadata":{"execution":{"iopub.status.busy":"2025-07-30T05:15:53.248864Z","iopub.execute_input":"2025-07-30T05:15:53.249123Z","iopub.status.idle":"2025-07-30T05:15:53.284577Z","shell.execute_reply.started":"2025-07-30T05:15:53.249097Z","shell.execute_reply":"2025-07-30T05:15:53.284054Z"},"executionInfo":{"elapsed":273054,"status":"aborted","timestamp":1752231504935,"user":{"displayName":"Gà Lê văn","userId":"04852251220989386905"},"user_tz":-420},"id":"ZgrWBTHTGrDs","trusted":true},"outputs":[{"name":"stdout","text":"/kaggle/working/DecT/datasets\n","output_type":"stream"}],"execution_count":28},{"cell_type":"markdown","source":"## SST2(chạy được)","metadata":{"id":"a48T0ess0MPh"}},{"cell_type":"code","source":"%cd /kaggle/working/DecT\n!python datasets/download_glue_data.py --data_dir ./datasets --tasks SST\n!cp datasets/SST-2/*.tsv datasets/sst2/\n!rm -r /kaggle/working/DecT/datasets/SST-2","metadata":{"execution":{"iopub.status.busy":"2025-07-30T05:15:53.285367Z","iopub.execute_input":"2025-07-30T05:15:53.285622Z","iopub.status.idle":"2025-07-30T05:15:54.206759Z","shell.execute_reply.started":"2025-07-30T05:15:53.285600Z","shell.execute_reply":"2025-07-30T05:15:54.205949Z"},"executionInfo":{"elapsed":273055,"status":"aborted","timestamp":1752231504938,"user":{"displayName":"Gà Lê văn","userId":"04852251220989386905"},"user_tz":-420},"id":"KXKhcz5m8q43","trusted":true},"outputs":[{"name":"stdout","text":"/kaggle/working/DecT\nDownloading and extracting SST...\n\tCompleted!\n","output_type":"stream"}],"execution_count":29},{"cell_type":"markdown","source":"## Yahoo(chạy được )","metadata":{"id":"_-CvYY900Vn0"}},{"cell_type":"code","source":"# Tạo thư mục\n!mkdir -p %cd /kaggle/working/DecT/datasets/yahoo_raw\n%cd /kaggle/working/DecT/datasets/yahoo_raw\n\n# Tải trực tiếp từ snapshot HF\n!wget https://huggingface.co/datasets/community-datasets/yahoo_answers_topics/resolve/main/yahoo_answers_topics/train-00000-of-00002.parquet\n!wget https://huggingface.co/datasets/community-datasets/yahoo_answers_topics/resolve/main/yahoo_answers_topics/train-00001-of-00002.parquet\n!wget https://huggingface.co/datasets/community-datasets/yahoo_answers_topics/resolve/main/yahoo_answers_topics/test-00000-of-00001.parquet","metadata":{"execution":{"iopub.status.busy":"2025-07-30T05:15:54.207839Z","iopub.execute_input":"2025-07-30T05:15:54.208080Z","iopub.status.idle":"2025-07-30T05:16:02.313434Z","shell.execute_reply.started":"2025-07-30T05:15:54.208057Z","shell.execute_reply":"2025-07-30T05:16:02.312276Z"},"executionInfo":{"elapsed":273054,"status":"aborted","timestamp":1752231504939,"user":{"displayName":"Gà Lê văn","userId":"04852251220989386905"},"user_tz":-420},"id":"dJgyTjfx4xAU","trusted":true},"outputs":[{"name":"stdout","text":"/kaggle/working/DecT/datasets/yahoo_raw\n--2025-07-30 05:15:54--  https://huggingface.co/datasets/community-datasets/yahoo_answers_topics/resolve/main/yahoo_answers_topics/train-00000-of-00002.parquet\nResolving huggingface.co (huggingface.co)... 3.165.160.12, 3.165.160.61, 3.165.160.59, ...\nConnecting to huggingface.co (huggingface.co)|3.165.160.12|:443... connected.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://cas-bridge.xethub.hf.co/xet-bridge-us/621ffdd236468d709f182031/d16eae77ad3b0323a2871f7146be7bb6ed67725b33441e9c2593f9def04c9263?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=cas%2F20250730%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250730T051554Z&X-Amz-Expires=3600&X-Amz-Signature=f58442729f9848f6804205bb6836d2e228a92e57dc33bb555ca7f988d6906991&X-Amz-SignedHeaders=host&X-Xet-Cas-Uid=public&response-content-disposition=inline%3B+filename*%3DUTF-8%27%27train-00000-of-00002.parquet%3B+filename%3D%22train-00000-of-00002.parquet%22%3B&x-id=GetObject&Expires=1753856154&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc1Mzg1NjE1NH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2FzLWJyaWRnZS54ZXRodWIuaGYuY28veGV0LWJyaWRnZS11cy82MjFmZmRkMjM2NDY4ZDcwOWYxODIwMzEvZDE2ZWFlNzdhZDNiMDMyM2EyODcxZjcxNDZiZTdiYjZlZDY3NzI1YjMzNDQxZTljMjU5M2Y5ZGVmMDRjOTI2MyoifV19&Signature=QkRmjamYXkxSoPD8tIgsTwx15xj-ByJWtSnvpI4JHKWuf2sTXeamOyLeoZVYovUJt8L3X2oRUtpq-dRC7AaESYA7-VJs6U8dO2MGaYpm7U1xnXfNv8Oq5Ii1A7bRS49vvHWnuxzF9-OhjVC67Xp96Adj9QrnT8hWrMyLXnfiPgGN81BwAJoKfpODtz36tjNEeyR77zpPBimirXzelm7HcKr4msJ62Mk-0mOjTQx17ZKuPRbXhe%7EDE11Rhc4IZnTz4AbakoJ4X4jG5TB751bWTMY6raMQRFARJv-Tkj8JL4sYD-pKjhShKRi4IO5vqoWXwhX-TOpIme0WTzOh%7Eu-Qlg__&Key-Pair-Id=K2L8F4GPSG1IFC [following]\n--2025-07-30 05:15:54--  https://cas-bridge.xethub.hf.co/xet-bridge-us/621ffdd236468d709f182031/d16eae77ad3b0323a2871f7146be7bb6ed67725b33441e9c2593f9def04c9263?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=cas%2F20250730%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250730T051554Z&X-Amz-Expires=3600&X-Amz-Signature=f58442729f9848f6804205bb6836d2e228a92e57dc33bb555ca7f988d6906991&X-Amz-SignedHeaders=host&X-Xet-Cas-Uid=public&response-content-disposition=inline%3B+filename*%3DUTF-8%27%27train-00000-of-00002.parquet%3B+filename%3D%22train-00000-of-00002.parquet%22%3B&x-id=GetObject&Expires=1753856154&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc1Mzg1NjE1NH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2FzLWJyaWRnZS54ZXRodWIuaGYuY28veGV0LWJyaWRnZS11cy82MjFmZmRkMjM2NDY4ZDcwOWYxODIwMzEvZDE2ZWFlNzdhZDNiMDMyM2EyODcxZjcxNDZiZTdiYjZlZDY3NzI1YjMzNDQxZTljMjU5M2Y5ZGVmMDRjOTI2MyoifV19&Signature=QkRmjamYXkxSoPD8tIgsTwx15xj-ByJWtSnvpI4JHKWuf2sTXeamOyLeoZVYovUJt8L3X2oRUtpq-dRC7AaESYA7-VJs6U8dO2MGaYpm7U1xnXfNv8Oq5Ii1A7bRS49vvHWnuxzF9-OhjVC67Xp96Adj9QrnT8hWrMyLXnfiPgGN81BwAJoKfpODtz36tjNEeyR77zpPBimirXzelm7HcKr4msJ62Mk-0mOjTQx17ZKuPRbXhe%7EDE11Rhc4IZnTz4AbakoJ4X4jG5TB751bWTMY6raMQRFARJv-Tkj8JL4sYD-pKjhShKRi4IO5vqoWXwhX-TOpIme0WTzOh%7Eu-Qlg__&Key-Pair-Id=K2L8F4GPSG1IFC\nResolving cas-bridge.xethub.hf.co (cas-bridge.xethub.hf.co)... 18.238.217.88, 18.238.217.64, 18.238.217.126, ...\nConnecting to cas-bridge.xethub.hf.co (cas-bridge.xethub.hf.co)|18.238.217.88|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 241340294 (230M)\nSaving to: ‘train-00000-of-00002.parquet’\n\ntrain-00000-of-0000 100%[===================>] 230.16M  76.3MB/s    in 3.0s    \n\n2025-07-30 05:15:57 (76.3 MB/s) - ‘train-00000-of-00002.parquet’ saved [241340294/241340294]\n\n--2025-07-30 05:15:57--  https://huggingface.co/datasets/community-datasets/yahoo_answers_topics/resolve/main/yahoo_answers_topics/train-00001-of-00002.parquet\nResolving huggingface.co (huggingface.co)... 3.165.160.61, 3.165.160.59, 3.165.160.11, ...\nConnecting to huggingface.co (huggingface.co)|3.165.160.61|:443... connected.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://cas-bridge.xethub.hf.co/xet-bridge-us/621ffdd236468d709f182031/acdeaa90a9aa83650cd671eb89b709448780af6c457c2a79612578ec2d615126?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=cas%2F20250730%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250730T051557Z&X-Amz-Expires=3600&X-Amz-Signature=b392384e63ac044de9ba77c96e649528a41d840dba15d0108219a8a09c8559c8&X-Amz-SignedHeaders=host&X-Xet-Cas-Uid=public&response-content-disposition=inline%3B+filename*%3DUTF-8%27%27train-00001-of-00002.parquet%3B+filename%3D%22train-00001-of-00002.parquet%22%3B&x-id=GetObject&Expires=1753856157&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc1Mzg1NjE1N319LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2FzLWJyaWRnZS54ZXRodWIuaGYuY28veGV0LWJyaWRnZS11cy82MjFmZmRkMjM2NDY4ZDcwOWYxODIwMzEvYWNkZWFhOTBhOWFhODM2NTBjZDY3MWViODliNzA5NDQ4NzgwYWY2YzQ1N2MyYTc5NjEyNTc4ZWMyZDYxNTEyNioifV19&Signature=dPS9jC2qSxxqPiyizr21BvtWH%7E1t-ItPXKPfBraWnftS4wurSg7aWRg7ywhW818JJq6F-%7EUVL16j-FiE89NC1u3FiOAzB5fOVMu3peWM%7EkGyChaatkiaUG7qMh3eB6GxMuDDLdM2rQJNz-RNFlls1DJf3cLm2hD33DtPPS2i0aQL-wigV4mc0XxuVUNOcOgQ4Hc-F6QxYhmR9KM6XVqOxdiWYxGXXEp7JNVrYVzQGDpZs7Zi4myQNjvt4%7E-6uVV0b7PA9AO98CcQ21sG6c9KZ93WqRdYz3uLHRbFdhbi5wh7WvBu%7E15QQiQtPCFeRYqcjTZYWfRsTrbNkCA8Q6EfNA__&Key-Pair-Id=K2L8F4GPSG1IFC [following]\n--2025-07-30 05:15:58--  https://cas-bridge.xethub.hf.co/xet-bridge-us/621ffdd236468d709f182031/acdeaa90a9aa83650cd671eb89b709448780af6c457c2a79612578ec2d615126?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=cas%2F20250730%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250730T051557Z&X-Amz-Expires=3600&X-Amz-Signature=b392384e63ac044de9ba77c96e649528a41d840dba15d0108219a8a09c8559c8&X-Amz-SignedHeaders=host&X-Xet-Cas-Uid=public&response-content-disposition=inline%3B+filename*%3DUTF-8%27%27train-00001-of-00002.parquet%3B+filename%3D%22train-00001-of-00002.parquet%22%3B&x-id=GetObject&Expires=1753856157&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc1Mzg1NjE1N319LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2FzLWJyaWRnZS54ZXRodWIuaGYuY28veGV0LWJyaWRnZS11cy82MjFmZmRkMjM2NDY4ZDcwOWYxODIwMzEvYWNkZWFhOTBhOWFhODM2NTBjZDY3MWViODliNzA5NDQ4NzgwYWY2YzQ1N2MyYTc5NjEyNTc4ZWMyZDYxNTEyNioifV19&Signature=dPS9jC2qSxxqPiyizr21BvtWH%7E1t-ItPXKPfBraWnftS4wurSg7aWRg7ywhW818JJq6F-%7EUVL16j-FiE89NC1u3FiOAzB5fOVMu3peWM%7EkGyChaatkiaUG7qMh3eB6GxMuDDLdM2rQJNz-RNFlls1DJf3cLm2hD33DtPPS2i0aQL-wigV4mc0XxuVUNOcOgQ4Hc-F6QxYhmR9KM6XVqOxdiWYxGXXEp7JNVrYVzQGDpZs7Zi4myQNjvt4%7E-6uVV0b7PA9AO98CcQ21sG6c9KZ93WqRdYz3uLHRbFdhbi5wh7WvBu%7E15QQiQtPCFeRYqcjTZYWfRsTrbNkCA8Q6EfNA__&Key-Pair-Id=K2L8F4GPSG1IFC\nResolving cas-bridge.xethub.hf.co (cas-bridge.xethub.hf.co)... 18.238.217.88, 18.238.217.63, 18.238.217.126, ...\nConnecting to cas-bridge.xethub.hf.co (cas-bridge.xethub.hf.co)|18.238.217.88|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 270150048 (258M)\nSaving to: ‘train-00001-of-00002.parquet’\n\ntrain-00001-of-0000 100%[===================>] 257.63M  76.7MB/s    in 3.4s    \n\n2025-07-30 05:16:01 (76.7 MB/s) - ‘train-00001-of-00002.parquet’ saved [270150048/270150048]\n\n--2025-07-30 05:16:01--  https://huggingface.co/datasets/community-datasets/yahoo_answers_topics/resolve/main/yahoo_answers_topics/test-00000-of-00001.parquet\nResolving huggingface.co (huggingface.co)... 3.165.160.11, 3.165.160.61, 3.165.160.12, ...\nConnecting to huggingface.co (huggingface.co)|3.165.160.11|:443... connected.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://cas-bridge.xethub.hf.co/xet-bridge-us/621ffdd236468d709f182031/b2d8ebaae5df7ac911db4ba8a6bb6acf6c3b2a9859de35d9f66b53b667be3c2a?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=cas%2F20250730%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250730T051601Z&X-Amz-Expires=3600&X-Amz-Signature=ddedd7188f2c8687af59fa046cbf578b5a36736c18c02453f20a13e5eba6208c&X-Amz-SignedHeaders=host&X-Xet-Cas-Uid=public&response-content-disposition=inline%3B+filename*%3DUTF-8%27%27test-00000-of-00001.parquet%3B+filename%3D%22test-00000-of-00001.parquet%22%3B&x-id=GetObject&Expires=1753856161&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc1Mzg1NjE2MX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2FzLWJyaWRnZS54ZXRodWIuaGYuY28veGV0LWJyaWRnZS11cy82MjFmZmRkMjM2NDY4ZDcwOWYxODIwMzEvYjJkOGViYWFlNWRmN2FjOTExZGI0YmE4YTZiYjZhY2Y2YzNiMmE5ODU5ZGUzNWQ5ZjY2YjUzYjY2N2JlM2MyYSoifV19&Signature=fGbVVpRgh1GjD2SEvekK5ATUDMH8LdbVT51aAz4tRl-1YRXjG9Ez33kZO0eMxONiMzI29ketcZpXPpsSUO2GHTnCPZTbKDN7oSqxpdmk8EYwBMsU6djsLVnlZdt-NSHY4jrh68ibx09nv8Ooq5Zj1voaMixiBg-FBNRfS%7EYKl4gUvCa6GBZodpftbsSqQMNdklckkqU2BWfPSGScnIYo%7EjmVGT7izBvvp2e-wzjcWN1MZ0oy7IuhbXZxEnasp1q%7EcRWtfnGf0ADsokVZK1la-IEw0t9lFZmS1kwk5rf5rYcLYqtaXqkTCQa0Fn9Up1UdkcAsYSDdZYcoEMJb6Lyx5g__&Key-Pair-Id=K2L8F4GPSG1IFC [following]\n--2025-07-30 05:16:01--  https://cas-bridge.xethub.hf.co/xet-bridge-us/621ffdd236468d709f182031/b2d8ebaae5df7ac911db4ba8a6bb6acf6c3b2a9859de35d9f66b53b667be3c2a?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=cas%2F20250730%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250730T051601Z&X-Amz-Expires=3600&X-Amz-Signature=ddedd7188f2c8687af59fa046cbf578b5a36736c18c02453f20a13e5eba6208c&X-Amz-SignedHeaders=host&X-Xet-Cas-Uid=public&response-content-disposition=inline%3B+filename*%3DUTF-8%27%27test-00000-of-00001.parquet%3B+filename%3D%22test-00000-of-00001.parquet%22%3B&x-id=GetObject&Expires=1753856161&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc1Mzg1NjE2MX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2FzLWJyaWRnZS54ZXRodWIuaGYuY28veGV0LWJyaWRnZS11cy82MjFmZmRkMjM2NDY4ZDcwOWYxODIwMzEvYjJkOGViYWFlNWRmN2FjOTExZGI0YmE4YTZiYjZhY2Y2YzNiMmE5ODU5ZGUzNWQ5ZjY2YjUzYjY2N2JlM2MyYSoifV19&Signature=fGbVVpRgh1GjD2SEvekK5ATUDMH8LdbVT51aAz4tRl-1YRXjG9Ez33kZO0eMxONiMzI29ketcZpXPpsSUO2GHTnCPZTbKDN7oSqxpdmk8EYwBMsU6djsLVnlZdt-NSHY4jrh68ibx09nv8Ooq5Zj1voaMixiBg-FBNRfS%7EYKl4gUvCa6GBZodpftbsSqQMNdklckkqU2BWfPSGScnIYo%7EjmVGT7izBvvp2e-wzjcWN1MZ0oy7IuhbXZxEnasp1q%7EcRWtfnGf0ADsokVZK1la-IEw0t9lFZmS1kwk5rf5rYcLYqtaXqkTCQa0Fn9Up1UdkcAsYSDdZYcoEMJb6Lyx5g__&Key-Pair-Id=K2L8F4GPSG1IFC\nResolving cas-bridge.xethub.hf.co (cas-bridge.xethub.hf.co)... 18.238.217.88, 18.238.217.64, 18.238.217.126, ...\nConnecting to cas-bridge.xethub.hf.co (cas-bridge.xethub.hf.co)|18.238.217.88|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 21939321 (21M)\nSaving to: ‘test-00000-of-00001.parquet’\n\ntest-00000-of-00001 100%[===================>]  20.92M  75.5MB/s    in 0.3s    \n\n2025-07-30 05:16:02 (75.5 MB/s) - ‘test-00000-of-00001.parquet’ saved [21939321/21939321]\n\n","output_type":"stream"}],"execution_count":30},{"cell_type":"code","source":"# Đọc từng file parquet và nối lại\n%cd /kaggle/working/DecT/datasets\ndf_train_1 = pd.read_parquet(\"yahoo_raw/train-00000-of-00002.parquet\")\ndf_train_2 = pd.read_parquet(\"yahoo_raw/train-00001-of-00002.parquet\")\ndf_train = pd.concat([df_train_1, df_train_2], ignore_index=True)\ndf_test = pd.read_parquet(\"yahoo_raw/test-00000-of-00001.parquet\")\n\n# Tạo thư mục output\nout_dir = \"yahoo\"\n\n# Ghi file CSV đúng format YahooProcessor\ndef save_csv(df, out_path):\n    with open(out_path, \"w\", encoding=\"utf-8\", newline=\"\") as f:\n        writer = csv.writer(f)\n        for _, row in df.iterrows():\n            label = row[\"topic\"] + 1\n            writer.writerow([label, row[\"question_title\"], row[\"question_content\"], row[\"best_answer\"]])\n\nsave_csv(df_train, f\"{out_dir}/train.csv\")\nsave_csv(df_test,  f\"{out_dir}/test.csv\")\n!cp {out_dir}/test.csv {out_dir}/val.csv\n!rm -r yahoo_raw","metadata":{"execution":{"iopub.status.busy":"2025-07-30T05:16:02.314721Z","iopub.execute_input":"2025-07-30T05:16:02.315454Z","iopub.status.idle":"2025-07-30T05:17:25.878768Z","shell.execute_reply.started":"2025-07-30T05:16:02.315408Z","shell.execute_reply":"2025-07-30T05:17:25.877725Z"},"executionInfo":{"elapsed":273053,"status":"aborted","timestamp":1752231504941,"user":{"displayName":"Gà Lê văn","userId":"04852251220989386905"},"user_tz":-420},"id":"vEOyqOdFcTzK","trusted":true},"outputs":[{"name":"stdout","text":"/kaggle/working/DecT/datasets\n","output_type":"stream"}],"execution_count":31},{"cell_type":"markdown","source":"## Yelp (chạy đc)","metadata":{"id":"JqWk91AO0afo"}},{"cell_type":"code","source":"# Tạo thư mục raw và chuyển vào đó\n!mkdir -p %cd /kaggle/working/DecT/datasets/yelp_raw\n%cd /kaggle/working/DecT/datasets/yelp_raw\n\n# Tải và giải nén gói dữ liệu CSV chuẩn\n!wget https://s3.amazonaws.com/fast-ai-nlp/yelp_review_polarity_csv.tgz\n!tar -xzf yelp_review_polarity_csv.tgz","metadata":{"execution":{"iopub.status.busy":"2025-07-30T05:17:25.880079Z","iopub.execute_input":"2025-07-30T05:17:25.880363Z","iopub.status.idle":"2025-07-30T05:17:35.240299Z","shell.execute_reply.started":"2025-07-30T05:17:25.880338Z","shell.execute_reply":"2025-07-30T05:17:35.239424Z"},"executionInfo":{"elapsed":273052,"status":"aborted","timestamp":1752231504942,"user":{"displayName":"Gà Lê văn","userId":"04852251220989386905"},"user_tz":-420},"id":"_DIInQnLf8fQ","trusted":true},"outputs":[{"name":"stdout","text":"/kaggle/working/DecT/datasets/yelp_raw\n--2025-07-30 05:17:26--  https://s3.amazonaws.com/fast-ai-nlp/yelp_review_polarity_csv.tgz\nResolving s3.amazonaws.com (s3.amazonaws.com)... 52.217.91.78, 52.217.171.72, 52.217.119.88, ...\nConnecting to s3.amazonaws.com (s3.amazonaws.com)|52.217.91.78|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 166373201 (159M) [application/x-tar]\nSaving to: ‘yelp_review_polarity_csv.tgz’\n\nyelp_review_polarit 100%[===================>] 158.67M  33.9MB/s    in 5.0s    \n\n2025-07-30 05:17:31 (31.8 MB/s) - ‘yelp_review_polarity_csv.tgz’ saved [166373201/166373201]\n\n","output_type":"stream"}],"execution_count":32},{"cell_type":"code","source":"# Tạo thư mục datasets trong DecT nếu chưa có\n%cd /kaggle/working/DecT/datasets\n\n# Copy file CSV cần thiết\n!cp yelp_raw/yelp_review_polarity_csv/train.csv yelp/train.csv\n!cp yelp_raw/yelp_review_polarity_csv/test.csv yelp/test.csv\n\n!rm -r yelp_raw","metadata":{"execution":{"iopub.status.busy":"2025-07-30T05:17:35.241396Z","iopub.execute_input":"2025-07-30T05:17:35.241654Z","iopub.status.idle":"2025-07-30T05:17:36.359688Z","shell.execute_reply.started":"2025-07-30T05:17:35.241623Z","shell.execute_reply":"2025-07-30T05:17:36.358825Z"},"executionInfo":{"elapsed":273051,"status":"aborted","timestamp":1752231504943,"user":{"displayName":"Gà Lê văn","userId":"04852251220989386905"},"user_tz":-420},"id":"pV7uT5x70baw","trusted":true},"outputs":[{"name":"stdout","text":"/kaggle/working/DecT/datasets\n","output_type":"stream"}],"execution_count":33},{"cell_type":"markdown","source":"# Chạy model gpt2","metadata":{"id":"tMu9Gsgn3VnZ"}},{"cell_type":"markdown","source":"chú thích :\n\n1. model : tải và cài đặt các model\n\n2. size : base (để mặc định)\n\n3. type : cách thư mục trong script\n\n4. dataset : thì là tên gọi nó ra (sst2, imdb, yelp, agnews, dbpedia, yahoo, rte, snli, mnli-m, mnli-mm, fewnerd)\n\n5. max_epochs : tùy chọn\n\n6. batch_size : tùy chọn nếu máy mạnh\n\n7. các thông số sau cứ giữ nguyên","metadata":{"id":"gvk8fqBXliwu"}},{"cell_type":"code","source":"%%writefile /kaggle/working/DecT/src/process_data.py\nimport os\nimport json, csv\nfrom abc import ABC, abstractmethod\nfrom collections import defaultdict, Counter\nfrom typing import *\nimport pandas as pd\nfrom openprompt.utils.logging import logger\nfrom openprompt.data_utils.utils import InputExample\nfrom openprompt.data_utils.data_processor import DataProcessor\n\ndef load_dataset(dataset):\n    r\"\"\"A dataset loader using a global config.\n    It will load the train, valid, and test set (if exists) simulatenously.\n\n    \"\"\"\n\n    processor = PROCESSORS[dataset.lower()]()\n    path = 'datasets/' + dataset.lower()\n\n    train_dataset = None\n    valid_dataset = None\n\n    try:\n        train_dataset = processor.get_train_examples(path)\n    except FileNotFoundError:\n        logger.warning(f\"Has no training dataset in {path}.\")\n    try:\n        valid_dataset = processor.get_dev_examples(path)\n    except FileNotFoundError:\n        logger.warning(f\"Has no validation dataset in {path}.\")\n\n    test_dataset = None\n    try:\n        test_dataset = processor.get_test_examples(path)\n    except FileNotFoundError:\n        logger.warning(f\"Has no test dataset in {path}.\")\n    # checking whether donwloaded.\n    if (train_dataset is None) and \\\n       (valid_dataset is None) and \\\n       (test_dataset is None):\n        logger.error(\"Dataset is empty. Either there is no download or the path is wrong. \"+ \\\n        \"If not downloaded, please `cd datasets/` and `bash download_xxx.sh`\")\n        exit()\n    return train_dataset, valid_dataset, test_dataset, processor\n\n\nclass MnlimmProcessor(DataProcessor):\n    # TODO Test needed\n    dataset_project = {\"train\": \"train\",\n                        \"dev\": \"train\",\n                        \"test\": \"dev_mismatched\"\n                        }\n    def __init__(self):\n        super().__init__()\n        self.labels = [\"contradiction\", \"entailment\", \"neutral\"]\n\n    def get_examples(self, data_dir, split):\n        path = os.path.join(data_dir, \"{}.tsv\".format(self.dataset_project[split]))\n        examples = []\n        with open(path, encoding='utf8') as f:\n            reader = csv.reader(f, delimiter='\\t', quoting=csv.QUOTE_NONE)\n            next(reader, None)\n            for row in reader:\n                label, headline, body = self.get_label_id(row[-1]), row[8], row[9]\n                text_a = headline.replace('\\\\', ' ')\n                text_b = body.replace('\\\\', ' ')\n                example = InputExample(\n                    guid=str(0), text_a=text_a, text_b=text_b, label=label)\n                examples.append(example)\n\n        return examples\n\nclass MnlimProcessor(DataProcessor):\n    # TODO Test needed\n    dataset_project = {\"train\": \"train\",\n                        \"dev\": \"train\",\n                        \"test\": \"dev_matched\"\n                        }\n    def __init__(self):\n        super().__init__()\n        self.labels = [\"contradiction\", \"entailment\", \"neutral\"]\n\n    def get_examples(self, data_dir, split):\n        path = os.path.join(data_dir, \"{}.tsv\".format(self.dataset_project[split]))\n        examples = []\n        with open(path, encoding='utf8') as f:\n            reader = csv.reader(f, delimiter='\\t', quoting=csv.QUOTE_NONE)\n            next(reader, None)\n            for row in reader:\n                label, headline, body = self.get_label_id(row[-1]), row[8], row[9]\n                text_a = headline.replace('\\\\', ' ')\n                text_b = body.replace('\\\\', ' ')\n                example = InputExample(\n                    guid=str(0), text_a=text_a, text_b=text_b, label=label)\n                examples.append(example)\n\n        return examples\n\n\nclass AgnewsProcessor(DataProcessor):\n    \"\"\"\n    `AG News <https://arxiv.org/pdf/1509.01626.pdf>`_ is a News Topic classification dataset\n\n    we use dataset provided by `LOTClass <https://github.com/yumeng5/LOTClass>`_\n    \"\"\"\n\n    def __init__(self):\n        super().__init__()\n        self.labels = [\"World\", \"Sports\", \"Business\", \"Tech\"]\n\n    def get_examples(self, data_dir, split):\n        path = os.path.join(data_dir, \"{}.csv\".format(split))\n        examples = []\n        with open(path, encoding='utf8') as f:\n            reader = csv.reader(f, delimiter=',')\n            for idx, row in enumerate(reader):\n                label, headline, body = row\n                text_a = headline.replace('\\\\', ' ')\n                text_b = body.replace('\\\\', ' ')\n                example = InputExample(guid=str(idx), text_a=text_a, text_b=text_b, label=int(label)-1)\n                examples.append(example)\n        return examples\n\nclass DBpediaProcessor(DataProcessor):\n    \"\"\"\n    `Dbpedia <https://aclanthology.org/L16-1532.pdf>`_ is a Wikipedia Topic Classification dataset.\n\n    we use dataset provided by `LOTClass <https://github.com/yumeng5/LOTClass>`_\n    \"\"\"\n\n    def __init__(self):\n        super().__init__()\n        self.labels = [\"company\", \"school\", \"artist\", \"athlete\", \"politics\", \"transportation\", \"building\", \"river\", \"village\", \"animal\", \"plant\", \"album\", \"film\", \"book\",]\n\n    def get_examples(self, data_dir, split):\n        examples = []\n        label_file  = open(os.path.join(data_dir,\"{}_labels.txt\".format(split)),'r')\n        labels  = [int(x.strip()) for x in label_file.readlines()]\n        with open(os.path.join(data_dir,'{}.txt'.format(split)),'r') as fin:\n            for idx, line in enumerate(fin):\n                splited = line.strip().split(\". \")\n                text_a, text_b = splited[0], splited[1:]\n                text_a = text_a+\".\"\n                text_b = \". \".join(text_b)\n                example = InputExample(guid=str(idx), text_a=text_a, text_b=text_b, label=int(labels[idx]))\n                examples.append(example)\n        return examples\n\n\nclass ImdbProcessor(DataProcessor):\n    \"\"\"\n    `IMDB <https://ai.stanford.edu/~ang/papers/acl11-WordVectorsSentimentAnalysis.pdf>`_ is a Movie Review Sentiment Classification dataset.\n\n    we use dataset provided by `LOTClass <https://github.com/yumeng5/LOTClass>`_\n    \"\"\"\n\n    def __init__(self):\n        super().__init__()\n        self.labels = [\"negative\", \"positive\"]\n\n    def get_examples(self, data_dir, split):\n        examples = []\n        label_file = open(os.path.join(data_dir, \"{}_labels.txt\".format(split)), 'r')\n        labels = [int(x.strip()) for x in label_file.readlines()]\n        with open(os.path.join(data_dir, '{}.txt'.format(split)),'r') as fin:\n            for idx, line in enumerate(fin):\n                text_a = line.strip()\n                example = InputExample(guid=str(idx), text_a=text_a, label=int(labels[idx]))\n                examples.append(example)\n        return examples\n\n\n    @staticmethod\n    def get_test_labels_only(data_dir, dirname):\n        label_file  = open(os.path.join(data_dir,dirname,\"{}_labels.txt\".format('test')),'r')\n        labels  = [int(x.strip()) for x in label_file.readlines()]\n        return labels\n\nclass YahooProcessor(DataProcessor):\n    \"\"\"\n    Yahoo! Answers Topic Classification Dataset\n    \"\"\"\n\n    def __init__(self):\n        super().__init__()\n        self.labels = [\"Society & Culture\", \"Science & Mathematics\", \"Health\", \"Education & Reference\", \"Computers & Internet\", \"Sports\", \"Business & Finance\", \"Entertainment & Music\"\n                        ,\"Family & Relationships\", \"Politics & Government\"]\n\n    def get_examples(self, data_dir, split):\n        path = os.path.join(data_dir, \"{}.csv\".format(split))\n        examples = []\n        with open(path, encoding='utf8') as f:\n            reader = csv.reader(f, delimiter=',')\n            for idx, row in enumerate(reader):\n                label, question_title, question_body, answer = row\n                text_a = ' '.join([question_title.replace('\\\\n', ' ').replace('\\\\', ' '),\n                                   question_body.replace('\\\\n', ' ').replace('\\\\', ' ')])\n                text_b = answer.replace('\\\\n', ' ').replace('\\\\', ' ')\n                example = InputExample(guid=str(idx), text_a=text_a, text_b=text_b, label=int(label)-1)\n                examples.append(example)\n        return examples\n\n\nclass SST2Processor(DataProcessor):\n    \"\"\"\n    `SST-2 <https://nlp.stanford.edu/sentiment/index.html>`_ dataset is a dataset for sentiment analysis. It is a modified version containing only binary labels (negative or somewhat negative vs somewhat positive or positive with neutral sentences discarded) on top of the original 5-labeled dataset released first in `Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank <https://aclanthology.org/D13-1170.pdf>`_\n\n    We use the data released in `Making Pre-trained Language Models Better Few-shot Learners (Gao et al. 2020) <https://arxiv.org/pdf/2012.15723.pdf>`_\n\n    \"\"\"\n    dataset_project = {\"train\": \"train\",\n                        \"dev\": \"train\",\n                        \"test\": \"dev\"\n                        }\n    def __init__(self):\n        super().__init__()\n        self.labels = ['0', '1']\n\n    def get_examples(self, data_dir, split):\n        path = os.path.join(data_dir, f\"{self.dataset_project[split]}.tsv\")\n        examples = []\n        with open(path, encoding='utf-8')as f:\n            lines = f.readlines()\n            for idx, line in enumerate(lines[1:]):\n                linelist = line.strip().split('\\t')\n                text_a = linelist[0]\n                label = linelist[1]\n                guid = \"%s-%s\" % (split, idx)\n                example = InputExample(guid=guid, text_a=text_a, label=self.get_label_id(label))\n                examples.append(example)\n        return examples\n\n\nclass SnliProcessor(DataProcessor):\n    dataset_project = {\"train\": \"snli_1.0_train\",\n                        \"dev\": \"snli_1.0_train\",\n                        \"test\": \"snli_1.0_dev\"\n                        }\n    def __init__(self):\n        super().__init__()\n        self.labels = [\"contradiction\", \"entailment\", \"neutral\"]\n\n    def get_examples(self, data_dir, split):\n        path = os.path.join(data_dir, \"{}.jsonl\".format(self.dataset_project[split]))\n        examples = []\n        with open(path)as f:\n            lines = f.readlines()\n            for idx, line in enumerate(lines):\n                data = json.loads(line)\n                text_a = data[\"sentence1\"]\n                text_b = data[\"sentence2\"]\n                if data[\"gold_label\"] not in self.labels:\n                    continue\n                label = self.get_label_id(data[\"gold_label\"])\n                example = InputExample(\n                        guid=str(idx), text_a=text_a, text_b=text_b, label=label)\n                examples.append(example)\n        return examples\n\nclass YelpProcessor(DataProcessor):\n    dataset_project = {\"train\": \"train\",\n                        \"dev\": \"train\",\n                        \"test\": \"test\"\n                        }\n    def __init__(self):\n        super().__init__()\n        self.labels = [\"1\", \"2\"]\n\n    def get_examples(self, data_dir, split):\n        path = os.path.join(data_dir, \"{}.csv\".format(self.dataset_project[split]))\n        df = pd.read_csv(path, header=None)\n        examples = []\n        for idx, (label, text) in enumerate(zip(df[0], df[1])):\n            text_a = text\n            label = self.get_label_id(str(label))\n            example = InputExample(\n                    guid=str(idx), text_a=text_a, text_b=\"\", label=label)\n            examples.append(example)\n        return examples\n\n\nclass RteProcessor(DataProcessor):\n    dataset_project = {\"train\": \"train\",\n                        \"dev\": \"train\",\n                        \"test\": \"dev\"\n                        }\n    def __init__(self):\n        super().__init__()\n        self.labels = [\"not_entailment\", \"entailment\"]\n\n    def get_examples(self, data_dir, split):\n        path = os.path.join(data_dir, \"{}.tsv\".format(self.dataset_project[split]))\n        examples = []\n        with open(path)as f:\n            lines = f.readlines()\n            for idx, line in enumerate(lines[1:]):\n                line_list = line.strip().split('\\t')\n                text_a = line_list[-3]\n                text_b = line_list[-2]\n                label = self.get_label_id(line_list[-1])\n                example = InputExample(\n                        guid=str(idx), text_a=text_a, text_b=text_b, label=label)\n                examples.append(example)\n        return examples\n\nclass FewNERDProcessor(DataProcessor):\n    \"\"\"\n    `Few-NERD <https://ningding97.github.io/fewnerd/>`_ a large-scale, fine-grained manually annotated named entity recognition dataset\n\n    It was released together with `Few-NERD: Not Only a Few-shot NER Dataset (Ning Ding et al. 2021) <https://arxiv.org/pdf/2105.07464.pdf>`_\n    \"\"\"\n    def __init__(self):\n        super().__init__()\n        self.labels = [\n            \"person-actor\", \"person-director\", \"person-artist/author\", \"person-athlete\", \"person-politician\", \"person-scholar\", \"person-soldier\", \"person-other\",\n            \"organization-showorganization\", \"organization-religion\", \"organization-company\", \"organization-sportsteam\", \"organization-education\", \"organization-government/governmentagency\", \"organization-media/newspaper\", \"organization-politicalparty\", \"organization-sportsleague\", \"organization-other\",\n            \"location-GPE\", \"location-road/railway/highway/transit\", \"location-bodiesofwater\", \"location-park\", \"location-mountain\", \"location-island\", \"location-other\",\n            \"product-software\", \"product-food\", \"product-game\", \"product-ship\", \"product-train\", \"product-airplane\", \"product-car\", \"product-weapon\", \"product-other\",\n            \"building-theater\", \"building-sportsfacility\", \"building-airport\", \"building-hospital\", \"building-library\", \"building-hotel\", \"building-restaurant\", \"building-other\",\n            \"event-sportsevent\", \"event-attack/battle/war/militaryconflict\", \"event-disaster\", \"event-election\", \"event-protest\", \"event-other\",\n            \"art-music\", \"art-writtenart\", \"art-film\", \"art-painting\", \"art-broadcastprogram\", \"art-other\",\n            \"other-biologything\", \"other-chemicalthing\", \"other-livingthing\", \"other-astronomything\", \"other-god\", \"other-law\", \"other-award\", \"other-disease\", \"other-medical\", \"other-language\", \"other-currency\", \"other-educationaldegree\",\n        ]\n\n    def get_examples(self, data_dir, split):\n        path = os.path.join(data_dir, \"supervised/{}.txt\".format(split))\n        with open(path, encoding='utf8') as f:\n            data = FewNERDProcessor.load_data(f)\n\n            examples = []\n\n            for idx, (xs, ys, spans) in enumerate(data):\n                for span in spans:\n                    text_a = \" \".join(xs)\n                    meta = {\n                        \"entity\": \" \".join(xs[span[0]: span[1]+1])\n                    }\n                    example = InputExample(guid=str(idx), text_a=text_a, meta=meta, label=self.get_label_id(ys[span[0]][2:]))\n                    examples.append(example)\n\n        return examples\n\n    @staticmethod\n    def load_data(file):\n        data = []\n        xs = []\n        ys = []\n        spans = []\n\n        for line in file.readlines():\n            pair = line.split()\n            if pair == []:\n                if xs != []:\n                    data.append((xs, ys, spans))\n                xs = []\n                ys = []\n                spans = []\n            else:\n                xs.append(pair[0])\n\n                tag = pair[-1]\n                if tag != 'O':\n                    if len(ys) == 0 or tag != ys[-1][2:]:\n                        tag = 'B-' + tag\n                        spans.append([len(ys), len(ys)])\n                    else:\n                        tag = 'I-' + tag\n                        spans[-1][-1] = len(ys)\n                ys.append(tag)\n        return data\n\nPROCESSORS = {\n    \"agnews\": AgnewsProcessor,\n    \"dbpedia\": DBpediaProcessor,\n    \"imdb\": ImdbProcessor,\n    \"sst2\": SST2Processor,\n    \"mnli-m\": MnlimProcessor,\n    \"mnli-mm\": MnlimmProcessor,\n    \"yahoo\": YahooProcessor,\n    \"yelp\": YelpProcessor,\n    \"snli\": SnliProcessor,\n    \"rte\": RteProcessor,\n    \"fewnerd\": FewNERDProcessor,\n}","metadata":{"execution":{"iopub.status.busy":"2025-07-30T05:17:36.360927Z","iopub.execute_input":"2025-07-30T05:17:36.361221Z","iopub.status.idle":"2025-07-30T05:17:36.374264Z","shell.execute_reply.started":"2025-07-30T05:17:36.361188Z","shell.execute_reply":"2025-07-30T05:17:36.373580Z"},"executionInfo":{"elapsed":273051,"status":"aborted","timestamp":1752231504945,"user":{"displayName":"Gà Lê văn","userId":"04852251220989386905"},"user_tz":-420},"id":"6z60wNs4fsb9","trusted":true},"outputs":[{"name":"stdout","text":"Overwriting /kaggle/working/DecT/src/process_data.py\n","output_type":"stream"}],"execution_count":34},{"cell_type":"code","source":"%%writefile /kaggle/working/DecT/src/dect_trainer.py\nimport os, shutil\nimport sys\nsys.path.append(\".\")\n\nimport torch\nfrom torch import nn\nfrom tqdm import tqdm\nimport dill\nimport warnings\n\nfrom typing import Callable, Union, Dict\ntry:\n    from typing import OrderedDict\nexcept ImportError:\n    from collections import OrderedDict\nfrom sklearn.metrics import accuracy_score\nfrom openprompt.pipeline_base import PromptForClassification\nfrom openprompt import PromptDataLoader\nfrom openprompt.prompts import *\nfrom openprompt.utils.logging import logger\n\nclass DecTRunner(object):\n    r\"\"\"A runner for DecT\n    This class is specially implemented for classification.\n\n    Args:\n        model (:obj:`PromptForClassification`): One ``PromptForClassification`` object.\n        train_dataloader (:obj:`PromptDataloader`, optional): The dataloader to bachify and process the training data.\n        valid_dataloader (:obj:`PromptDataloader`, optionla): The dataloader to bachify and process the val data.\n        test_dataloader (:obj:`PromptDataloader`, optional): The dataloader to bachify and process the test data.\n    \"\"\"\n    def __init__(self, \n                 model: PromptForClassification,\n                 train_dataloader: Optional[PromptDataLoader] = None,\n                 valid_dataloader: Optional[PromptDataLoader] = None,\n                 test_dataloader: Optional[PromptDataLoader] = None,\n                 calibrate_dataloader: Optional[PromptDataLoader] = None,\n                 id2label: Optional[Dict] = None,\n                 verbalizer = None,\n                 ):\n        self.model = model.cuda()\n        self.train_dataloader = train_dataloader\n        self.valid_dataloader = valid_dataloader\n        self.test_dataloader = test_dataloader\n        self.calibrate_dataloader = calibrate_dataloader\n        self.loss_function = torch.nn.CrossEntropyLoss()\n        self.id2label = id2label\n        self.verbalizer = verbalizer\n        self.clean = True\n    \n    def inference_step(self, batch, batch_idx):\n        label = batch.pop('label')\n        logits = self.model(batch)\n        pred = torch.argmax(logits, dim=-1)\n        return pred.cpu().tolist(), label.cpu().tolist()\n    \n    def inference_epoch(self, split: str): \n        outputs = []\n        scores = {}\n        self.model.eval()\n        with torch.no_grad():\n            data_loader = self.valid_dataloader if split=='validation' else self.test_dataloader\n            model_preds, preds, labels = self.verbalizer.test(self.model, data_loader)\n            # zs_score = accuracy_score(labels, model_preds)\n            score = accuracy_score(labels, preds)\n            scores = {\"dect acc\": score}\n        return scores\n\n    def inference_epoch_end(self, outputs):\n        preds = []\n        labels = []\n        for pred, label in outputs:\n            preds.extend(pred)\n            labels.extend(label)\n\n        score = accuracy_score(labels, preds)\n        return score\n\n    def training_step(self, batch, batch_idx):\n        logits = self.model(batch)\n        loss = self.loss_function(logits, batch['label'])\n        return loss\n\n    def fit(self, ckpt: Optional[str] = None):\n\n        if ckpt:\n            if not self.load_checkpoint(ckpt):\n                logger.warning(\"Train from scratch instead ...\")\n\n        self.model.verbalizer.train_proto(self.model, self.train_dataloader, self.calibrate_dataloader)\n\n        return 0\n    \n    def test(self, ckpt: Optional[str] = None) -> dict:\n        if ckpt:\n            if not self.load_checkpoint(ckpt, load_state = False):\n                exit()\n        return self.inference_epoch(\"test\")\n\n    def run(self, ckpt: Optional[str] = None) -> dict:\n        self.fit(ckpt)\n        return self.test(ckpt = None if self.clean else 'best')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-30T05:17:36.374860Z","iopub.execute_input":"2025-07-30T05:17:36.375035Z","iopub.status.idle":"2025-07-30T05:17:36.396567Z","shell.execute_reply.started":"2025-07-30T05:17:36.375020Z","shell.execute_reply":"2025-07-30T05:17:36.395854Z"}},"outputs":[{"name":"stdout","text":"Overwriting /kaggle/working/DecT/src/dect_trainer.py\n","output_type":"stream"}],"execution_count":35},{"cell_type":"code","source":"%%writefile /kaggle/working/DecT/src/dect_verbalizer.py\nfrom inspect import Parameter\nimport json\nimport time\nfrom os import stat\nimport os\nfrom transformers.file_utils import ModelOutput\nfrom transformers.tokenization_utils import PreTrainedTokenizer\nfrom transformers.utils.dummy_pt_objects import PreTrainedModel\nfrom openprompt.data_utils import InputExample, InputFeatures\nimport re\nfrom openprompt import Verbalizer\nfrom typing import *\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport copy\nfrom transformers.modeling_outputs import CausalLMOutputWithCrossAttentions, Seq2SeqLMOutput, MaskedLMOutput\n\nclass DecTVerbalizer(Verbalizer):\n    r\"\"\"\n    The implementation of the verbalizer in `Prototypical Verbalizer for Prompt-based Few-shot Tuning`\n\n    Args:   \n        tokenizer (:obj:`PreTrainedTokenizer`): The tokenizer of the current pre-trained model to point out the vocabulary.\n        classes (:obj:`List[Any]`): The classes (or labels) of the current task.\n        label_words (:obj:`Union[List[str], List[List[str]], Dict[List[str]]]`, optional): The label words that are projected by the labels.\n        prefix (:obj:`str`, optional): The prefix string of the verbalizer (used in PLMs like RoBERTa, which is sensitive to prefix space)\n        multi_token_handler (:obj:`str`, optional): The handling strategy for multiple tokens produced by the tokenizer.\n        post_log_softmax (:obj:`bool`, optional): Whether to apply log softmax post processing on label_logits. Default to True.\n        lr: (:obj:`float`, optional): The learning rate for prototypes.\n        hidden_size: (:obj:`int`, optional): The dimension of model hidden states.\n        mid_dim: (:obj:`int`, optional): The dimension of prototype embeddings.\n        epochs: (:obj:`int`, optional): The training epochs of prototypes.\n        model_logits_weight: (:obj:`float`, optional): Weight factor (\\lambda) for model logits.\n    \"\"\"\n    def __init__(self, \n                 tokenizer: Optional[PreTrainedTokenizer],\n                 classes: Optional[List] = None,\n                 num_classes: Optional[Sequence[str]] = None,\n                 label_words: Optional[Union[Sequence[str], Mapping[str, str]]] = None,\n                 prefix: Optional[str] = \"\",\n                 multi_token_handler: Optional[str] = \"first\",\n                 post_log_softmax: Optional[bool] = True,\n                 lr: Optional[float] = 1e-3,\n                 hidden_size: Optional[int] = 1024,\n                 mid_dim: Optional[int] = 64,\n                 epochs: Optional[int] = 5,\n                 model_logits_weight: Optional[float] = 1,\n                 save_dir: Optional[str] = None,\n                ):\n        super().__init__(tokenizer=tokenizer, num_classes=num_classes, classes=classes)\n        self.prefix = prefix\n        self.multi_token_handler = multi_token_handler\n        self.post_log_softmax = post_log_softmax\n        self.lr = lr\n        self.mid_dim = mid_dim\n        self.epochs = epochs\n        self.model_logits_weight = model_logits_weight\n        self.save_dir = save_dir\n        self.hidden_dims = hidden_size\n        self.head = nn.Linear(self.hidden_dims, self.mid_dim, bias=False)\n        if label_words is not None: # use label words as an initialization\n            self.label_words = label_words\n        w = torch.empty((self.num_classes, self.mid_dim))\n        nn.init.xavier_uniform_(w)\n        self.proto = nn.Parameter(w, requires_grad=False)\n        r = torch.ones(self.num_classes)\n        self.proto_r = nn.Parameter(r, requires_grad=True)\n        self.optimizer = torch.optim.Adam(self.group_parameters_proto, lr=self.lr)\n        \n    @property\n    def group_parameters_proto(self,):\n        r\"\"\"Include the last layer's parameters\n        \"\"\"\n        return [p for n, p in self.head.named_parameters()] + [self.proto_r]\n\n    def on_label_words_set(self):\n        self.label_words = self.add_prefix(self.label_words, self.prefix)\n        self.generate_parameters()\n        \n    @staticmethod\n    def add_prefix(label_words, prefix):\n        r\"\"\"Add prefix to label words. For example, if a label words is in the middle of a template,\n        the prefix should be ``' '``.\n\n        Args:\n            label_words (:obj:`Union[Sequence[str], Mapping[str, str]]`, optional): The label words that are projected by the labels.\n            prefix (:obj:`str`, optional): The prefix string of the verbalizer.\n        \n        Returns:\n            :obj:`Sequence[str]`: New label words with prefix.\n        \"\"\"\n        new_label_words = []\n        if isinstance(label_words[0], str):\n            label_words = [[w] for w in label_words]  #wrapped it to a list of list of label words.\n\n        for label_words_per_label in label_words:\n            new_label_words_per_label = []\n            for word in label_words_per_label:\n                if word.startswith(\"<!>\"):\n                    new_label_words_per_label.append(word.split(\"<!>\")[1])\n                else:\n                    new_label_words_per_label.append(prefix + word)\n            new_label_words.append(new_label_words_per_label)\n        return new_label_words\n\n    def generate_parameters(self) -> List:\n        r\"\"\"In basic manual template, the parameters are generated from label words directly.\n        In this implementation, the label_words should not be tokenized into more than one token. \n        \"\"\"\n        all_ids = []\n        for words_per_label in self.label_words:\n            ids_per_label = []\n            for word in words_per_label:\n                ids = self.tokenizer.encode(word, add_special_tokens=False)\n                ids_per_label.append(ids)\n            all_ids.append(ids_per_label)\n\n        max_len  = max([max([len(ids) for ids in ids_per_label]) for ids_per_label in all_ids])\n        max_num_label_words = max([len(ids_per_label) for ids_per_label in all_ids])\n        words_ids_mask = torch.zeros(max_num_label_words, max_len)\n        words_ids_mask = [[[1]*len(ids) + [0]*(max_len-len(ids)) for ids in ids_per_label]\n                             + [[0]*max_len]*(max_num_label_words-len(ids_per_label)) \n                             for ids_per_label in all_ids]\n        words_ids = [[ids + [0]*(max_len-len(ids)) for ids in ids_per_label]\n                             + [[0]*max_len]*(max_num_label_words-len(ids_per_label)) \n                             for ids_per_label in all_ids]\n        \n        words_ids_tensor = torch.tensor(words_ids)\n        words_ids_mask = torch.tensor(words_ids_mask)\n        self.label_words_ids = nn.Parameter(words_ids_tensor, requires_grad=False)\n        self.words_ids_mask = nn.Parameter(words_ids_mask, requires_grad=False) # A 3-d mask\n        self.label_words_mask = nn.Parameter(torch.clamp(words_ids_mask.sum(dim=-1), max=1), requires_grad=False)\n\n    def process_hiddens(self, hiddens: torch.Tensor, model_logits, **kwargs):\n        r\"\"\"A whole framework to process the original logits over the vocabulary, which contains four steps: \n        \"\"\"\n        proto_logits = self.sim(self.head(hiddens), self.proto, self.proto_r, model_logits, self.model_logits_weight)\n        return proto_logits\n\n    def project(self,\n                logits: torch.Tensor,\n                **kwargs,\n                ) -> torch.Tensor:\n        r\"\"\"\n        Project the labels, the return value is the normalized (sum to 1) probs of label words. \n        \n        Args:\n            logits (:obj:`torch.Tensor`): The orginal logits of label words.\n        \n        Returns:\n            :obj:`torch.Tensor`: The normalized logits of label words\n        \"\"\"\n\n        label_words_logits = logits[:, self.label_words_ids]\n        label_words_logits = self.handle_multi_token(label_words_logits, self.words_ids_mask)\n        label_words_logits -= 10000*(1-self.label_words_mask)\n        label_words_logits = torch.max(label_words_logits, dim=-1, keepdim=True)[0]\n        return label_words_logits\n\n    def process_logits(self, logits: torch.Tensor, **kwargs):\n        r\"\"\"A whole framework to process the original logits over the vocabulary, which contains four steps: \n\n        (1) Project the logits into logits of label words\n\n        if self.post_log_softmax is True:\n\n            (2) Normalize over all label words\n\n            (3) Calibrate (optional)\n\n        (4) Aggregate (for multiple label words)\n\n        Args:\n            logits (:obj:`torch.Tensor`): The orginal logits.\n        \n        Returns:\n            (:obj:`torch.Tensor`): The final processed logits over the labels (classes).\n        \"\"\"\n        # project\n        label_words_logits = self.project(logits, **kwargs)  #Output: (batch_size, num_classes) or  (batch_size, num_classes, num_label_words_per_label)\n\n        \n        if self.post_log_softmax:\n            # normalize\n            # label_words_probs = self.normalize(label_words_logits)\n\n            # calibrate\n            if  hasattr(self, \"_calibrate_logits\") and self._calibrate_logits is not None:\n                label_words_logits = self.calibrate(label_words_probs=label_words_logits)\n\n            # convert to logits\n            # label_words_logits = torch.log(label_words_probs+1e-15)\n\n        # aggreate\n        label_logits = self.aggregate(label_words_logits)\n        return label_logits\n    \n    def normalize(self, logits: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Given logits regarding the entire vocabulary, return the probs over the label words set.\n        \n        Args:\n            logits (:obj:`Tensor`): The logits over the entire vocabulary.\n\n        Returns:\n            :obj:`Tensor`: The logits over the label words set.\n        \n        \"\"\"\n        batch_size = logits.shape[0]\n        return F.softmax(logits.reshape(batch_size, -1), dim=-1).reshape(*logits.shape)\n\n\n    def aggregate(self, label_words_logits: torch.Tensor) -> torch.Tensor:\n        r\"\"\"Use weight to aggregate the logits of label words.\n\n        Args:\n            label_words_logits(:obj:`torch.Tensor`): The logits of the label words.\n        \n        Returns:\n            :obj:`torch.Tensor`: The aggregated logits from the label words. \n        \"\"\"\n        label_words_logits = (label_words_logits * self.label_words_mask).sum(-1)/self.label_words_mask.sum(-1)\n        return label_words_logits\n\n    def calibrate(self, label_words_probs: torch.Tensor, **kwargs) -> torch.Tensor:\n        r\"\"\"\n        \n        Args:\n            label_words_probs (:obj:`torch.Tensor`): The probability distribution of the label words with the shape of [``batch_size``, ``num_classes``, ``num_label_words_per_class``]\n        \n        Returns:\n            :obj:`torch.Tensor`: The calibrated probability of label words.\n        \"\"\"\n        shape = label_words_probs.shape\n        calibrate_label_words_probs = self._calibrate_logits\n        assert calibrate_label_words_probs.shape[1:] == label_words_probs.shape[1:] \\\n             and calibrate_label_words_probs.shape[0]==1, \"shape not match\"\n        label_words_probs /= (calibrate_label_words_probs+1e-15)\n        \n        return label_words_probs\n\n    def process_outputs(self, outputs: Union[torch.Tensor, torch.Tensor], batch: Union[Dict, InputFeatures], **kwargs):\n        model_logits = self.process_logits(outputs[1])\n        proto_logits = self.process_hiddens(outputs[0], model_logits)\n        return proto_logits\n\n    def gather_outputs(self, outputs: ModelOutput):\n        logits = outputs.logits\n        if isinstance(outputs, Seq2SeqLMOutput):\n            ret = outputs.decoder_hidden_states[-1]\n        elif isinstance(outputs, MaskedLMOutput) or isinstance(outputs, CausalLMOutputWithCrossAttentions):\n            ret = outputs.hidden_states[-1]\n        else:\n            try:\n                ret = outputs.hidden_states[-1]\n            except AttributeError:\n                raise NotImplementedError(f\"Gather outputs method for outputs' type {type(outputs)} not implemented\")\n\n        return ret, logits\n\n    @staticmethod\n    def sim(x, y, r=0, model_logits=0, model_logits_weight=1):\n        x = F.normalize(torch.unsqueeze(x, -2), p=2, dim=-1)\n        dist = torch.norm((x - y), dim=-1) - model_logits * model_logits_weight - r\n        return -dist\n    \n    # Thay đổi hàm loss_func thành:\n    def loss_func(self, x, model_logits, labels):\n        x = F.normalize(x, p=2, dim=-1)  # Chuẩn hóa đầu vào\n        prototypes = F.normalize(self.proto, p=2, dim=-1)  # Chuẩn hóa prototype\n        \n        # Tính khoảng cách cosine thay vì Euclidean\n        logits = torch.mm(x, prototypes.t()) * torch.exp(self.proto_r)\n        \n        # Thêm temperature scaling\n        logits = logits / 0.1\n        \n        # Tính loss với label smoothing\n        loss = F.cross_entropy(logits, labels, label_smoothing=0.1)\n        return loss\n    \n    # Thay đổi hàm sim thành:\n    @staticmethod\n    def sim(x, y, r=0, model_logits=0, model_logits_weight=1):\n        x = F.normalize(x, p=2, dim=-1)\n        y = F.normalize(y, p=2, dim=-1)\n        return torch.mm(x, y.t()) * torch.exp(r) + model_logits * model_logits_weight\n    \n\n    def test(self, model, dataloader):\n        model.eval()\n        model_preds, preds, labels = [], [], []\n        \n        # Process all batches sequentially\n        with torch.no_grad():\n            for batch in dataloader:\n                batch = batch.cuda().to_dict()\n                batch_labels = batch.pop('label').cpu().tolist()\n                labels.extend(batch_labels)\n                \n                outputs = model.prompt_model(batch)\n                outputs = self.gather_outputs(outputs)\n                batch_hidden = model.extract_at_mask(outputs[0], batch)\n                batch_logits = model.extract_at_mask(outputs[1], batch)\n                \n                model_logits = self.process_logits(batch_logits)\n                proto_logits = self.process_hiddens(batch_hidden, model_logits)\n                \n                model_pred = torch.argmax(model_logits, dim=-1)\n                pred = torch.argmax(proto_logits, dim=-1)\n                \n                preds.extend(pred.cpu().tolist())\n                model_preds.extend(model_pred.cpu().tolist())\n    \n        # Verify lengths match\n        assert len(labels) == len(preds), f\"Length mismatch: labels({len(labels)}) vs preds({len(preds)})\"\n        assert len(labels) == len(model_preds), f\"Length mismatch: labels({len(labels)}) vs model_preds({len(model_preds)})\"\n        \n        return model_preds, preds, labels\n    def train_proto(self, model, dataloader, calibrate_dataloader):\n        model.eval()\n        embeds = [[] for _ in range(self.num_classes)]\n        labels = [[] for _ in range(self.num_classes)]\n        model_logits = [[] for _ in range(self.num_classes)]\n        total_num = 0\n        start_time = time.time()\n        with torch.no_grad():\n            # collect calibration logits\n            if calibrate_dataloader is not None:\n                for i, batch in enumerate(calibrate_dataloader):\n                    batch = batch.cuda().to_dict()\n                    outputs = model.prompt_model(batch)\n                    outputs = self.gather_outputs(outputs)\n                    logits = self.project(model.extract_at_mask(outputs[1], batch))\n                    self._calibrate_logits = logits / torch.mean(logits)\n\n            # collect model logits and hidden states\n            for i, batch in enumerate(dataloader):\n                batch = batch.cuda().to_dict()\n                outputs = model.prompt_model(batch)\n                outputs = self.gather_outputs(outputs)\n                hidden, logits = model.extract_at_mask(outputs[0], batch), model.extract_at_mask(outputs[1], batch)\n                logits = self.process_logits(logits)\n                total_num += len(hidden)\n                for j in range(len(hidden)):\n                    label = batch['label'][j]\n                    labels[label].append(label)\n                    embeds[label].append(hidden[j])\n                    model_logits[label].append(logits[j])\n            \n        embeds = list(map(torch.stack, embeds))\n        labels = torch.cat(list(map(torch.stack, labels)))\n        model_logits = torch.cat(list(map(torch.stack, model_logits)))\n\n        dist = list(map(lambda x: torch.norm(self.head(x) - self.head(x.mean(0)), dim=-1).mean(), embeds))\n        self.proto_r.data = torch.stack(dist)\n\n        loss = 0.\n        \n        for epoch in range(self.epochs):\n            x = self.head(torch.cat(embeds))\n            self.optimizer.zero_grad()\n            loss = self.loss_func(x, model_logits, labels)\n            \n            # Kiểm tra loss\n            if torch.isnan(loss).any():\n                print(f\"Epoch {epoch+1}: NaN detected in loss! Skipping backward step.\")\n                continue\n                \n            loss.backward()\n            \n            # Kiểm tra và sửa gradient NaN\n            for name, param in self.named_parameters():\n                if param.grad is not None and torch.isnan(param.grad).any():\n                    print(f\"Epoch {epoch+1}: NaN gradient in {name}\")\n                    param.grad = torch.where(torch.isnan(param.grad), \n                                           torch.zeros_like(param.grad), \n                                           param.grad)\n            \n            # Clipping gradient\n            torch.nn.utils.clip_grad_norm_(self.group_parameters_proto, max_norm=1.0)\n            \n            self.optimizer.step()\n            print(f\"Epoch {epoch+1}/{self.epochs}, Loss: {loss.item()}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-30T05:17:36.397529Z","iopub.execute_input":"2025-07-30T05:17:36.397783Z","iopub.status.idle":"2025-07-30T05:17:36.418232Z","shell.execute_reply.started":"2025-07-30T05:17:36.397766Z","shell.execute_reply":"2025-07-30T05:17:36.417534Z"}},"outputs":[{"name":"stdout","text":"Overwriting /kaggle/working/DecT/src/dect_verbalizer.py\n","output_type":"stream"}],"execution_count":36},{"cell_type":"code","source":"import os\nimport json\n\nprint(\" BẮT ĐẦU: Tạo lại tất cả các file template và verbalizer với các KEY ĐÃ ĐỒNG BỘ...\")\n\nconfigs = {\n    # --- Phân loại câu đơn ---\n    \"agnews\": (\n        '{\"placeholder\":\"text_a\"} {\"placeholder\":\"text_b\"} This news is about {\"mask\"}.',\n        {\n            \"0\": [\"world news\", \"global affairs\", \"international\"],\n            \"1\": [\"sports\", \"athletics\", \"games\"],\n            \"2\": [\"business\", \"finance\", \"economy\", \"markets\"],\n            \"3\": [\"technology\", \"computers\", \"science\", \"innovation\"]\n        }\n    ),\n    \"imdb\": (\n        '{\"placeholder\":\"text_a\"} . This movie is {\"mask\"}.',\n        { \"0\": [\"bad\", \"terrible\"], \"1\": [\"good\", \"great\"] }\n    ),\n    \"sst2\": (\n        '{\"placeholder\":\"text_a\"} . It was {\"mask\"}.',\n        { \"0\": [\"terrible\"], \"1\": [\"great\"] }\n    ),\n    \"yahoo\": (\n        'Topic: {\"mask\"}. Content: {\"placeholder\":\"text_a\"} {\"placeholder\":\"text_b\"}',\n        {\n            \"0\": [\"Society\", \"Culture\"], \"1\": [\"Science\", \"Mathematics\"], \"2\": [\"Health\"],\n            \"3\": [\"Education\", \"Reference\"], \"4\": [\"Computers\", \"Internet\"], \"5\": [\"Sports\"],\n            \"6\": [\"Business\", \"Finance\"], \"7\": [\"Entertainment\", \"Music\"],\n            \"8\": [\"Family\", \"Relationships\"], \"9\": [\"Politics\", \"Government\"]\n        }\n    ),\n    \"yelp\": (\n        'Review: {\"placeholder\":\"text_a\"} sentiment: {\"mask\"}.',\n        { \"1\": [\"bad\", \"terrible\"], \"2\": [\"good\", \"great\"] }\n    ),\n    \"dbpedia\": (\n        'This piece of text is about {\"mask\"}. {\"placeholder\":\"text_a\"} {\"placeholder\":\"text_b\"}',\n        {\n            \"0\": [\"Company\"], \"1\": [\"School\"], \"2\": [\"Artist\"], \"3\": [\"Athlete\"],\n            \"4\": [\"Office Holder\", \"Politician\"], \"5\": [\"Mean of Transportation\", \"Transport\"],\n            \"6\": [\"Building\"], \"7\": [\"Natural Place\", \"River\"], \"8\": [\"Village\"],\n            \"9\": [\"Animal\"], \"10\": [\"Plant\"], \"11\": [\"Album\"], \"12\": [\"Film\"], \"13\": [\"Written Work\", \"Book\"]\n        }\n    ),\n\n    # ==============================================================================\n    # == ĐÂY LÀ PHẦN SỬA LỖI QUAN TRỌNG CHO CÁC DATASET ENTAILMENT ===============\n    # ==============================================================================\n    \"rte\": (\n        '{\"placeholder\":\"text_a\"} ? {\"mask\"}, {\"placeholder\":\"text_b\"}',\n        { \"not_entailment\": [\"No\"], \"entailment\": [\"Yes\"] } # Key khớp với RteProcessor\n    ),\n    \"snli\": (\n        '{\"placeholder\":\"text_a\"} ? {\"mask\"}, {\"placeholder\":\"text_b\"}',\n        { \"contradiction\": [\"contradiction\"], \"entailment\": [\"entailment\"], \"neutral\": [\"neutral\"] } # Key khớp với SnliProcessor\n    ),\n    \"mnli-m\": (\n        '{\"placeholder\":\"text_a\"} ? {\"mask\"}, {\"placeholder\":\"text_b\"}',\n        { \"contradiction\": [\"contradiction\"], \"entailment\": [\"entailment\"], \"neutral\": [\"neutral\"] } # Key khớp với MnlimProcessor\n    ),\n    \"mnli-mm\": (\n        '{\"placeholder\":\"text_a\"} ? {\"mask\"}, {\"placeholder\":\"text_b\"}',\n        { \"contradiction\": [\"contradiction\"], \"entailment\": [\"entailment\"], \"neutral\": [\"neutral\"] } # Key khớp với MnlimmProcessor\n    ),\n    # ==============================================================================\n\n    # --- Nhận dạng thực thể ---\n    \"fewnerd\": (\n        'Find the entity in the following text. The entity is a {\"mask\"}. Text: {\"placeholder\":\"text_a\"}',\n        {\n            \"art-broadcastprogram\": [\"broadcast program\"], \"art-film\": [\"film\"], \"art-music\": [\"music\"],\n            \"art-other\": [\"art\"], \"art-painting\": [\"painting\"], \"art-writtenart\": [\"written art\"],\n            \"building-airport\": [\"airport\"], \"building-hospital\": [\"hospital\"], \"building-hotel\": [\"hotel\"],\n            \"building-library\": [\"library\"], \"building-other\": [\"building\"], \"building-restaurant\": [\"restaurant\"],\n            \"building-sportsfacility\": [\"sports facility\"], \"building-theater\": [\"theater\"],\n            \"event-attack/battle/war/militaryconflict\": [\"military conflict\"], \"event-disaster\": [\"disaster\"],\n            \"event-election\": [\"election\"], \"event-other\": [\"event\"], \"event-protest\": [\"protest\"],\n            \"event-sportsevent\": [\"sports event\"], \"location-GPE\": [\"geopolitical entity\"],\n            \"location-bodiesofwater\": [\"body of water\"], \"location-island\": [\"island\"], \"location-mountain\": [\"mountain\"],\n            \"location-other\": [\"location\"], \"location-park\": [\"park\"], \"location-road/railway/highway/transit\": [\"road\"],\n            \"organization-company\": [\"company\"], \"organization-education\": [\"education organization\"],\n            \"organization-government/governmentagency\": [\"government agency\"], \"organization-media/newspaper\": [\"media organization\"],\n            \"organization-other\": [\"organization\"], \"organization-politicalparty\": [\"political party\"],\n            \"organization-religion\": [\"religious organization\"], \"organization-showorganization\": [\"show organization\"],\n            \"organization-sportsleague\": [\"sports league\"], \"organization-sportsteam\": [\"sports team\"],\n            \"other-astronomything\": [\"astronomical object\"], \"other-award\": [\"award\"],\n            \"other-biologything\": [\"biological thing\"], \"other-chemicalthing\": [\"chemical thing\"],\n            \"other-currency\": [\"currency\"], \"other-disease\": [\"disease\"], \"other-educationaldegree\": [\"educational degree\"],\n            \"other-god\": [\"god\"], \"other-language\": [\"language\"], \"other-law\": [\"law\"],\n            \"other-livingthing\": [\"living thing\"], \"other-medical\": [\"medical\"],\n            \"person-actor\": [\"actor\"], \"person-artist/author\": [\"artist\"], \"person-athlete\": [\"athlete\"],\n            \"person-director\": [\"director\"], \"person-other\": [\"person\"], \"person-politician\": [\"politician\"],\n            \"person-scholar\": [\"scholar\"], \"person-soldier\": [\"soldier\"],\n            \"product-airplane\": [\"airplane\"], \"product-car\": [\"car\"], \"product-food\": [\"food\"],\n            \"product-game\": [\"game\"], \"product-other\": [\"product\"], \"product-ship\": [\"ship\"],\n            \"product-software\": [\"software\"], \"product-train\": [\"train\"], \"product-weapon\": [\"weapon\"]\n        }\n    )\n}\n\n# --- Vòng lặp để tạo file ---\nbase_path = \"/kaggle/working/DecT/scripts\"\nfor dataset_name, (template_content, verbalizer_dict) in configs.items():\n    dir_name = dataset_name.split('-')[0]\n    dataset_path = os.path.join(base_path, dir_name)\n    os.makedirs(dataset_path, exist_ok=True)\n    \n    template_path = os.path.join(dataset_path, \"manual_template.txt\")\n    with open(template_path, 'w') as f:\n        f.write(template_content)\n    \n    verbalizer_path = os.path.join(dataset_path, \"manual_verbalizer.json\")\n    with open(verbalizer_path, 'w') as f:\n        json.dump(verbalizer_dict, f, indent=4)\n    print(f\"✅ Đã tạo lại cấu hình cho: {dir_name}\")\n\nprint(\"🎉 HOÀN TẤT: Tạo lại tất cả các file template và verbalizer!\")\nprint(\"-\" * 50)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-30T05:17:36.419119Z","iopub.execute_input":"2025-07-30T05:17:36.419400Z","iopub.status.idle":"2025-07-30T05:17:36.442876Z","shell.execute_reply.started":"2025-07-30T05:17:36.419372Z","shell.execute_reply":"2025-07-30T05:17:36.442089Z"}},"outputs":[{"name":"stdout","text":" BẮT ĐẦU: Tạo lại tất cả các file template và verbalizer với các KEY ĐÃ ĐỒNG BỘ...\n✅ Đã tạo lại cấu hình cho: agnews\n✅ Đã tạo lại cấu hình cho: imdb\n✅ Đã tạo lại cấu hình cho: sst2\n✅ Đã tạo lại cấu hình cho: yahoo\n✅ Đã tạo lại cấu hình cho: yelp\n✅ Đã tạo lại cấu hình cho: dbpedia\n✅ Đã tạo lại cấu hình cho: rte\n✅ Đã tạo lại cấu hình cho: snli\n✅ Đã tạo lại cấu hình cho: mnli\n✅ Đã tạo lại cấu hình cho: mnli\n✅ Đã tạo lại cấu hình cho: fewnerd\n🎉 HOÀN TẤT: Tạo lại tất cả các file template và verbalizer!\n--------------------------------------------------\n","output_type":"stream"}],"execution_count":37},{"cell_type":"code","source":"%%writefile /kaggle/working/DecT/src/run_dect.py\nimport os\nimport sys\nsys.path.append(\".\")\n\nimport argparse\nimport csv\nfrom re import template\nfrom process_data import load_dataset\nfrom dect_verbalizer import DecTVerbalizer\nfrom dect_trainer import DecTRunner\nfrom openprompt.prompts import ManualTemplate\nfrom openprompt.pipeline_base import PromptForClassification\nfrom openprompt.utils.reproduciblity import set_seed\nfrom openprompt import PromptDataLoader\nfrom openprompt.data_utils import FewShotSampler\nfrom openprompt.plms import load_plm, LMTokenizerWrapper, T5LMTokenizerWrapper, T5TokenizerWrapper\nfrom openprompt.data_utils.utils import InputExample\nfrom transformers import LlamaTokenizer, LlamaForCausalLM, AutoTokenizer, AutoModelForCausalLM, T5Tokenizer, AutoModelForSeq2SeqLM, AutoConfig\nimport torch\nimport numpy as np\n\nparser = argparse.ArgumentParser(\"\")\n\nparser.add_argument(\"--model\", type=str, default='gpt2', help=\"plm name\")\nparser.add_argument(\"--size\", type=str, default='large', help=\"plm size\")\nparser.add_argument(\"--type\", type=str, default='mlm', help=\"plm type\")\nparser.add_argument(\"--model_name_or_path\", default='gpt2', help=\"default load from Huggingface cache\")\nparser.add_argument(\"--shot\", type=int, default=1, help=\"number of shots per class\")\nparser.add_argument(\"--seed\", type=int, default=0, help=\"data sampling seed\")\nparser.add_argument(\"--template_id\", type=int, default=0)\nparser.add_argument(\"--dataset\", type=str, default='sst2')\nparser.add_argument(\"--max_epochs\", type=int, default=30, help=\"number of training epochs for DecT\")\nparser.add_argument(\"--batch_size\", type=int, default=64, help=\"batch size for train and test\")\nparser.add_argument(\"--proto_dim\", type=int, default=128, help=\"hidden dimension for DecT prototypes\")\nparser.add_argument(\"--model_logits_weight\", type=float, default=1, help=\"weight factor (\\lambda) for model logits\")\nparser.add_argument(\"--lr\", default=0.001, type=float, help=\"learning rate for DecT\")\nargs = parser.parse_args()\n\ndef load_model(name, size, path):\n    if name == \"llama\":\n        tokenizer = LlamaTokenizer.from_pretrained(path)\n        tokenizer.bos_token_id = 1\n        tokenizer.eos_token_id = 2\n        tokenizer.pad_token_id = 0\n        model = LlamaForCausalLM.from_pretrained(path)\n        wrapper = LMTokenizerWrapper\n        if size == \"7b\":\n            hidden_size = 4096\n        elif size == \"13b\":\n            hidden_size = 5120\n    elif name == \"alpaca\":\n        tokenizer = LlamaTokenizer.from_pretrained(path)\n        tokenizer.bos_token_id = 1\n        tokenizer.eos_token_id = 2\n        tokenizer.pad_token_id = 0\n        model = AutoModelForCausalLM.from_pretrained(path)\n        wrapper = LMTokenizerWrapper\n        hidden_size = 4096\n    elif name == \"vicuna\":\n        tokenizer = LlamaTokenizer.from_pretrained(path)\n        tokenizer.bos_token_id = 1\n        tokenizer.eos_token_id = 2\n        tokenizer.pad_token_id = 0\n        model = AutoModelForCausalLM.from_pretrained(path)\n        wrapper = LMTokenizerWrapper\n        hidden_size = 5120\n    else:\n        model, tokenizer, model_config, wrapper = load_plm(args.model, args.model_name_or_path)\n        hidden_size = model_config.hidden_size\n    \n    return model, tokenizer, hidden_size, wrapper\n\n\ndef build_dataloader(dataset, template, verbalizer, tokenizer, tokenizer_wrapper_class, batch_size):\n    dataloader = PromptDataLoader(\n        dataset = dataset, \n        template = template, \n        verbalizer = verbalizer, \n        tokenizer = tokenizer, \n        tokenizer_wrapper_class=tokenizer_wrapper_class, \n        decoder_max_length=128,\n        batch_size = batch_size,\n        shuffle = True,\n    )\n\n    return dataloader\n\n\ndef main():\n    # set hyperparameter\n    datasets = args.dataset.split(',')\n    data_path = datasets[0].split('-')[0]\n    if data_path == \"mnli\":\n        args.model_logits_weight = 1\n    elif data_path == \"fewnerd\":\n        args.model_logits_weight = 1/16\n    else:\n        args.model_logits_weight = 1/args.shot\n\n    # load dataset. The valid_dataset can be None\n    train_dataset, valid_dataset, test_dataset, Processor = load_dataset(datasets[0])\n    \n    # Use full test set (removed the 20% sampling)\n    if test_dataset:\n        print(f\"Using full test set with size: {len(test_dataset)}\")\n    \n    set_seed(123)\n    # sample data\n    sampler = FewShotSampler(\n        num_examples_per_label = args.shot,\n        also_sample_dev = True,\n        num_examples_per_label_dev = args.shot)\n\n    train_sampled_dataset, valid_sampled_dataset = sampler(\n        train_dataset = train_dataset,\n        valid_dataset = valid_dataset,\n        seed = args.seed\n    )\n\n    plm, tokenizer, hidden_size, plm_wrapper_class = load_model(args.model, args.size, args.model_name_or_path)\n    # make dir to save hiddens and logits\n    save_dir = f\"vectors/{args.model}/{args.size}/{args.dataset}\"\n    os.makedirs(save_dir, exist_ok=True)\n\n    # define template and verbalizer\n    # define prompt\n    template = ManualTemplate(\n        tokenizer=tokenizer).from_file(f\"scripts/{args.type}/{data_path}/manual_template.txt\", choice=args.template_id)\n\n    verbalizer = DecTVerbalizer(\n        tokenizer=tokenizer, \n        classes=Processor.labels, \n        hidden_size=hidden_size, \n        lr=args.lr, \n        mid_dim=args.proto_dim, \n        epochs=args.max_epochs, \n        model_logits_weight=args.model_logits_weight,\n        save_dir=save_dir).from_file(f\"scripts/{args.type}/{data_path}/manual_verbalizer.json\")\n        \n    # load prompt's pipeline model\n    prompt_model = PromptForClassification(plm, template, verbalizer)\n            \n    # process data and get data_loader\n    train_dataloader = build_dataloader(train_sampled_dataset, template, verbalizer, tokenizer, plm_wrapper_class, args.batch_size) if train_dataset else None\n    valid_dataloader = build_dataloader(valid_sampled_dataset, template, verbalizer, tokenizer, plm_wrapper_class, args.batch_size) if valid_dataset else None\n\n    test_dataloader = build_dataloader(test_dataset, template, verbalizer, tokenizer, plm_wrapper_class, args.batch_size) if test_dataset else None\n\n    calibrate_dataloader =  PromptDataLoader(\n        dataset = [InputExample(guid=str(0), text_a=\"\", text_b=\"\", meta={\"entity\": \"It\"}, label=0)], \n        template = template, \n        tokenizer = tokenizer, \n        tokenizer_wrapper_class=plm_wrapper_class,\n        decoder_max_length=128,\n    )\n\n    runner = DecTRunner(\n        model = prompt_model,\n        train_dataloader = train_dataloader,\n        valid_dataloader = valid_dataloader,\n        test_dataloader = test_dataloader,\n        calibrate_dataloader = calibrate_dataloader,\n        id2label = Processor.id2label,\n        verbalizer = verbalizer\n    )                                   \n\n        \n    res = runner.run()\n    print('Dataset: {} | Shot: {} |'.format(args.dataset, args.shot))\n    print(res)\n    return res[\"dect acc\"]\n    \n\n\nif __name__ == \"__main__\":\n    res = []\n    for seed in range(1):\n        args.seed = seed\n        res.append(main())\n    print('Average: {}'.format(sum(res)/len(res)*100))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-30T05:17:36.443796Z","iopub.execute_input":"2025-07-30T05:17:36.444129Z","iopub.status.idle":"2025-07-30T05:17:36.461112Z","shell.execute_reply.started":"2025-07-30T05:17:36.444106Z","shell.execute_reply":"2025-07-30T05:17:36.460403Z"}},"outputs":[{"name":"stdout","text":"Overwriting /kaggle/working/DecT/src/run_dect.py\n","output_type":"stream"}],"execution_count":38},{"cell_type":"markdown","source":"--model gpt2\nMô hình nền (Base Model): Chọn gpt2 làm mô hình ngôn ngữ nền. Script sẽ tải mô hình đã được huấn luyện sẵn của GPT-2 để tinh chỉnh (fine-tune) cho nhiệm vụ mới.\n\n--size base\nKích thước mô hình (Model Size): Chọn phiên bản base của GPT-2. Các mô hình thường có nhiều kích thước (ví dụ: small, base, large). base là kích thước tiêu chuẩn, cân bằng giữa hiệu năng và tài nguyên yêu cầu.\n\n--type chat\nLoại mẫu câu (Prompt Type): Xác định cách dữ liệu được định dạng để đưa vào mô hình. chat có thể định dạng dữ liệu theo kiểu hỏi-đáp hoặc một cấu trúc hội thoại, giúp mô hình hiểu rõ hơn về nhiệm vụ cần làm.\n\n--dataset dbpedia\nBộ dữ liệu (Dataset): Chỉ định rằng script sẽ huấn luyện và đánh giá trên bộ dữ liệu dbpedia. Đây là bài toán phân loại văn bản thành 14 chủ đề khác nhau (công ty, nghệ sĩ, phim ảnh...).\n\n--max_epochs 3\nSố Epoch tối đa (Maximum Epochs): Mô hình sẽ học trên toàn bộ dữ liệu huấn luyện tối đa là 3 lần. Một epoch là một lượt đi qua hết tất cả các mẫu trong tập huấn luyện.\n\n--batch_size 32\nKích thước lô (Batch Size): Trong mỗi bước huấn luyện, mô hình sẽ xử lý đồng thời 32 mẫu dữ liệu. Batch size lớn hơn có thể tăng tốc độ huấn luyện nhưng đòi hỏi nhiều bộ nhớ VRAM hơn.\n\n--proto_dim 768\nSố chiều của Prototype (Prototype Dimension): Đặt kích thước của mỗi vector \"hình mẫu\" (prototype) là 768. Con số này được chọn để khớp với số chiều ẩn (hidden dimension) của mô hình gpt2-base, giúp việc so sánh giữa vector văn bản và vector prototype trở nên hiệu quả.\n\n--lr 1e-4\nTốc độ học (Learning Rate): Đặt tốc độ học là 1e-4 (tức 0.0001). Đây là tham số quan trọng, quyết định \"bước đi\" lớn hay nhỏ khi mô hình cập nhật trọng số của nó. 1e-4 là một giá trị khởi đầu tốt cho việc fine-tuning.\n\n--seed 42\nHạt giống ngẫu nhiên (Random Seed): Đặt một con số cố định (42) để khởi tạo tất cả các quá trình ngẫu nhiên (như khởi tạo trọng số, xáo trộn dữ liệu). Điều này đảm bảo rằng mỗi lần bạn chạy lại lệnh này, bạn sẽ nhận được kết quả y hệt nhau, giúp cho việc thí nghiệm có thể tái lặp.\n\n--template_id 0\nMã mẫu câu (Template ID): Chọn mẫu câu (prompt template) có mã số là 0. Trong cùng một --type (ví dụ chat), có thể có nhiều cách diễn đạt khác nhau. Tham số này cho phép chọn một cách diễn đạt cụ thể. 0 thường là mẫu mặc định.","metadata":{"id":"_3SgAe2OGrDx"}},{"cell_type":"markdown","source":"## GPT 2","metadata":{}},{"cell_type":"markdown","source":"## AGNews","metadata":{"id":"Pg7bllHbGrDx"}},{"cell_type":"code","source":"%cd /kaggle/working/DecT","metadata":{"execution":{"iopub.status.busy":"2025-07-28T12:01:01.891198Z","iopub.execute_input":"2025-07-28T12:01:01.891923Z","iopub.status.idle":"2025-07-28T12:01:01.896573Z","shell.execute_reply.started":"2025-07-28T12:01:01.891864Z","shell.execute_reply":"2025-07-28T12:01:01.895834Z"},"executionInfo":{"elapsed":273050,"status":"aborted","timestamp":1752231504947,"user":{"displayName":"Gà Lê văn","userId":"04852251220989386905"},"user_tz":-420},"id":"cGtk-crfGrDx","trusted":true},"outputs":[{"name":"stdout","text":"/kaggle/working/DecT\n","output_type":"stream"}],"execution_count":47},{"cell_type":"code","source":"!python src/run_dect.py \\\n--model gpt2 \\\n--size large \\\n--type chat \\\n--dataset agnews \\\n--max_epochs 30 \\\n--batch_size 16 \\\n--proto_dim 512 \\\n--lr 5e-4 \\\n--seed 42 \\\n--template_id 0\n","metadata":{"execution":{"iopub.status.busy":"2025-07-28T12:05:07.725625Z","iopub.execute_input":"2025-07-28T12:05:07.725966Z","iopub.status.idle":"2025-07-28T12:08:55.912662Z","shell.execute_reply.started":"2025-07-28T12:05:07.725943Z","shell.execute_reply":"2025-07-28T12:08:55.911776Z"},"executionInfo":{"elapsed":273057,"status":"aborted","timestamp":1752231504956,"user":{"displayName":"Gà Lê văn","userId":"04852251220989386905"},"user_tz":-420},"id":"bo2pyz0Igt9H","trusted":true},"outputs":[{"name":"stdout","text":"/usr/local/lib/python3.11/dist-packages/transformers/generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.\n  warnings.warn(\n2025-07-28 12:05:12.361674: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1753704312.386416     867 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1753704312.393685     867 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nUsing full test set with size: 7600\n/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\nUsing pad_token, but it is not set yet.\ntokenizing: 4it [00:00, 571.33it/s]\ntokenizing: 7600it [00:09, 838.86it/s]\ntokenizing: 1it [00:00, 1081.01it/s]\nEpoch 1/30, Loss: 1.4594558477401733\nEpoch 2/30, Loss: 1.3658769130706787\nEpoch 3/30, Loss: 1.313360571861267\nEpoch 4/30, Loss: 1.284614086151123\nEpoch 5/30, Loss: 1.248258113861084\nEpoch 6/30, Loss: 1.2038302421569824\nEpoch 7/30, Loss: 1.1577601432800293\nEpoch 8/30, Loss: 1.1149883270263672\nEpoch 9/30, Loss: 1.0755645036697388\nEpoch 10/30, Loss: 1.0346221923828125\nEpoch 11/30, Loss: 0.9904517531394958\nEpoch 12/30, Loss: 0.9438412189483643\nEpoch 13/30, Loss: 0.896682858467102\nEpoch 14/30, Loss: 0.8503357172012329\nEpoch 15/30, Loss: 0.804561972618103\nEpoch 16/30, Loss: 0.7582975625991821\nEpoch 17/30, Loss: 0.7110336422920227\nEpoch 18/30, Loss: 0.6633296608924866\nEpoch 19/30, Loss: 0.6164932250976562\nEpoch 20/30, Loss: 0.5718008875846863\nEpoch 21/30, Loss: 0.5298237800598145\nEpoch 22/30, Loss: 0.49057191610336304\nEpoch 23/30, Loss: 0.45427533984184265\nEpoch 24/30, Loss: 0.42184317111968994\nEpoch 25/30, Loss: 0.39461979269981384\nEpoch 26/30, Loss: 0.37373673915863037\nEpoch 27/30, Loss: 0.35983383655548096\nEpoch 28/30, Loss: 0.35272476077079773\nEpoch 29/30, Loss: 0.35174861550331116\nEpoch 30/30, Loss: 0.355624794960022\nDataset: agnews | Shot: 1 |\n{'dect acc': 0.5368421052631579}\nAverage: 53.68421052631579\n","output_type":"stream"}],"execution_count":51},{"cell_type":"markdown","source":"## DBPedia","metadata":{"id":"ECu7ESXHGrDz"}},{"cell_type":"code","source":"# !python src/run_dect.py \\\n# --model gpt2 \\\n# --size base \\\n# --type chat \\\n# --dataset dbpedia \\\n# --max_epochs 3 \\\n# --batch_size 32 \\\n# --proto_dim 768 \\\n# --lr 1e-4 \\\n# --seed 42 \\\n# --template_id 0","metadata":{"execution":{"iopub.execute_input":"2025-07-23T03:49:55.619406Z","iopub.status.busy":"2025-07-23T03:49:55.619147Z","iopub.status.idle":"2025-07-23T03:49:55.623377Z","shell.execute_reply":"2025-07-23T03:49:55.622637Z","shell.execute_reply.started":"2025-07-23T03:49:55.619365Z"},"executionInfo":{"elapsed":273058,"status":"aborted","timestamp":1752231504958,"user":{"displayName":"Gà Lê văn","userId":"04852251220989386905"},"user_tz":-420},"id":"utBabbYHrFH_","trusted":true},"outputs":[],"execution_count":37},{"cell_type":"markdown","source":"## FewNERD","metadata":{"id":"HPi82fbJGrD1"}},{"cell_type":"code","source":"%cd /kaggle/working/DecT","metadata":{"execution":{"iopub.status.busy":"2025-07-28T12:15:14.516913Z","iopub.execute_input":"2025-07-28T12:15:14.517235Z","iopub.status.idle":"2025-07-28T12:15:14.523197Z","shell.execute_reply.started":"2025-07-28T12:15:14.517207Z","shell.execute_reply":"2025-07-28T12:15:14.522450Z"},"executionInfo":{"elapsed":273057,"status":"aborted","timestamp":1752231504959,"user":{"displayName":"Gà Lê văn","userId":"04852251220989386905"},"user_tz":-420},"id":"87qePDRfGrD1","trusted":true},"outputs":[{"name":"stdout","text":"/kaggle/working/DecT\n","output_type":"stream"}],"execution_count":52},{"cell_type":"code","source":"!python src/run_dect.py \\\n--model gpt2 \\\n--size large \\\n--type chat \\\n--dataset fewnerd \\\n--max_epochs 30 \\\n--batch_size 16 \\\n--proto_dim 512 \\\n--lr 5e-4 \\\n--seed 42 \\\n--template_id 0\n","metadata":{"execution":{"iopub.status.busy":"2025-07-28T12:15:16.570924Z","iopub.execute_input":"2025-07-28T12:15:16.571193Z","iopub.status.idle":"2025-07-28T12:57:44.211480Z","shell.execute_reply.started":"2025-07-28T12:15:16.571174Z","shell.execute_reply":"2025-07-28T12:57:44.210581Z"},"executionInfo":{"elapsed":273055,"status":"aborted","timestamp":1752231504960,"user":{"displayName":"Gà Lê văn","userId":"04852251220989386905"},"user_tz":-420},"id":"qCozDIJ9kgkh","trusted":true},"outputs":[{"name":"stdout","text":"/usr/local/lib/python3.11/dist-packages/transformers/generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.\n  warnings.warn(\n2025-07-28 12:15:21.379114: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1753704921.406426     886 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1753704921.415382     886 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nUsing full test set with size: 96902\n/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\nUsing pad_token, but it is not set yet.\ntokenizing: 66it [00:00, 770.26it/s]\ntokenizing: 96902it [01:32, 1043.73it/s]\ntokenizing: 1it [00:00, 1259.17it/s]\nEpoch 1/30, Loss: 4.293471336364746\nEpoch 2/30, Loss: 4.250784873962402\nEpoch 3/30, Loss: 4.212082386016846\nEpoch 4/30, Loss: 4.176746845245361\nEpoch 5/30, Loss: 4.144201278686523\nEpoch 6/30, Loss: 4.113861083984375\nEpoch 7/30, Loss: 4.085161209106445\nEpoch 8/30, Loss: 4.057587146759033\nEpoch 9/30, Loss: 4.030674457550049\nEpoch 10/30, Loss: 4.004011631011963\nEpoch 11/30, Loss: 3.9772136211395264\nEpoch 12/30, Loss: 3.9499077796936035\nEpoch 13/30, Loss: 3.921712636947632\nEpoch 14/30, Loss: 3.892228841781616\nEpoch 15/30, Loss: 3.8610317707061768\nEpoch 16/30, Loss: 3.8276615142822266\nEpoch 17/30, Loss: 3.7916336059570312\nEpoch 18/30, Loss: 3.7525434494018555\nEpoch 19/30, Loss: 3.7101333141326904\nEpoch 20/30, Loss: 3.6640989780426025\nEpoch 21/30, Loss: 3.6140830516815186\nEpoch 22/30, Loss: 3.5596683025360107\nEpoch 23/30, Loss: 3.500375270843506\nEpoch 24/30, Loss: 3.435655355453491\nEpoch 25/30, Loss: 3.364894151687622\nEpoch 26/30, Loss: 3.287414073944092\nEpoch 27/30, Loss: 3.2024924755096436\nEpoch 28/30, Loss: 3.10939621925354\nEpoch 29/30, Loss: 3.0074362754821777\nEpoch 30/30, Loss: 2.8960533142089844\nDataset: fewnerd | Shot: 1 |\n{'dect acc': 0.14179273905595344}\nAverage: 14.179273905595343\n","output_type":"stream"}],"execution_count":53},{"cell_type":"markdown","source":"## IMDB","metadata":{"id":"99qzSdBPGrD1"}},{"cell_type":"code","source":"%cd /kaggle/working/DecT","metadata":{"execution":{"iopub.status.busy":"2025-07-28T12:57:44.213447Z","iopub.execute_input":"2025-07-28T12:57:44.213706Z","iopub.status.idle":"2025-07-28T12:57:44.220085Z","shell.execute_reply.started":"2025-07-28T12:57:44.213683Z","shell.execute_reply":"2025-07-28T12:57:44.219253Z"},"executionInfo":{"elapsed":273053,"status":"aborted","timestamp":1752231504961,"user":{"displayName":"Gà Lê văn","userId":"04852251220989386905"},"user_tz":-420},"id":"LFSmEauTGrD2","trusted":true},"outputs":[{"name":"stdout","text":"/kaggle/working/DecT\n","output_type":"stream"}],"execution_count":54},{"cell_type":"code","source":"!python src/run_dect.py \\\n--model gpt2 \\\n--size large \\\n--type chat \\\n--dataset imdb \\\n--max_epochs 30 \\\n--batch_size 16 \\\n--proto_dim 512 \\\n--lr 5e-4 \\\n--seed 42 \\\n--template_id 0\n","metadata":{"execution":{"iopub.status.busy":"2025-07-28T12:57:44.220894Z","iopub.execute_input":"2025-07-28T12:57:44.221134Z","iopub.status.idle":"2025-07-28T13:09:37.979752Z","shell.execute_reply.started":"2025-07-28T12:57:44.221117Z","shell.execute_reply":"2025-07-28T13:09:37.978852Z"},"executionInfo":{"elapsed":273051,"status":"aborted","timestamp":1752231504962,"user":{"displayName":"Gà Lê văn","userId":"04852251220989386905"},"user_tz":-420},"id":"OArS_Xl2kjWr","trusted":true},"outputs":[{"name":"stdout","text":"/usr/local/lib/python3.11/dist-packages/transformers/generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.\n  warnings.warn(\n2025-07-28 12:57:49.061351: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1753707469.088354     907 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1753707469.096172     907 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nUsing full test set with size: 25000\n/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\nUsing pad_token, but it is not set yet.\ntokenizing: 2it [00:00, 219.30it/s]\ntokenizing: 57it [00:00, 284.38it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1101 > 1024). Running this sequence through the model will result in indexing errors\ntokenizing: 25000it [01:00, 414.42it/s]\ntokenizing: 1it [00:00, 1227.84it/s]\nEpoch 1/30, Loss: 0.7113558650016785\nEpoch 2/30, Loss: 0.6829575300216675\nEpoch 3/30, Loss: 0.6587924957275391\nEpoch 4/30, Loss: 0.6324108839035034\nEpoch 5/30, Loss: 0.6142815351486206\nEpoch 6/30, Loss: 0.5913254618644714\nEpoch 7/30, Loss: 0.5672570466995239\nEpoch 8/30, Loss: 0.5471802353858948\nEpoch 9/30, Loss: 0.5252879858016968\nEpoch 10/30, Loss: 0.5017327666282654\nEpoch 11/30, Loss: 0.48038485646247864\nEpoch 12/30, Loss: 0.4593227505683899\nEpoch 13/30, Loss: 0.43673521280288696\nEpoch 14/30, Loss: 0.4147734045982361\nEpoch 15/30, Loss: 0.39423272013664246\nEpoch 16/30, Loss: 0.3732977509498596\nEpoch 17/30, Loss: 0.3521720767021179\nEpoch 18/30, Loss: 0.3324151039123535\nEpoch 19/30, Loss: 0.31364312767982483\nEpoch 20/30, Loss: 0.2950619161128998\nEpoch 21/30, Loss: 0.2775663435459137\nEpoch 22/30, Loss: 0.2618755102157593\nEpoch 23/30, Loss: 0.24747870862483978\nEpoch 24/30, Loss: 0.23438939452171326\nEpoch 25/30, Loss: 0.22338034212589264\nEpoch 26/30, Loss: 0.21457138657569885\nEpoch 27/30, Loss: 0.20761314034461975\nEpoch 28/30, Loss: 0.2026030421257019\nEpoch 29/30, Loss: 0.19972072541713715\nEpoch 30/30, Loss: 0.19867047667503357\nDataset: imdb | Shot: 1 |\n{'dect acc': 0.65796}\nAverage: 65.79599999999999\n","output_type":"stream"}],"execution_count":55},{"cell_type":"markdown","source":"## MNLI","metadata":{"id":"w49Xt438GrD3"}},{"cell_type":"code","source":"%cd /kaggle/working/DecT","metadata":{"execution":{"iopub.status.busy":"2025-07-28T13:09:37.981848Z","iopub.execute_input":"2025-07-28T13:09:37.982133Z","iopub.status.idle":"2025-07-28T13:09:37.988141Z","shell.execute_reply.started":"2025-07-28T13:09:37.982110Z","shell.execute_reply":"2025-07-28T13:09:37.987275Z"},"executionInfo":{"elapsed":273050,"status":"aborted","timestamp":1752231504963,"user":{"displayName":"Gà Lê văn","userId":"04852251220989386905"},"user_tz":-420},"id":"WwDPiFBDGrD3","trusted":true},"outputs":[{"name":"stdout","text":"/kaggle/working/DecT\n","output_type":"stream"}],"execution_count":56},{"cell_type":"code","source":"!python src/run_dect.py \\\n--model gpt2 \\\n--size large \\\n--type chat \\\n--dataset mnli-m \\\n--max_epochs 30 \\\n--batch_size 16 \\\n--proto_dim 512 \\\n--lr 5e-4 \\\n--seed 42 \\\n--template_id 0\n","metadata":{"execution":{"iopub.status.busy":"2025-07-28T13:09:37.988917Z","iopub.execute_input":"2025-07-28T13:09:37.989192Z","iopub.status.idle":"2025-07-28T13:14:26.243492Z","shell.execute_reply.started":"2025-07-28T13:09:37.989163Z","shell.execute_reply":"2025-07-28T13:14:26.242318Z"},"executionInfo":{"elapsed":273048,"status":"aborted","timestamp":1752231504964,"user":{"displayName":"Gà Lê văn","userId":"04852251220989386905"},"user_tz":-420},"id":"QcaKL4o-kl1J","trusted":true},"outputs":[{"name":"stdout","text":"/usr/local/lib/python3.11/dist-packages/transformers/generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.\n  warnings.warn(\n2025-07-28 13:09:42.987731: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1753708183.013746     926 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1753708183.021404     926 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nUsing full test set with size: 9815\n/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\nUsing pad_token, but it is not set yet.\ntokenizing: 3it [00:00, 683.85it/s]\ntokenizing: 3it [00:00, 1135.03it/s]\ntokenizing: 9815it [00:10, 939.86it/s] \ntokenizing: 1it [00:00, 1148.81it/s]\nEpoch 1/30, Loss: 1.1301237344741821\nEpoch 2/30, Loss: 1.0764085054397583\nEpoch 3/30, Loss: 1.0468225479125977\nEpoch 4/30, Loss: 1.0083335638046265\nEpoch 5/30, Loss: 0.971441388130188\nEpoch 6/30, Loss: 0.9388228058815002\nEpoch 7/30, Loss: 0.9037883281707764\nEpoch 8/30, Loss: 0.8670093417167664\nEpoch 9/30, Loss: 0.831554651260376\nEpoch 10/30, Loss: 0.7974877953529358\nEpoch 11/30, Loss: 0.7623931765556335\nEpoch 12/30, Loss: 0.7260944247245789\nEpoch 13/30, Loss: 0.6902034878730774\nEpoch 14/30, Loss: 0.6552446484565735\nEpoch 15/30, Loss: 0.6202374696731567\nEpoch 16/30, Loss: 0.5849803686141968\nEpoch 17/30, Loss: 0.5501099824905396\nEpoch 18/30, Loss: 0.516369104385376\nEpoch 19/30, Loss: 0.4838622808456421\nEpoch 20/30, Loss: 0.45247188210487366\nEpoch 21/30, Loss: 0.4226413071155548\nEpoch 22/30, Loss: 0.39520514011383057\nEpoch 23/30, Loss: 0.3707473874092102\nEpoch 24/30, Loss: 0.34951525926589966\nEpoch 25/30, Loss: 0.3320251703262329\nEpoch 26/30, Loss: 0.3189235031604767\nEpoch 27/30, Loss: 0.31047165393829346\nEpoch 28/30, Loss: 0.30649369955062866\nEpoch 29/30, Loss: 0.3064158260822296\nEpoch 30/30, Loss: 0.30923718214035034\nDataset: mnli-m | Shot: 1 |\n{'dect acc': 0.33316352521650533}\nAverage: 33.316352521650536\n","output_type":"stream"}],"execution_count":57},{"cell_type":"code","source":"%cd /kaggle/working/DecT","metadata":{"execution":{"iopub.status.busy":"2025-07-28T13:14:26.245033Z","iopub.execute_input":"2025-07-28T13:14:26.245366Z","iopub.status.idle":"2025-07-28T13:14:26.251479Z","shell.execute_reply.started":"2025-07-28T13:14:26.245332Z","shell.execute_reply":"2025-07-28T13:14:26.250757Z"},"executionInfo":{"elapsed":273047,"status":"aborted","timestamp":1752231504965,"user":{"displayName":"Gà Lê văn","userId":"04852251220989386905"},"user_tz":-420},"id":"uzE7eiRhGrD4","trusted":true},"outputs":[{"name":"stdout","text":"/kaggle/working/DecT\n","output_type":"stream"}],"execution_count":58},{"cell_type":"code","source":"!python src/run_dect.py \\\n--model gpt2 \\\n--size large \\\n--type chat \\\n--dataset mnli-mm \\\n--max_epochs 30 \\\n--batch_size 16 \\\n--proto_dim 512 \\\n--lr 5e-4 \\\n--seed 42 \\\n--template_id 0\n","metadata":{"execution":{"iopub.status.busy":"2025-07-28T13:14:26.252673Z","iopub.execute_input":"2025-07-28T13:14:26.253268Z","iopub.status.idle":"2025-07-28T13:19:16.556229Z","shell.execute_reply.started":"2025-07-28T13:14:26.253246Z","shell.execute_reply":"2025-07-28T13:19:16.555092Z"},"executionInfo":{"elapsed":273047,"status":"aborted","timestamp":1752231504967,"user":{"displayName":"Gà Lê văn","userId":"04852251220989386905"},"user_tz":-420},"id":"HljudQWakojG","trusted":true},"outputs":[{"name":"stdout","text":"/usr/local/lib/python3.11/dist-packages/transformers/generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.\n  warnings.warn(\n2025-07-28 13:14:31.251222: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1753708471.278954     943 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1753708471.286683     943 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nUsing full test set with size: 9832\n/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\nUsing pad_token, but it is not set yet.\ntokenizing: 3it [00:00, 471.96it/s]\ntokenizing: 3it [00:00, 1005.19it/s]\ntokenizing: 9832it [00:10, 936.65it/s]\ntokenizing: 1it [00:00, 985.50it/s]\nEpoch 1/30, Loss: 1.1263450384140015\nEpoch 2/30, Loss: 1.0783030986785889\nEpoch 3/30, Loss: 1.0553107261657715\nEpoch 4/30, Loss: 1.0230835676193237\nEpoch 5/30, Loss: 0.9947670698165894\nEpoch 6/30, Loss: 0.9683616161346436\nEpoch 7/30, Loss: 0.938095211982727\nEpoch 8/30, Loss: 0.9080501794815063\nEpoch 9/30, Loss: 0.8799983263015747\nEpoch 10/30, Loss: 0.8506556153297424\nEpoch 11/30, Loss: 0.8194566965103149\nEpoch 12/30, Loss: 0.7887015342712402\nEpoch 13/30, Loss: 0.7584614157676697\nEpoch 14/30, Loss: 0.7269527316093445\nEpoch 15/30, Loss: 0.6941919326782227\nEpoch 16/30, Loss: 0.6615584492683411\nEpoch 17/30, Loss: 0.6291148662567139\nEpoch 18/30, Loss: 0.5960096120834351\nEpoch 19/30, Loss: 0.5625895261764526\nEpoch 20/30, Loss: 0.5298333168029785\nEpoch 21/30, Loss: 0.4978894591331482\nEpoch 22/30, Loss: 0.46654531359672546\nEpoch 23/30, Loss: 0.4365193247795105\nEpoch 24/30, Loss: 0.408770889043808\nEpoch 25/30, Loss: 0.3834156095981598\nEpoch 26/30, Loss: 0.3609614968299866\nEpoch 27/30, Loss: 0.3426724672317505\nEpoch 28/30, Loss: 0.3289463520050049\nEpoch 29/30, Loss: 0.3203033208847046\nEpoch 30/30, Loss: 0.31690192222595215\nDataset: mnli-mm | Shot: 1 |\n{'dect acc': 0.33696094385679415}\nAverage: 33.696094385679416\n","output_type":"stream"}],"execution_count":59},{"cell_type":"markdown","source":"## SNLI","metadata":{"id":"jTwaP5USGrD5"}},{"cell_type":"code","source":"%cd /kaggle/working/DecT","metadata":{"execution":{"iopub.status.busy":"2025-07-28T13:19:16.557637Z","iopub.execute_input":"2025-07-28T13:19:16.558013Z","iopub.status.idle":"2025-07-28T13:19:16.563766Z","shell.execute_reply.started":"2025-07-28T13:19:16.557970Z","shell.execute_reply":"2025-07-28T13:19:16.563069Z"},"executionInfo":{"elapsed":273048,"status":"aborted","timestamp":1752231504970,"user":{"displayName":"Gà Lê văn","userId":"04852251220989386905"},"user_tz":-420},"id":"novrXIJPGrD5","trusted":true},"outputs":[{"name":"stdout","text":"/kaggle/working/DecT\n","output_type":"stream"}],"execution_count":60},{"cell_type":"code","source":"!python src/run_dect.py \\\n--model gpt2 \\\n--size large \\\n--type chat \\\n--dataset snli \\\n--max_epochs 30 \\\n--batch_size 16 \\\n--proto_dim 512 \\\n--lr 5e-4 \\\n--seed 42 \\\n--template_id 0\n","metadata":{"execution":{"iopub.status.busy":"2025-07-28T13:19:16.564772Z","iopub.execute_input":"2025-07-28T13:19:16.565099Z","iopub.status.idle":"2025-07-28T13:24:04.752211Z","shell.execute_reply.started":"2025-07-28T13:19:16.565074Z","shell.execute_reply":"2025-07-28T13:24:04.751212Z"},"executionInfo":{"elapsed":273048,"status":"aborted","timestamp":1752231504972,"user":{"displayName":"Gà Lê văn","userId":"04852251220989386905"},"user_tz":-420},"id":"Z4aiWXHwkrC1","trusted":true},"outputs":[{"name":"stdout","text":"/usr/local/lib/python3.11/dist-packages/transformers/generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.\n  warnings.warn(\n2025-07-28 13:19:21.515495: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1753708761.541866     960 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1753708761.549801     960 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nUsing full test set with size: 9831\n/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\nUsing pad_token, but it is not set yet.\ntokenizing: 3it [00:00, 575.19it/s]\ntokenizing: 3it [00:00, 1032.83it/s]\ntokenizing: 9831it [00:09, 1069.34it/s]\ntokenizing: 1it [00:00, 1198.72it/s]\nEpoch 1/30, Loss: 1.1221973896026611\nEpoch 2/30, Loss: 1.0874040126800537\nEpoch 3/30, Loss: 1.072339415550232\nEpoch 4/30, Loss: 1.049078345298767\nEpoch 5/30, Loss: 1.031569242477417\nEpoch 6/30, Loss: 1.0130376815795898\nEpoch 7/30, Loss: 0.9915763735771179\nEpoch 8/30, Loss: 0.9712845087051392\nEpoch 9/30, Loss: 0.9520878195762634\nEpoch 10/30, Loss: 0.9308155179023743\nEpoch 11/30, Loss: 0.9081902503967285\nEpoch 12/30, Loss: 0.8862249255180359\nEpoch 13/30, Loss: 0.8638831973075867\nEpoch 14/30, Loss: 0.8395592570304871\nEpoch 15/30, Loss: 0.8140226602554321\nEpoch 16/30, Loss: 0.788240909576416\nEpoch 17/30, Loss: 0.7613500356674194\nEpoch 18/30, Loss: 0.7324329018592834\nEpoch 19/30, Loss: 0.7021173238754272\nEpoch 20/30, Loss: 0.6708717346191406\nEpoch 21/30, Loss: 0.6381776928901672\nEpoch 22/30, Loss: 0.6039046049118042\nEpoch 23/30, Loss: 0.5686933994293213\nEpoch 24/30, Loss: 0.5330333709716797\nEpoch 25/30, Loss: 0.496809184551239\nEpoch 26/30, Loss: 0.4604766368865967\nEpoch 27/30, Loss: 0.4252853989601135\nEpoch 28/30, Loss: 0.39205822348594666\nEpoch 29/30, Loss: 0.3614843487739563\nEpoch 30/30, Loss: 0.3351215124130249\nDataset: snli | Shot: 1 |\n{'dect acc': 0.33760553351642764}\nAverage: 33.76055335164276\n","output_type":"stream"}],"execution_count":61},{"cell_type":"markdown","source":"## RTE","metadata":{"id":"eAMywA9XGrD6"}},{"cell_type":"code","source":"%cd /kaggle/working/DecT","metadata":{"execution":{"iopub.status.busy":"2025-07-28T13:24:04.755721Z","iopub.execute_input":"2025-07-28T13:24:04.756014Z","iopub.status.idle":"2025-07-28T13:24:04.762131Z","shell.execute_reply.started":"2025-07-28T13:24:04.755988Z","shell.execute_reply":"2025-07-28T13:24:04.761306Z"},"executionInfo":{"elapsed":273047,"status":"aborted","timestamp":1752231504973,"user":{"displayName":"Gà Lê văn","userId":"04852251220989386905"},"user_tz":-420},"id":"Q7BskPctGrD6","trusted":true},"outputs":[{"name":"stdout","text":"/kaggle/working/DecT\n","output_type":"stream"}],"execution_count":62},{"cell_type":"code","source":"!python src/run_dect.py \\\n--model gpt2 \\\n--size large \\\n--type chat \\\n--dataset rte \\\n--max_epochs 30 \\\n--batch_size 16 \\\n--proto_dim 512 \\\n--lr 5e-4 \\\n--seed 42 \\\n--template_id 0\n","metadata":{"execution":{"iopub.status.busy":"2025-07-28T13:24:04.763072Z","iopub.execute_input":"2025-07-28T13:24:04.763308Z","iopub.status.idle":"2025-07-28T13:24:45.272619Z","shell.execute_reply.started":"2025-07-28T13:24:04.763290Z","shell.execute_reply":"2025-07-28T13:24:45.271690Z"},"executionInfo":{"elapsed":273047,"status":"aborted","timestamp":1752231504975,"user":{"displayName":"Gà Lê văn","userId":"04852251220989386905"},"user_tz":-420},"id":"QXAzh4ODkszo","trusted":true},"outputs":[{"name":"stdout","text":"/usr/local/lib/python3.11/dist-packages/transformers/generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.\n  warnings.warn(\n2025-07-28 13:24:09.733150: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1753709049.758476     977 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1753709049.766181     977 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nUsing full test set with size: 277\n/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\nUsing pad_token, but it is not set yet.\ntokenizing: 2it [00:00, 440.56it/s]\ntokenizing: 2it [00:00, 892.88it/s]\ntokenizing: 277it [00:00, 589.10it/s]\ntokenizing: 1it [00:00, 1132.37it/s]\nEpoch 1/30, Loss: 0.708318293094635\nEpoch 2/30, Loss: 0.6585820913314819\nEpoch 3/30, Loss: 0.6221620440483093\nEpoch 4/30, Loss: 0.5843093991279602\nEpoch 5/30, Loss: 0.5526328682899475\nEpoch 6/30, Loss: 0.5174797773361206\nEpoch 7/30, Loss: 0.4837222695350647\nEpoch 8/30, Loss: 0.4532891809940338\nEpoch 9/30, Loss: 0.4217548072338104\nEpoch 10/30, Loss: 0.3913486897945404\nEpoch 11/30, Loss: 0.3640315532684326\nEpoch 12/30, Loss: 0.3378332853317261\nEpoch 13/30, Loss: 0.3125871419906616\nEpoch 14/30, Loss: 0.2900504469871521\nEpoch 15/30, Loss: 0.27030956745147705\nEpoch 16/30, Loss: 0.2524681091308594\nEpoch 17/30, Loss: 0.23688670992851257\nEpoch 18/30, Loss: 0.22433070838451385\nEpoch 19/30, Loss: 0.2146381437778473\nEpoch 20/30, Loss: 0.20728625357151031\nEpoch 21/30, Loss: 0.20218956470489502\nEpoch 22/30, Loss: 0.19939307868480682\nEpoch 23/30, Loss: 0.1985933780670166\nEpoch 24/30, Loss: 0.19923271238803864\nEpoch 25/30, Loss: 0.20079785585403442\nEpoch 26/30, Loss: 0.20289865136146545\nEpoch 27/30, Loss: 0.20519042015075684\nEpoch 28/30, Loss: 0.20733776688575745\nEpoch 29/30, Loss: 0.20905330777168274\nEpoch 30/30, Loss: 0.21014726161956787\nDataset: rte | Shot: 1 |\n{'dect acc': 0.5270758122743683}\nAverage: 52.707581227436826\n","output_type":"stream"}],"execution_count":63},{"cell_type":"markdown","source":"## SST2","metadata":{"id":"2ptFQeCPGrD8"}},{"cell_type":"code","source":"%cd /kaggle/working/DecT","metadata":{"execution":{"iopub.status.busy":"2025-07-28T13:24:45.273783Z","iopub.execute_input":"2025-07-28T13:24:45.274031Z","iopub.status.idle":"2025-07-28T13:24:45.280024Z","shell.execute_reply.started":"2025-07-28T13:24:45.274008Z","shell.execute_reply":"2025-07-28T13:24:45.279054Z"},"executionInfo":{"elapsed":273055,"status":"aborted","timestamp":1752231504985,"user":{"displayName":"Gà Lê văn","userId":"04852251220989386905"},"user_tz":-420},"id":"FSYqTyUnGrD8","trusted":true},"outputs":[{"name":"stdout","text":"/kaggle/working/DecT\n","output_type":"stream"}],"execution_count":64},{"cell_type":"code","source":"!python src/run_dect.py \\\n--model gpt2 \\\n--size large \\\n--type chat \\\n--dataset sst2 \\\n--max_epochs 30 \\\n--batch_size 16 \\\n--proto_dim 512 \\\n--lr 5e-4 \\\n--seed 42 \\\n--template_id 0\n","metadata":{"execution":{"iopub.status.busy":"2025-07-28T13:24:45.280928Z","iopub.execute_input":"2025-07-28T13:24:45.281460Z","iopub.status.idle":"2025-07-28T13:25:40.764302Z","shell.execute_reply.started":"2025-07-28T13:24:45.281434Z","shell.execute_reply":"2025-07-28T13:25:40.763391Z"},"executionInfo":{"elapsed":273055,"status":"aborted","timestamp":1752231504987,"user":{"displayName":"Gà Lê văn","userId":"04852251220989386905"},"user_tz":-420},"id":"IJFgx0bik0ts","trusted":true},"outputs":[{"name":"stdout","text":"/usr/local/lib/python3.11/dist-packages/transformers/generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.\n  warnings.warn(\n2025-07-28 13:24:50.183770: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1753709090.210366     996 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1753709090.218425     996 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nUsing full test set with size: 872\n/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\nUsing pad_token, but it is not set yet.\ntokenizing: 2it [00:00, 561.22it/s]\ntokenizing: 2it [00:00, 1103.91it/s]\ntokenizing: 872it [00:00, 947.76it/s] \ntokenizing: 1it [00:00, 1086.33it/s]\nEpoch 1/30, Loss: 0.7106929421424866\nEpoch 2/30, Loss: 0.6836114525794983\nEpoch 3/30, Loss: 0.6634767651557922\nEpoch 4/30, Loss: 0.6405874490737915\nEpoch 5/30, Loss: 0.6246811151504517\nEpoch 6/30, Loss: 0.6041098237037659\nEpoch 7/30, Loss: 0.5828644037246704\nEpoch 8/30, Loss: 0.5650573372840881\nEpoch 9/30, Loss: 0.545030951499939\nEpoch 10/30, Loss: 0.5238388180732727\nEpoch 11/30, Loss: 0.5047017931938171\nEpoch 12/30, Loss: 0.4853767156600952\nEpoch 13/30, Loss: 0.46449005603790283\nEpoch 14/30, Loss: 0.444266676902771\nEpoch 15/30, Loss: 0.4250422418117523\nEpoch 16/30, Loss: 0.40503546595573425\nEpoch 17/30, Loss: 0.38473400473594666\nEpoch 18/30, Loss: 0.36549389362335205\nEpoch 19/30, Loss: 0.3466106057167053\nEpoch 20/30, Loss: 0.32746267318725586\nEpoch 21/30, Loss: 0.30910617113113403\nEpoch 22/30, Loss: 0.29194509983062744\nEpoch 23/30, Loss: 0.2753315269947052\nEpoch 24/30, Loss: 0.2596316635608673\nEpoch 25/30, Loss: 0.24566015601158142\nEpoch 26/30, Loss: 0.2332647144794464\nEpoch 27/30, Loss: 0.22235074639320374\nEpoch 28/30, Loss: 0.21349552273750305\nEpoch 29/30, Loss: 0.2068716585636139\nEpoch 30/30, Loss: 0.20218417048454285\nDataset: sst2 | Shot: 1 |\n{'dect acc': 0.6456422018348624}\nAverage: 64.56422018348624\n","output_type":"stream"}],"execution_count":65},{"cell_type":"markdown","source":"## Yahoo","metadata":{"id":"F4q7sxAuGrD9"}},{"cell_type":"code","source":"%cd /kaggle/working/DecT","metadata":{"execution":{"iopub.status.busy":"2025-07-28T13:25:40.765660Z","iopub.execute_input":"2025-07-28T13:25:40.766129Z","iopub.status.idle":"2025-07-28T13:25:40.772656Z","shell.execute_reply.started":"2025-07-28T13:25:40.766091Z","shell.execute_reply":"2025-07-28T13:25:40.771922Z"},"executionInfo":{"elapsed":273054,"status":"aborted","timestamp":1752231504988,"user":{"displayName":"Gà Lê văn","userId":"04852251220989386905"},"user_tz":-420},"id":"Tcn9ip29GrD9","trusted":true},"outputs":[{"name":"stdout","text":"/kaggle/working/DecT\n","output_type":"stream"}],"execution_count":66},{"cell_type":"code","source":"!python src/run_dect.py \\\n--model gpt2 \\\n--size large \\\n--type chat \\\n--dataset yahoo \\\n--max_epochs 30 \\\n--batch_size 16 \\\n--proto_dim 512 \\\n--lr 5e-4 \\\n--seed 42 \\\n--template_id 0\n","metadata":{"execution":{"iopub.status.busy":"2025-07-28T13:25:40.773493Z","iopub.execute_input":"2025-07-28T13:25:40.773701Z","iopub.status.idle":"2025-07-28T13:53:01.235037Z","shell.execute_reply.started":"2025-07-28T13:25:40.773675Z","shell.execute_reply":"2025-07-28T13:53:01.234193Z"},"executionInfo":{"elapsed":273052,"status":"aborted","timestamp":1752231504989,"user":{"displayName":"Gà Lê văn","userId":"04852251220989386905"},"user_tz":-420},"id":"d_gkXFvAk55Z","trusted":true},"outputs":[{"name":"stdout","text":"/usr/local/lib/python3.11/dist-packages/transformers/generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.\n  warnings.warn(\n2025-07-28 13:25:45.759716: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1753709145.807331    1013 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1753709145.816712    1013 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nUsing full test set with size: 60000\n/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\nUsing pad_token, but it is not set yet.\ntokenizing: 10it [00:00, 427.01it/s]\ntokenizing: 154it [00:00, 510.83it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1329 > 1024). Running this sequence through the model will result in indexing errors\ntokenizing: 60000it [01:33, 638.43it/s]\ntokenizing: 1it [00:00, 1086.33it/s]\nEpoch 1/30, Loss: 2.3639538288116455\nEpoch 2/30, Loss: 2.3092496395111084\nEpoch 3/30, Loss: 2.266692638397217\nEpoch 4/30, Loss: 2.236372709274292\nEpoch 5/30, Loss: 2.2152953147888184\nEpoch 6/30, Loss: 2.195699691772461\nEpoch 7/30, Loss: 2.171584129333496\nEpoch 8/30, Loss: 2.143292188644409\nEpoch 9/30, Loss: 2.1127727031707764\nEpoch 10/30, Loss: 2.082104444503784\nEpoch 11/30, Loss: 2.0521388053894043\nEpoch 12/30, Loss: 2.022484302520752\nEpoch 13/30, Loss: 1.9920432567596436\nEpoch 14/30, Loss: 1.9596306085586548\nEpoch 15/30, Loss: 1.9244096279144287\nEpoch 16/30, Loss: 1.8862732648849487\nEpoch 17/30, Loss: 1.8453280925750732\nEpoch 18/30, Loss: 1.8017656803131104\nEpoch 19/30, Loss: 1.7557379007339478\nEpoch 20/30, Loss: 1.7072515487670898\nEpoch 21/30, Loss: 1.6561185121536255\nEpoch 22/30, Loss: 1.6019827127456665\nEpoch 23/30, Loss: 1.5444142818450928\nEpoch 24/30, Loss: 1.483030915260315\nEpoch 25/30, Loss: 1.4176065921783447\nEpoch 26/30, Loss: 1.3481413125991821\nEpoch 27/30, Loss: 1.2748878002166748\nEpoch 28/30, Loss: 1.1983420848846436\nEpoch 29/30, Loss: 1.119214415550232\nEpoch 30/30, Loss: 1.0384001731872559\nDataset: yahoo | Shot: 1 |\n{'dect acc': 0.25393333333333334}\nAverage: 25.393333333333334\n","output_type":"stream"}],"execution_count":67},{"cell_type":"markdown","source":"## Yelp","metadata":{"id":"_T_p04RXGrD-"}},{"cell_type":"code","source":"%cd /kaggle/working/DecT","metadata":{"execution":{"iopub.status.busy":"2025-07-28T13:53:01.236251Z","iopub.execute_input":"2025-07-28T13:53:01.236562Z","iopub.status.idle":"2025-07-28T13:53:01.243024Z","shell.execute_reply.started":"2025-07-28T13:53:01.236531Z","shell.execute_reply":"2025-07-28T13:53:01.242289Z"},"executionInfo":{"elapsed":273053,"status":"aborted","timestamp":1752231504992,"user":{"displayName":"Gà Lê văn","userId":"04852251220989386905"},"user_tz":-420},"id":"kFlCJBdcGrD-","trusted":true},"outputs":[{"name":"stdout","text":"/kaggle/working/DecT\n","output_type":"stream"}],"execution_count":68},{"cell_type":"code","source":"!python src/run_dect.py \\\n--model gpt2 \\\n--size large \\\n--type chat \\\n--dataset yelp \\\n--max_epochs 30 \\\n--batch_size 16 \\\n--proto_dim 512 \\\n--lr 5e-4 \\\n--seed 42 \\\n--template_id 0\n","metadata":{"execution":{"iopub.status.busy":"2025-07-28T13:53:01.243842Z","iopub.execute_input":"2025-07-28T13:53:01.244072Z","iopub.status.idle":"2025-07-28T14:10:32.707367Z","shell.execute_reply.started":"2025-07-28T13:53:01.244055Z","shell.execute_reply":"2025-07-28T14:10:32.706513Z"},"executionInfo":{"elapsed":273052,"status":"aborted","timestamp":1752231504994,"user":{"displayName":"Gà Lê văn","userId":"04852251220989386905"},"user_tz":-420},"id":"PTxW6lxpk8RL","trusted":true},"outputs":[{"name":"stdout","text":"/usr/local/lib/python3.11/dist-packages/transformers/generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.\n  warnings.warn(\n2025-07-28 13:53:06.112974: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1753710786.139634    1032 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1753710786.147763    1032 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nUsing full test set with size: 38000\n/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\nUsing pad_token, but it is not set yet.\ntokenizing: 2it [00:00, 227.83it/s]\ntokenizing: 2it [00:00, 602.11it/s]\ntokenizing: 1205it [00:02, 554.75it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1206 > 1024). Running this sequence through the model will result in indexing errors\ntokenizing: 38000it [01:04, 585.46it/s]\ntokenizing: 1it [00:00, 1127.20it/s]\nEpoch 1/30, Loss: 0.7068471908569336\nEpoch 2/30, Loss: 0.6773067712783813\nEpoch 3/30, Loss: 0.6539446115493774\nEpoch 4/30, Loss: 0.6201781630516052\nEpoch 5/30, Loss: 0.5970404744148254\nEpoch 6/30, Loss: 0.575068473815918\nEpoch 7/30, Loss: 0.5479602217674255\nEpoch 8/30, Loss: 0.5230838656425476\nEpoch 9/30, Loss: 0.5016387701034546\nEpoch 10/30, Loss: 0.4782153367996216\nEpoch 11/30, Loss: 0.4532272219657898\nEpoch 12/30, Loss: 0.43004411458969116\nEpoch 13/30, Loss: 0.4088783264160156\nEpoch 14/30, Loss: 0.38642585277557373\nEpoch 15/30, Loss: 0.3636265993118286\nEpoch 16/30, Loss: 0.3429463505744934\nEpoch 17/30, Loss: 0.3235647678375244\nEpoch 18/30, Loss: 0.3040564954280853\nEpoch 19/30, Loss: 0.28544872999191284\nEpoch 20/30, Loss: 0.26906245946884155\nEpoch 21/30, Loss: 0.2543356418609619\nEpoch 22/30, Loss: 0.24060288071632385\nEpoch 23/30, Loss: 0.2285948097705841\nEpoch 24/30, Loss: 0.21900230646133423\nEpoch 25/30, Loss: 0.21147418022155762\nEpoch 26/30, Loss: 0.20557184517383575\nEpoch 27/30, Loss: 0.20144370198249817\nEpoch 28/30, Loss: 0.19924598932266235\nEpoch 29/30, Loss: 0.1986618936061859\nEpoch 30/30, Loss: 0.1991685926914215\nDataset: yelp | Shot: 1 |\n{'dect acc': 0.7251842105263158}\nAverage: 72.51842105263158\n","output_type":"stream"}],"execution_count":69},{"cell_type":"markdown","source":"## mistral 7B","metadata":{}},{"cell_type":"code","source":"%cd /kaggle/working/DecT\n!pip show transformers\n!python -c \"import transformers; print(transformers.__version__)\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-30T05:17:36.461942Z","iopub.execute_input":"2025-07-30T05:17:36.462267Z","iopub.status.idle":"2025-07-30T05:17:39.493781Z","shell.execute_reply.started":"2025-07-30T05:17:36.462241Z","shell.execute_reply":"2025-07-30T05:17:39.492965Z"}},"outputs":[{"name":"stdout","text":"/kaggle/working/DecT\nName: transformers\nVersion: 4.31.0\nSummary: State-of-the-art Machine Learning for JAX, PyTorch and TensorFlow\nHome-page: https://github.com/huggingface/transformers\nAuthor: The Hugging Face team (past and future) with the help of all our contributors (https://github.com/huggingface/transformers/graphs/contributors)\nAuthor-email: transformers@huggingface.co\nLicense: Apache 2.0 License\nLocation: /usr/local/lib/python3.11/dist-packages\nRequires: filelock, huggingface-hub, numpy, packaging, pyyaml, regex, requests, safetensors, tokenizers, tqdm\nRequired-by: kaggle-environments, openprompt, peft, sentence-transformers\n4.31.0\n","output_type":"stream"}],"execution_count":39},{"cell_type":"code","source":"import os\nimport sys\n\n# Đặt biến môi trường cho Hugging Face Token (nếu chưa có)\nos.environ['HF_TOKEN'] = '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'\n\n# Điều hướng đến thư mục làm việc của bạn\nprint(\"Đang chuyển đến thư mục /kaggle/working/DecT...\")\nos.chdir('/kaggle/working/DecT')\nprint(\"Đã chuyển đến:\", os.getcwd())\n\n# Gỡ cài đặt tất cả các thư viện liên quan để đảm bảo một \"khởi đầu sạch\"\nprint(\"\\nĐang gỡ cài đặt các thư viện hiện có...\")\nos.system('pip uninstall -y torch torchvision torchaudio transformers accelerate bitsandbytes peft trl openprompt tokenizers numpy scipy pandas scikit-learn matplotlib tensorflow keras')\nprint(\"Hoàn tất gỡ cài đặt.\")\n\n# Cài đặt lại Torch, Torchvision, và Torchaudio cho CUDA 11.8\nprint(\"\\nĐang cài đặt Torch, Torchvision, Torchaudio...\")\nos.system('pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118')\nprint(\"Hoàn tất cài đặt Torch.\")\n\n# Cài đặt phiên bản transformers cụ thể (4.31.0)\nprint(\"\\nĐang cài đặt transformers==4.31.0...\")\nos.system('pip install transformers==4.31.0')\nprint(\"Hoàn tất cài đặt transformers.\")\n\n# Cài đặt các thư viện khác liên quan đến mô hình ngôn ngữ lớn và huấn luyện\nprint(\"\\nĐang cài đặt accelerate, bitsandbytes, peft, trl...\")\nos.system('pip install accelerate bitsandbytes peft trl')\nprint(\"Hoàn tất cài đặt các thư viện LLM.\")\n\n# Hạ cấp NumPy xuống phiên bản 1.26.4 để khắc phục lỗi tương thích\nprint(\"\\nĐang cài đặt numpy==1.26.4 và các thư viện phụ thuộc...\")\nos.system('pip install numpy==1.26.4 pandas scipy scikit-learn')\nprint(\"Hoàn tất cài đặt numpy và các thư viện phụ thuộc.\")\n\n# Cài đặt openprompt và tokenizers\nprint(\"\\nĐang cài đặt và nâng cấp openprompt, tokenizers...\")\nos.system('pip install openprompt tokenizers --upgrade')\nprint(\"Hoàn tất cài đặt openprompt.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-30T05:17:39.494889Z","iopub.execute_input":"2025-07-30T05:17:39.495101Z","iopub.status.idle":"2025-07-30T05:21:37.570851Z","shell.execute_reply.started":"2025-07-30T05:17:39.495080Z","shell.execute_reply":"2025-07-30T05:21:37.570035Z"}},"outputs":[{"name":"stdout","text":"Đang chuyển đến thư mục /kaggle/working/DecT...\nĐã chuyển đến: /kaggle/working/DecT\n\nĐang gỡ cài đặt các thư viện hiện có...\nFound existing installation: torch 2.6.0+cu124\nUninstalling torch-2.6.0+cu124:\n  Successfully uninstalled torch-2.6.0+cu124\nFound existing installation: torchvision 0.21.0+cu124\nUninstalling torchvision-0.21.0+cu124:\n  Successfully uninstalled torchvision-0.21.0+cu124\nFound existing installation: torchaudio 2.6.0+cu124\nUninstalling torchaudio-2.6.0+cu124:\n  Successfully uninstalled torchaudio-2.6.0+cu124\nFound existing installation: transformers 4.31.0\nUninstalling transformers-4.31.0:\n  Successfully uninstalled transformers-4.31.0\nFound existing installation: accelerate 1.8.1\nUninstalling accelerate-1.8.1:\n  Successfully uninstalled accelerate-1.8.1\n","output_type":"stream"},{"name":"stderr","text":"WARNING: Skipping bitsandbytes as it is not installed.\n","output_type":"stream"},{"name":"stdout","text":"Found existing installation: peft 0.15.2\nUninstalling peft-0.15.2:\n  Successfully uninstalled peft-0.15.2\n","output_type":"stream"},{"name":"stderr","text":"WARNING: Skipping trl as it is not installed.\n","output_type":"stream"},{"name":"stdout","text":"Found existing installation: openprompt 1.0.1\nUninstalling openprompt-1.0.1:\n  Successfully uninstalled openprompt-1.0.1\nFound existing installation: tokenizers 0.13.3\nUninstalling tokenizers-0.13.3:\n  Successfully uninstalled tokenizers-0.13.3\nFound existing installation: numpy 1.26.4\nUninstalling numpy-1.26.4:\n  Successfully uninstalled numpy-1.26.4\nFound existing installation: scipy 1.15.3\nUninstalling scipy-1.15.3:\n  Successfully uninstalled scipy-1.15.3\nFound existing installation: pandas 2.2.3\nUninstalling pandas-2.2.3:\n  Successfully uninstalled pandas-2.2.3\nFound existing installation: scikit-learn 1.2.2\nUninstalling scikit-learn-1.2.2:\n  Successfully uninstalled scikit-learn-1.2.2\nFound existing installation: matplotlib 3.7.2\nUninstalling matplotlib-3.7.2:\n  Successfully uninstalled matplotlib-3.7.2\nFound existing installation: tensorflow 2.18.0\nUninstalling tensorflow-2.18.0:\n  Successfully uninstalled tensorflow-2.18.0\nFound existing installation: keras 3.8.0\nUninstalling keras-3.8.0:\n  Successfully uninstalled keras-3.8.0\nHoàn tất gỡ cài đặt.\n\nĐang cài đặt Torch, Torchvision, Torchaudio...\nLooking in indexes: https://download.pytorch.org/whl/cu118\nCollecting torch\n  Downloading https://download.pytorch.org/whl/cu118/torch-2.7.1%2Bcu118-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (28 kB)\nCollecting torchvision\n  Downloading https://download.pytorch.org/whl/cu118/torchvision-0.22.1%2Bcu118-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (6.1 kB)\nCollecting torchaudio\n  Downloading https://download.pytorch.org/whl/cu118/torchaudio-2.7.1%2Bcu118-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (6.6 kB)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.0)\nCollecting sympy>=1.13.3 (from torch)\n  Downloading https://download.pytorch.org/whl/sympy-1.13.3-py3-none-any.whl.metadata (12 kB)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.0)\nCollecting nvidia-cuda-nvrtc-cu11==11.8.89 (from torch)\n  Downloading https://download.pytorch.org/whl/cu118/nvidia_cuda_nvrtc_cu11-11.8.89-py3-none-manylinux1_x86_64.whl (23.2 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 23.2/23.2 MB 89.0 MB/s eta 0:00:00\nCollecting nvidia-cuda-runtime-cu11==11.8.89 (from torch)\n  Downloading https://download.pytorch.org/whl/cu118/nvidia_cuda_runtime_cu11-11.8.89-py3-none-manylinux1_x86_64.whl (875 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 875.6/875.6 kB 44.4 MB/s eta 0:00:00\nCollecting nvidia-cuda-cupti-cu11==11.8.87 (from torch)\n  Downloading https://download.pytorch.org/whl/cu118/nvidia_cuda_cupti_cu11-11.8.87-py3-none-manylinux1_x86_64.whl (13.1 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 13.1/13.1 MB 101.0 MB/s eta 0:00:00\nCollecting nvidia-cudnn-cu11==9.1.0.70 (from torch)\n  Downloading https://download.pytorch.org/whl/cu118/nvidia_cudnn_cu11-9.1.0.70-py3-none-manylinux2014_x86_64.whl (663.9 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 663.9/663.9 MB 1.5 MB/s eta 0:00:00\nCollecting nvidia-cublas-cu11==11.11.3.6 (from torch)\n  Downloading https://download.pytorch.org/whl/cu118/nvidia_cublas_cu11-11.11.3.6-py3-none-manylinux1_x86_64.whl (417.9 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 417.9/417.9 MB 2.7 MB/s eta 0:00:00\nCollecting nvidia-cufft-cu11==10.9.0.58 (from torch)\n  Downloading https://download.pytorch.org/whl/cu118/nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux1_x86_64.whl (168.4 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 168.4/168.4 MB 10.2 MB/s eta 0:00:00\nCollecting nvidia-curand-cu11==10.3.0.86 (from torch)\n  Downloading https://download.pytorch.org/whl/cu118/nvidia_curand_cu11-10.3.0.86-py3-none-manylinux1_x86_64.whl (58.1 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 58.1/58.1 MB 5.4 MB/s eta 0:00:00\nCollecting nvidia-cusolver-cu11==11.4.1.48 (from torch)\n  Downloading https://download.pytorch.org/whl/cu118/nvidia_cusolver_cu11-11.4.1.48-py3-none-manylinux1_x86_64.whl (128.2 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 128.2/128.2 MB 13.4 MB/s eta 0:00:00\nCollecting nvidia-cusparse-cu11==11.7.5.86 (from torch)\n  Downloading https://download.pytorch.org/whl/cu118/nvidia_cusparse_cu11-11.7.5.86-py3-none-manylinux1_x86_64.whl (204.1 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 204.1/204.1 MB 8.3 MB/s eta 0:00:00\nCollecting nvidia-nccl-cu11==2.21.5 (from torch)\n  Downloading https://download.pytorch.org/whl/cu118/nvidia_nccl_cu11-2.21.5-py3-none-manylinux2014_x86_64.whl (147.8 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 147.8/147.8 MB 11.6 MB/s eta 0:00:00\nCollecting nvidia-nvtx-cu11==11.8.86 (from torch)\n  Downloading https://download.pytorch.org/whl/cu118/nvidia_nvtx_cu11-11.8.86-py3-none-manylinux1_x86_64.whl (99 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 99.1/99.1 kB 6.1 MB/s eta 0:00:00\nCollecting triton==3.3.1 (from torch)\n  Downloading https://download.pytorch.org/whl/triton-3.3.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from triton==3.3.1->torch) (75.2.0)\nCollecting numpy (from torchvision)\n  Downloading https://download.pytorch.org/whl/numpy-2.1.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 60.9/60.9 kB 3.9 MB/s eta 0:00:00\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.2.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\nDownloading https://download.pytorch.org/whl/cu118/torch-2.7.1%2Bcu118-cp311-cp311-manylinux_2_28_x86_64.whl (905.3 MB)\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 905.3/905.3 MB 970.5 kB/s eta 0:00:00\nDownloading https://download.pytorch.org/whl/triton-3.3.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (155.7 MB)\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 155.7/155.7 MB 11.0 MB/s eta 0:00:00\nDownloading https://download.pytorch.org/whl/cu118/torchvision-0.22.1%2Bcu118-cp311-cp311-manylinux_2_28_x86_64.whl (6.7 MB)\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.7/6.7 MB 101.4 MB/s eta 0:00:00\nDownloading https://download.pytorch.org/whl/cu118/torchaudio-2.7.1%2Bcu118-cp311-cp311-manylinux_2_28_x86_64.whl (3.3 MB)\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.3/3.3 MB 92.2 MB/s eta 0:00:00\nDownloading https://download.pytorch.org/whl/sympy-1.13.3-py3-none-any.whl (6.2 MB)\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.2/6.2 MB 58.2 MB/s eta 0:00:00\nDownloading https://download.pytorch.org/whl/numpy-2.1.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.3 MB)\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 16.3/16.3 MB 91.0 MB/s eta 0:00:00\nInstalling collected packages: triton, sympy, nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, numpy, nvidia-cusolver-cu11, nvidia-cudnn-cu11, torch, torchvision, torchaudio\n  Attempting uninstall: triton\n    Found existing installation: triton 3.2.0\n    Uninstalling triton-3.2.0:\n      Successfully uninstalled triton-3.2.0\n  Attempting uninstall: sympy\n    Found existing installation: sympy 1.13.1\n    Uninstalling sympy-1.13.1:\n      Successfully uninstalled sympy-1.13.1\n","output_type":"stream"},{"name":"stderr","text":"ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngensim 4.3.3 requires scipy<1.14.0,>=1.7.0, which is not installed.\ntorchtune 0.6.1 requires tokenizers, which is not installed.\nlibpysal 4.9.2 requires pandas>=1.4, which is not installed.\nlibpysal 4.9.2 requires scipy>=1.8, which is not installed.\ntreelite 4.4.1 requires scipy, which is not installed.\ndask-cudf-cu12 25.2.2 requires pandas<2.2.4dev0,>=2.0, which is not installed.\ndask-cuda 25.2.0 requires pandas>=1.3, which is not installed.\ncuml-cu12 25.2.1 requires scipy>=1.8.0, which is not installed.\ncudf-cu12 25.2.2 requires pandas<2.2.4dev0,>=2.0, which is not installed.\ndatasets 3.6.0 requires pandas, which is not installed.\nwoodwork 0.31.0 requires pandas>=2.0.0, which is not installed.\nwoodwork 0.31.0 requires scikit-learn>=1.1.0, which is not installed.\nwoodwork 0.31.0 requires scipy>=1.10.0, which is not installed.\nydata-profiling 4.16.1 requires matplotlib<=3.10,>=3.5, which is not installed.\nydata-profiling 4.16.1 requires pandas!=1.4.0,<3.0,>1.1, which is not installed.\nydata-profiling 4.16.1 requires scipy<1.16,>=1.4.1, which is not installed.\nboruta 0.4.3 requires scikit-learn>=0.17.1, which is not installed.\nboruta 0.4.3 requires scipy>=0.17.0, which is not installed.\nbayesian-optimization 3.0.0 requires scikit-learn<2.0.0,>=1.0.0, which is not installed.\nbayesian-optimization 3.0.0 requires scipy<2.0.0,>=1.0.0; python_version < \"3.13\", which is not installed.\nscikit-surprise 1.1.4 requires scipy>=1.6.0, which is not installed.\nfeaturetools 1.31.0 requires pandas>=2.0.0, which is not installed.\nfeaturetools 1.31.0 requires scipy>=1.10.0, which is not installed.\nplotly-express 0.4.1 requires pandas>=0.20.0, which is not installed.\nplotly-express 0.4.1 requires scipy>=0.18, which is not installed.\neasyocr 1.7.2 requires scipy, which is not installed.\nimagehash 4.3.1 requires scipy, which is not installed.\nxgboost 2.0.3 requires scipy, which is not installed.\npandasql 0.7.3 requires pandas, which is not installed.\ncatboost 1.2.8 requires matplotlib, which is not installed.\ncatboost 1.2.8 requires pandas>=0.24, which is not installed.\ncatboost 1.2.8 requires scipy, which is not installed.\nfury 0.12.0 requires scipy>=1.0, which is not installed.\ntpot 0.12.1 requires pandas>=0.24.2, which is not installed.\ntpot 0.12.1 requires scikit-learn>=0.22.0, which is not installed.\ntpot 0.12.1 requires scipy>=1.3.1, which is not installed.\nshap 0.44.1 requires pandas, which is not installed.\nshap 0.44.1 requires scikit-learn, which is not installed.\nshap 0.44.1 requires scipy, which is not installed.\ncartopy 0.24.1 requires matplotlib>=3.6, which is not installed.\nmlcrate 0.2.0 requires pandas, which is not installed.\npyupset 0.1.1.post7 requires matplotlib, which is not installed.\npyupset 0.1.1.post7 requires pandas, which is not installed.\nvisions 0.8.1 requires pandas>=2.0.0, which is not installed.\nkaggle-environments 1.17.6 requires scipy>=1.11.2, which is not installed.\nkaggle-environments 1.17.6 requires transformers>=4.33.1, which is not installed.\npymc3 3.11.4 requires pandas>=0.24.0, which is not installed.\npymc3 3.11.4 requires scipy>=1.2.0, which is not installed.\ndipy 1.11.0 requires scipy>=1.8, which is not installed.\nopen-spiel 1.6 requires scipy>=1.10.1, which is not installed.\npyldavis 3.4.1 requires pandas>=2.0.0, which is not installed.\npyldavis 3.4.1 requires scikit-learn>=1.0.0, which is not installed.\npyldavis 3.4.1 requires scipy, which is not installed.\nphik 0.12.4 requires matplotlib>=2.2.3, which is not installed.\nphik 0.12.4 requires pandas>=0.25.1, which is not installed.\nphik 0.12.4 requires scipy>=1.5.2, which is not installed.\ngeopandas 0.14.4 requires pandas>=1.4.0, which is not installed.\ntheano 1.0.5 requires scipy>=0.14, which is not installed.\nnilearn 0.10.4 requires pandas>=1.1.5, which is not installed.\nnilearn 0.10.4 requires scikit-learn>=1.0.0, which is not installed.\nnilearn 0.10.4 requires scipy>=1.8.0, which is not installed.\neli5 0.13.0 requires scikit-learn>=0.20, which is not installed.\neli5 0.13.0 requires scipy, which is not installed.\nipympl 0.9.7 requires matplotlib<4,>=3.5.0, which is not installed.\nstable-baselines3 2.1.0 requires matplotlib, which is not installed.\nstable-baselines3 2.1.0 requires pandas, which is not installed.\nscikit-learn-intelex 2025.6.1 requires scikit-learn>=0.22, which is not installed.\nseaborn 0.12.2 requires matplotlib!=3.6.1,>=3.1, which is not installed.\nseaborn 0.12.2 requires pandas>=0.25, which is not installed.\ncategory-encoders 2.7.0 requires pandas>=1.0.5, which is not installed.\ncategory-encoders 2.7.0 requires scikit-learn<1.6.0,>=1.0.0, which is not installed.\ncategory-encoders 2.7.0 requires scipy>=1.0.0, which is not installed.\nlime 0.2.0.1 requires matplotlib, which is not installed.\nlime 0.2.0.1 requires scikit-learn>=0.18, which is not installed.\nlime 0.2.0.1 requires scipy, which is not installed.\ncesium 0.12.4 requires pandas>=0.17.0, which is not installed.\ncesium 0.12.4 requires scikit-learn>=0.22.1, which is not installed.\ncesium 0.12.4 requires scipy>=0.16.0, which is not installed.\ntheano-pymc 1.1.2 requires scipy>=0.14, which is not installed.\nhep-ml 0.8.0 requires pandas>=1.0.0, which is not installed.\nhep-ml 0.8.0 requires scikit-learn>=1.0, which is not installed.\nhep-ml 0.8.0 requires scipy>=1.0.0, which is not installed.\nscikit-optimize 0.10.2 requires scikit-learn>=1.0.0, which is not installed.\nscikit-optimize 0.10.2 requires scipy>=1.1.0, which is not installed.\nmne 1.9.0 requires matplotlib>=3.6, which is not installed.\nmne 1.9.0 requires scipy>=1.9, which is not installed.\njax 0.5.2 requires scipy>=1.11.1, which is not installed.\ntsfresh 0.21.0 requires pandas>=0.25.0, which is not installed.\ntsfresh 0.21.0 requires scikit-learn>=0.22.0, which is not installed.\ntsfresh 0.21.0 requires scipy>=1.14.0; python_version >= \"3.10\", which is not installed.\nprophet 1.1.7 requires matplotlib>=2.0.0, which is not installed.\nprophet 1.1.7 requires pandas>=1.0.4, which is not installed.\ngeemap 0.35.3 requires matplotlib, which is not installed.\ngeemap 0.35.3 requires pandas, which is not installed.\ndopamine-rl 4.1.2 requires pandas>=0.24.2, which is not installed.\ndopamine-rl 4.1.2 requires tensorflow>=2.2.0, which is not installed.\nbokeh 3.7.3 requires pandas>=1.2, which is not installed.\nsklearn-pandas 2.2.0 requires pandas>=1.1.4, which is not installed.\nsklearn-pandas 2.2.0 requires scikit-learn>=0.23.0, which is not installed.\nsklearn-pandas 2.2.0 requires scipy>=1.5.1, which is not installed.\npandas-gbq 0.29.1 requires pandas>=1.1.4, which is not installed.\nmatplotlib-venn 1.1.2 requires matplotlib, which is not installed.\nmatplotlib-venn 1.1.2 requires scipy, which is not installed.\ndatascience 0.17.6 requires matplotlib>=3.0.0, which is not installed.\ndatascience 0.17.6 requires pandas, which is not installed.\ndatascience 0.17.6 requires scipy, which is not installed.\nlibrosa 0.11.0 requires scikit-learn>=1.1.0, which is not installed.\nlibrosa 0.11.0 requires scipy>=1.6.0, which is not installed.\nsentence-transformers 4.1.0 requires scikit-learn, which is not installed.\nsentence-transformers 4.1.0 requires scipy, which is not installed.\nsentence-transformers 4.1.0 requires transformers<5.0.0,>=4.41.0, which is not installed.\nmusic21 9.3.0 requires matplotlib, which is not installed.\nscikit-image 0.25.2 requires scipy>=1.11.4, which is not installed.\nmissingno 0.5.2 requires matplotlib, which is not installed.\nmissingno 0.5.2 requires scipy, which is not installed.\narviz 0.21.0 requires matplotlib>=3.5, which is not installed.\narviz 0.21.0 requires pandas>=1.5.0, which is not installed.\narviz 0.21.0 requires scipy>=1.9.0, which is not installed.\numap-learn 0.5.7 requires scikit-learn>=0.22, which is not installed.\numap-learn 0.5.7 requires scipy>=1.3.1, which is not installed.\ncmdstanpy 1.2.5 requires pandas, which is not installed.\nhyperopt 0.2.7 requires scipy, which is not installed.\nimbalanced-learn 0.13.0 requires scikit-learn<2,>=1.3.2, which is not installed.\nimbalanced-learn 0.13.0 requires scipy<2,>=1.10.1, which is not installed.\nalbumentations 2.0.8 requires scipy>=1.10.0, which is not installed.\npymc 5.23.0 requires pandas>=0.24.0, which is not installed.\npymc 5.23.0 requires scipy>=1.4.1, which is not installed.\nbqplot 0.12.45 requires pandas<3.0.0,>=1.0.0, which is not installed.\ntensorflow-decision-forests 1.11.0 requires pandas, which is not installed.\ntensorflow-decision-forests 1.11.0 requires tensorflow==2.18.0, which is not installed.\nclarabel 0.11.1 requires scipy, which is not installed.\nscs 3.2.7.post2 requires scipy, which is not installed.\nxarray-einstats 0.9.1 requires scipy>=1.11, which is not installed.\ngradio 5.31.0 requires pandas<3.0,>=1.0, which is not installed.\nfastai 2.7.19 requires matplotlib, which is not installed.\nfastai 2.7.19 requires pandas, which is not installed.\nfastai 2.7.19 requires scikit-learn, which is not installed.\nfastai 2.7.19 requires scipy, which is not installed.\nmizani 0.13.5 requires pandas>=2.2.0, which is not installed.\nmizani 0.13.5 requires scipy>=1.8.0, which is not installed.\ncvxpy 1.6.6 requires scipy>=1.11.0, which is not installed.\nyfinance 0.2.63 requires pandas>=1.3.0, which is not installed.\nlightgbm 4.5.0 requires scipy, which is not installed.\nstatsmodels 0.14.4 requires pandas!=2.1.0,>=1.4, which is not installed.\nstatsmodels 0.14.4 requires scipy!=1.9.2,>=1.8, which is not installed.\nholoviews 1.20.2 requires pandas>=1.3, which is not installed.\nplotnine 0.14.5 requires matplotlib>=3.8.0, which is not installed.\nplotnine 0.14.5 requires pandas>=2.2.0, which is not installed.\nplotnine 0.14.5 requires scipy>=1.8.0, which is not installed.\nwordcloud 1.9.4 requires matplotlib, which is not installed.\ndb-dtypes 1.4.3 requires pandas>=1.5.3, which is not installed.\ncufflinks 0.17.3 requires pandas>=0.19.2, which is not installed.\npanel 1.7.1 requires pandas>=1.2, which is not installed.\npynndescent 0.5.13 requires scikit-learn>=0.18, which is not installed.\npynndescent 0.5.13 requires scipy>=1.0, which is not installed.\npytensor 2.31.4 requires scipy<2,>=1, which is not installed.\nhdbscan 0.8.40 requires scikit-learn>=0.20, which is not installed.\nhdbscan 0.8.40 requires scipy>=1.0, which is not installed.\nbigframes 2.8.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\nbigframes 2.8.0 requires matplotlib>=3.7.1, which is not installed.\nbigframes 2.8.0 requires pandas>=1.5.3, which is not installed.\nxarray 2025.3.1 requires pandas>=2.1, which is not installed.\njaxlib 0.5.1 requires scipy>=1.11.1, which is not installed.\nstumpy 1.13.0 requires scipy>=1.10, which is not installed.\nyellowbrick 1.5 requires matplotlib!=3.0.0,>=2.0.2, which is not installed.\nyellowbrick 1.5 requires scikit-learn>=1.0.0, which is not installed.\nyellowbrick 1.5 requires scipy>=1.0.0, which is not installed.\nmlxtend 0.23.4 requires matplotlib>=3.0.0, which is not installed.\nmlxtend 0.23.4 requires pandas>=0.24.2, which is not installed.\nmlxtend 0.23.4 requires scikit-learn>=1.3.1, which is not installed.\nmlxtend 0.23.4 requires scipy>=1.2.1, which is not installed.\nosqp 1.0.4 requires scipy>=0.13.2, which is not installed.\ngensim 4.3.3 requires numpy<2.0,>=1.18.5, but you have numpy 2.1.2 which is incompatible.\nmkl-umath 0.1.1 requires numpy<1.27.0,>=1.26.4, but you have numpy 2.1.2 which is incompatible.\nmkl-random 1.2.4 requires numpy<1.27.0,>=1.26.4, but you have numpy 2.1.2 which is incompatible.\nmkl-fft 1.3.8 requires numpy<1.27.0,>=1.26.4, but you have numpy 2.1.2 which is incompatible.\nnumba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.1.2 which is incompatible.\nonnx 1.18.0 requires protobuf>=4.25.1, but you have protobuf 3.20.3 which is incompatible.\ndopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\npandas-gbq 0.29.1 requires google-api-core<3.0.0,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\nfastai 2.7.19 requires torch<2.7,>=1.10, but you have torch 2.7.1+cu118 which is incompatible.\nbigframes 2.8.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\nbigframes 2.8.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\n","output_type":"stream"},{"name":"stdout","text":"Successfully installed numpy-2.1.2 nvidia-cublas-cu11-11.11.3.6 nvidia-cuda-cupti-cu11-11.8.87 nvidia-cuda-nvrtc-cu11-11.8.89 nvidia-cuda-runtime-cu11-11.8.89 nvidia-cudnn-cu11-9.1.0.70 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.3.0.86 nvidia-cusolver-cu11-11.4.1.48 nvidia-cusparse-cu11-11.7.5.86 nvidia-nccl-cu11-2.21.5 nvidia-nvtx-cu11-11.8.86 sympy-1.13.3 torch-2.7.1+cu118 torchaudio-2.7.1+cu118 torchvision-0.22.1+cu118 triton-3.3.1\nHoàn tất cài đặt Torch.\n\nĐang cài đặt transformers==4.31.0...\nCollecting transformers==4.31.0\n  Using cached transformers-4.31.0-py3-none-any.whl.metadata (116 kB)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.31.0) (3.18.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.31.0) (0.33.1)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.31.0) (2.1.2)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.31.0) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.31.0) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.31.0) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.31.0) (2.32.4)\nCollecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers==4.31.0)\n  Using cached tokenizers-0.13.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\nRequirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.31.0) (0.5.3)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.31.0) (4.67.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.31.0) (2025.3.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.31.0) (4.14.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.31.0) (1.1.5)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.31.0) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.31.0) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.31.0) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.31.0) (2025.6.15)\nUsing cached transformers-4.31.0-py3-none-any.whl (7.4 MB)\nUsing cached tokenizers-0.13.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\nInstalling collected packages: tokenizers, transformers\n","output_type":"stream"},{"name":"stderr","text":"ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nkaggle-environments 1.17.6 requires scipy>=1.11.2, which is not installed.\nsentence-transformers 4.1.0 requires scikit-learn, which is not installed.\nsentence-transformers 4.1.0 requires scipy, which is not installed.\nkaggle-environments 1.17.6 requires transformers>=4.33.1, but you have transformers 4.31.0 which is incompatible.\nsentence-transformers 4.1.0 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.31.0 which is incompatible.\n","output_type":"stream"},{"name":"stdout","text":"Successfully installed tokenizers-0.13.3 transformers-4.31.0\nHoàn tất cài đặt transformers.\n\nĐang cài đặt accelerate, bitsandbytes, peft, trl...\nCollecting accelerate\n  Downloading accelerate-1.9.0-py3-none-any.whl.metadata (19 kB)\nCollecting bitsandbytes\n  Downloading bitsandbytes-0.46.1-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\nCollecting peft\n  Downloading peft-0.16.0-py3-none-any.whl.metadata (14 kB)\nCollecting trl\n  Downloading trl-0.20.0-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.1.2)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (25.0)\nRequirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (7.0.0)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from accelerate) (6.0.2)\nRequirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.7.1+cu118)\nRequirement already satisfied: huggingface_hub>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (0.33.1)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate) (0.5.3)\nRequirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (from peft) (4.31.0)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from peft) (4.67.1)\nRequirement already satisfied: datasets>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from trl) (3.6.0)\nCollecting transformers (from peft)\n  Downloading transformers-4.54.1-py3-none-any.whl.metadata (41 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 41.7/41.7 kB 2.3 MB/s eta 0:00:00\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=3.0.0->trl) (3.18.0)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=3.0.0->trl) (19.0.1)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=3.0.0->trl) (0.3.8)\nCollecting pandas (from datasets>=3.0.0->trl)\n  Downloading pandas-2.3.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (91 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 91.2/91.2 kB 4.2 MB/s eta 0:00:00\nRequirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets>=3.0.0->trl) (2.32.4)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets>=3.0.0->trl) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets>=3.0.0->trl) (0.70.16)\nRequirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (2025.3.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.21.0->accelerate) (4.14.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.21.0->accelerate) (1.1.5)\nRequirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (1.13.3)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu11==11.8.89 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.8.89)\nRequirement already satisfied: nvidia-cuda-runtime-cu11==11.8.89 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.8.89)\nRequirement already satisfied: nvidia-cuda-cupti-cu11==11.8.87 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.8.87)\nRequirement already satisfied: nvidia-cudnn-cu11==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu11==11.11.3.6 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.11.3.6)\nRequirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (10.9.0.58)\nRequirement already satisfied: nvidia-curand-cu11==10.3.0.86 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (10.3.0.86)\nRequirement already satisfied: nvidia-cusolver-cu11==11.4.1.48 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.4.1.48)\nRequirement already satisfied: nvidia-cusparse-cu11==11.7.5.86 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.7.5.86)\nRequirement already satisfied: nvidia-nccl-cu11==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu11==11.8.86 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.8.86)\nRequirement already satisfied: triton==3.3.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.3.1)\nRequirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from triton==3.3.1->torch>=2.0.0->accelerate) (75.2.0)\nCollecting huggingface_hub>=0.21.0 (from accelerate)\n  Downloading huggingface_hub-0.34.3-py3-none-any.whl.metadata (14 kB)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers->peft) (2024.11.6)\nCollecting tokenizers<0.22,>=0.21 (from transformers->peft)\n  Downloading tokenizers-0.21.4-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (3.12.13)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (2025.6.15)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=3.0.0->trl) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=3.0.0->trl) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=3.0.0->trl) (2025.2)\nRequirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (1.7.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (6.6.3)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (0.3.2)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (1.20.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=3.0.0->trl) (1.17.0)\nDownloading accelerate-1.9.0-py3-none-any.whl (367 kB)\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 367.1/367.1 kB 16.3 MB/s eta 0:00:00\nDownloading bitsandbytes-0.46.1-py3-none-manylinux_2_24_x86_64.whl (72.9 MB)\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 72.9/72.9 MB 23.8 MB/s eta 0:00:00\nDownloading peft-0.16.0-py3-none-any.whl (472 kB)\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 472.3/472.3 kB 25.9 MB/s eta 0:00:00\nDownloading trl-0.20.0-py3-none-any.whl (504 kB)\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 504.6/504.6 kB 32.3 MB/s eta 0:00:00\nDownloading transformers-4.54.1-py3-none-any.whl (11.2 MB)\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 11.2/11.2 MB 107.1 MB/s eta 0:00:00\nDownloading huggingface_hub-0.34.3-py3-none-any.whl (558 kB)\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 558.8/558.8 kB 33.2 MB/s eta 0:00:00\nDownloading tokenizers-0.21.4-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.1/3.1 MB 95.3 MB/s eta 0:00:00\nDownloading pandas-2.3.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.4 MB)\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 12.4/12.4 MB 104.2 MB/s eta 0:00:00\nInstalling collected packages: pandas, huggingface_hub, tokenizers, bitsandbytes, accelerate, transformers, trl, peft\n  Attempting uninstall: huggingface_hub\n    Found existing installation: huggingface-hub 0.33.1\n    Uninstalling huggingface-hub-0.33.1:\n      Successfully uninstalled huggingface-hub-0.33.1\n  Attempting uninstall: tokenizers\n    Found existing installation: tokenizers 0.13.3\n    Uninstalling tokenizers-0.13.3:\n      Successfully uninstalled tokenizers-0.13.3\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.31.0\n    Uninstalling transformers-4.31.0:\n      Successfully uninstalled transformers-4.31.0\n","output_type":"stream"},{"name":"stderr","text":"ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nlibpysal 4.9.2 requires scipy>=1.8, which is not installed.\ncuml-cu12 25.2.1 requires scipy>=1.8.0, which is not installed.\nwoodwork 0.31.0 requires scikit-learn>=1.1.0, which is not installed.\nwoodwork 0.31.0 requires scipy>=1.10.0, which is not installed.\nydata-profiling 4.16.1 requires matplotlib<=3.10,>=3.5, which is not installed.\nydata-profiling 4.16.1 requires scipy<1.16,>=1.4.1, which is not installed.\nfeaturetools 1.31.0 requires scipy>=1.10.0, which is not installed.\nplotly-express 0.4.1 requires scipy>=0.18, which is not installed.\ncatboost 1.2.8 requires matplotlib, which is not installed.\ncatboost 1.2.8 requires scipy, which is not installed.\ntpot 0.12.1 requires scikit-learn>=0.22.0, which is not installed.\ntpot 0.12.1 requires scipy>=1.3.1, which is not installed.\nshap 0.44.1 requires scikit-learn, which is not installed.\nshap 0.44.1 requires scipy, which is not installed.\npyupset 0.1.1.post7 requires matplotlib, which is not installed.\nkaggle-environments 1.17.6 requires scipy>=1.11.2, which is not installed.\npymc3 3.11.4 requires scipy>=1.2.0, which is not installed.\npyldavis 3.4.1 requires scikit-learn>=1.0.0, which is not installed.\npyldavis 3.4.1 requires scipy, which is not installed.\nphik 0.12.4 requires matplotlib>=2.2.3, which is not installed.\nphik 0.12.4 requires scipy>=1.5.2, which is not installed.\nnilearn 0.10.4 requires scikit-learn>=1.0.0, which is not installed.\nnilearn 0.10.4 requires scipy>=1.8.0, which is not installed.\nstable-baselines3 2.1.0 requires matplotlib, which is not installed.\nseaborn 0.12.2 requires matplotlib!=3.6.1,>=3.1, which is not installed.\ncategory-encoders 2.7.0 requires scikit-learn<1.6.0,>=1.0.0, which is not installed.\ncategory-encoders 2.7.0 requires scipy>=1.0.0, which is not installed.\ncesium 0.12.4 requires scikit-learn>=0.22.1, which is not installed.\ncesium 0.12.4 requires scipy>=0.16.0, which is not installed.\nhep-ml 0.8.0 requires scikit-learn>=1.0, which is not installed.\nhep-ml 0.8.0 requires scipy>=1.0.0, which is not installed.\ntsfresh 0.21.0 requires scikit-learn>=0.22.0, which is not installed.\ntsfresh 0.21.0 requires scipy>=1.14.0; python_version >= \"3.10\", which is not installed.\nprophet 1.1.7 requires matplotlib>=2.0.0, which is not installed.\ngeemap 0.35.3 requires matplotlib, which is not installed.\ndopamine-rl 4.1.2 requires tensorflow>=2.2.0, which is not installed.\nsklearn-pandas 2.2.0 requires scikit-learn>=0.23.0, which is not installed.\nsklearn-pandas 2.2.0 requires scipy>=1.5.1, which is not installed.\ndatascience 0.17.6 requires matplotlib>=3.0.0, which is not installed.\ndatascience 0.17.6 requires scipy, which is not installed.\nsentence-transformers 4.1.0 requires scikit-learn, which is not installed.\nsentence-transformers 4.1.0 requires scipy, which is not installed.\nmissingno 0.5.2 requires matplotlib, which is not installed.\nmissingno 0.5.2 requires scipy, which is not installed.\narviz 0.21.0 requires matplotlib>=3.5, which is not installed.\narviz 0.21.0 requires scipy>=1.9.0, which is not installed.\npymc 5.23.0 requires scipy>=1.4.1, which is not installed.\ntensorflow-decision-forests 1.11.0 requires tensorflow==2.18.0, which is not installed.\nfastai 2.7.19 requires matplotlib, which is not installed.\nfastai 2.7.19 requires scikit-learn, which is not installed.\nfastai 2.7.19 requires scipy, which is not installed.\nmizani 0.13.5 requires scipy>=1.8.0, which is not installed.\nstatsmodels 0.14.4 requires scipy!=1.9.2,>=1.8, which is not installed.\nplotnine 0.14.5 requires matplotlib>=3.8.0, which is not installed.\nplotnine 0.14.5 requires scipy>=1.8.0, which is not installed.\nbigframes 2.8.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\nbigframes 2.8.0 requires matplotlib>=3.7.1, which is not installed.\nmlxtend 0.23.4 requires matplotlib>=3.0.0, which is not installed.\nmlxtend 0.23.4 requires scikit-learn>=1.3.1, which is not installed.\nmlxtend 0.23.4 requires scipy>=1.2.1, which is not installed.\ndask-cudf-cu12 25.2.2 requires pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.1 which is incompatible.\ncudf-cu12 25.2.2 requires pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.1 which is incompatible.\ngoogle-colab 1.0.0 requires google-auth==2.38.0, but you have google-auth 2.40.3 which is incompatible.\ngoogle-colab 1.0.0 requires notebook==6.5.7, but you have notebook 6.5.4 which is incompatible.\ngoogle-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.3.1 which is incompatible.\ngoogle-colab 1.0.0 requires requests==2.32.3, but you have requests 2.32.4 which is incompatible.\ngoogle-colab 1.0.0 requires tornado==6.4.2, but you have tornado 6.5.1 which is incompatible.\ndopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\npandas-gbq 0.29.1 requires google-api-core<3.0.0,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\nfastai 2.7.19 requires torch<2.7,>=1.10, but you have torch 2.7.1+cu118 which is incompatible.\nbigframes 2.8.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\nbigframes 2.8.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\n","output_type":"stream"},{"name":"stdout","text":"Successfully installed accelerate-1.9.0 bitsandbytes-0.46.1 huggingface_hub-0.34.3 pandas-2.3.1 peft-0.16.0 tokenizers-0.21.4 transformers-4.54.1 trl-0.20.0\nHoàn tất cài đặt các thư viện LLM.\n\nĐang cài đặt numpy==1.26.4 và các thư viện phụ thuộc...\nCollecting numpy==1.26.4\n  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 61.0/61.0 kB 2.3 MB/s eta 0:00:00\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.3.1)\nCollecting scipy\n  Downloading scipy-1.16.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (61 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 62.0/62.0 kB 4.1 MB/s eta 0:00:00\nCollecting scikit-learn\n  Downloading scikit_learn-1.7.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (11 kB)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\nRequirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.1)\nRequirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\nDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 18.3/18.3 MB 31.4 MB/s eta 0:00:00\nDownloading scipy-1.16.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (35.4 MB)\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 35.4/35.4 MB 30.5 MB/s eta 0:00:00\nDownloading scikit_learn-1.7.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (9.7 MB)\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 9.7/9.7 MB 43.1 MB/s eta 0:00:00\nInstalling collected packages: numpy, scipy, scikit-learn\n  Attempting uninstall: numpy\n    Found existing installation: numpy 2.1.2\n    Uninstalling numpy-2.1.2:\n      Successfully uninstalled numpy-2.1.2\n","output_type":"stream"},{"name":"stderr","text":"ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nydata-profiling 4.16.1 requires matplotlib<=3.10,>=3.5, which is not installed.\ncatboost 1.2.8 requires matplotlib, which is not installed.\ncartopy 0.24.1 requires matplotlib>=3.6, which is not installed.\npyupset 0.1.1.post7 requires matplotlib, which is not installed.\nscikit-plot 0.3.7 requires matplotlib>=1.4.0, which is not installed.\nphik 0.12.4 requires matplotlib>=2.2.3, which is not installed.\nipympl 0.9.7 requires matplotlib<4,>=3.5.0, which is not installed.\nstable-baselines3 2.1.0 requires matplotlib, which is not installed.\nseaborn 0.12.2 requires matplotlib!=3.6.1,>=3.1, which is not installed.\nlime 0.2.0.1 requires matplotlib, which is not installed.\nmne 1.9.0 requires matplotlib>=3.6, which is not installed.\nprophet 1.1.7 requires matplotlib>=2.0.0, which is not installed.\ngeemap 0.35.3 requires matplotlib, which is not installed.\ndopamine-rl 4.1.2 requires tensorflow>=2.2.0, which is not installed.\nmatplotlib-venn 1.1.2 requires matplotlib, which is not installed.\ndatascience 0.17.6 requires matplotlib>=3.0.0, which is not installed.\nmusic21 9.3.0 requires matplotlib, which is not installed.\nmissingno 0.5.2 requires matplotlib, which is not installed.\narviz 0.21.0 requires matplotlib>=3.5, which is not installed.\ntensorflow-decision-forests 1.11.0 requires tensorflow==2.18.0, which is not installed.\nfastai 2.7.19 requires matplotlib, which is not installed.\nplotnine 0.14.5 requires matplotlib>=3.8.0, which is not installed.\nwordcloud 1.9.4 requires matplotlib, which is not installed.\nbigframes 2.8.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\nbigframes 2.8.0 requires matplotlib>=3.7.1, which is not installed.\nyellowbrick 1.5 requires matplotlib!=3.0.0,>=2.0.2, which is not installed.\nmlxtend 0.23.4 requires matplotlib>=3.0.0, which is not installed.\ngensim 4.3.3 requires scipy<1.14.0,>=1.7.0, but you have scipy 1.16.1 which is incompatible.\ndask-cudf-cu12 25.2.2 requires pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.1 which is incompatible.\ncudf-cu12 25.2.2 requires pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.1 which is incompatible.\nydata-profiling 4.16.1 requires scipy<1.16,>=1.4.1, but you have scipy 1.16.1 which is incompatible.\nonnx 1.18.0 requires protobuf>=4.25.1, but you have protobuf 3.20.3 which is incompatible.\ncategory-encoders 2.7.0 requires scikit-learn<1.6.0,>=1.0.0, but you have scikit-learn 1.7.1 which is incompatible.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\ngoogle-colab 1.0.0 requires google-auth==2.38.0, but you have google-auth 2.40.3 which is incompatible.\ngoogle-colab 1.0.0 requires notebook==6.5.7, but you have notebook 6.5.4 which is incompatible.\ngoogle-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.3.1 which is incompatible.\ngoogle-colab 1.0.0 requires requests==2.32.3, but you have requests 2.32.4 which is incompatible.\ngoogle-colab 1.0.0 requires tornado==6.4.2, but you have tornado 6.5.1 which is incompatible.\ndopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\nsklearn-compat 0.1.3 requires scikit-learn<1.7,>=1.2, but you have scikit-learn 1.7.1 which is incompatible.\npandas-gbq 0.29.1 requires google-api-core<3.0.0,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\nthinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\nfastai 2.7.19 requires torch<2.7,>=1.10, but you have torch 2.7.1+cu118 which is incompatible.\nbigframes 2.8.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\nbigframes 2.8.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\n","output_type":"stream"},{"name":"stdout","text":"Successfully installed numpy-1.26.4 scikit-learn-1.7.1 scipy-1.16.1\nHoàn tất cài đặt numpy và các thư viện phụ thuộc.\n\nĐang cài đặt và nâng cấp openprompt, tokenizers...\nCollecting openprompt\n  Using cached openprompt-1.0.1-py3-none-any.whl.metadata (10 kB)\nRequirement already satisfied: tokenizers in /usr/local/lib/python3.11/dist-packages (0.21.4)\nRequirement already satisfied: transformers>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from openprompt) (4.54.1)\nRequirement already satisfied: sentencepiece==0.1.96 in /usr/local/lib/python3.11/dist-packages (from openprompt) (0.1.96)\nRequirement already satisfied: tqdm>=4.62.2 in /usr/local/lib/python3.11/dist-packages (from openprompt) (4.67.1)\nRequirement already satisfied: tensorboardX in /usr/local/lib/python3.11/dist-packages (from openprompt) (2.6.4)\nRequirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from openprompt) (3.9.1)\nRequirement already satisfied: yacs in /usr/local/lib/python3.11/dist-packages (from openprompt) (0.1.8)\nRequirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from openprompt) (0.3.8)\nRequirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (from openprompt) (3.6.0)\nRequirement already satisfied: rouge==1.0.0 in /usr/local/lib/python3.11/dist-packages (from openprompt) (1.0.0)\nRequirement already satisfied: pyarrow in /usr/local/lib/python3.11/dist-packages (from openprompt) (19.0.1)\nRequirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from openprompt) (1.16.1)\nRequirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from rouge==1.0.0->openprompt) (1.17.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.11/dist-packages (from tokenizers) (0.34.3)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (3.18.0)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (2025.3.0)\nRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (6.0.2)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (2.32.4)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (4.14.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (1.1.5)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.10.0->openprompt) (1.26.4)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.10.0->openprompt) (2024.11.6)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.10.0->openprompt) (0.5.3)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets->openprompt) (2.3.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets->openprompt) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets->openprompt) (0.70.16)\nRequirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->openprompt) (8.2.1)\nRequirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->openprompt) (1.5.1)\nRequirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.11/dist-packages (from tensorboardX->openprompt) (3.20.3)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets->openprompt) (3.12.13)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (2025.6.15)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets->openprompt) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets->openprompt) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets->openprompt) (2025.2)\nRequirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->openprompt) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->openprompt) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->openprompt) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->openprompt) (1.7.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->openprompt) (6.6.3)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->openprompt) (0.3.2)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->openprompt) (1.20.1)\nUsing cached openprompt-1.0.1-py3-none-any.whl (146 kB)\nInstalling collected packages: openprompt\nSuccessfully installed openprompt-1.0.1\nHoàn tất cài đặt openprompt.\n","output_type":"stream"}],"execution_count":40},{"cell_type":"code","source":"# BẢN VÁ CHO OPENPROMPT - SỬA LỖI IMPORT ADAMW trong nhiều file\nprint(\"\\nĐang áp dụng bản vá AdamW cho openprompt (kiểm tra nhiều file)...\")\npatch_adamw_content = \"\"\"\nimport os\n\nfiles_to_patch = [\n    \"/usr/local/lib/python3.11/dist-packages/openprompt/pipeline_base.py\",\n    \"/usr/local/lib/python3.11/dist-packages/openprompt/trainer.py\",\n    \"/usr/local/lib/python3.11/dist-packages/openprompt/protoverb_trainer.py\"\n]\n\nnew_adamw_import_line = \"from torch.optim import AdamW\"\nnew_schedule_import_line = \"from transformers import get_linear_schedule_with_warmup\"\n\nfor file_path in files_to_patch:\n    print(f\"\\\\nĐang kiểm tra và vá file: {{file_path}}...\")\n    try:\n        with open(file_path, \"r\") as f:\n            lines = f.readlines()\n\n        patched_this_file = False\n        new_lines = []\n        for i, line in enumerate(lines):\n            original_line = line.strip()\n            # Kiểm tra AdamW import từ transformers hoặc transformers.optimization\n            if (\"AdamW\" in original_line and \"from transformers\" in original_line and\n                \"torch.optim\" not in original_line):\n                # Dòng này chứa import AdamW có vấn đề\n                if \"get_linear_schedule_with_warmup\" in original_line:\n                    # Thay thế dòng này bằng hai dòng mới\n                    new_lines.append(f\"{new_adamw_import_line}\\\\n\")\n                    new_lines.append(f\"{new_schedule_import_line}\\\\n\")\n                    print(f\"  Đã vá dòng {{i + 1}}: '{{original_line}}' -> Sử dụng torch.optim.AdamW và transformers.get_linear_schedule_with_warmup.\")\n                else:\n                    # Chỉ thay thế AdamW\n                    new_lines.append(f\"{new_adamw_import_line}\\\\n\")\n                    print(f\"  Đã vá dòng {{i + 1}}: '{{original_line}}' -> Sử dụng torch.optim.AdamW.\")\n                patched_this_file = True\n            elif (new_adamw_import_line in original_line and\n                  (\"get_linear_schedule_with_warmup\" not in original_line or new_schedule_import_line in original_line)):\n                # Dòng đã đúng định dạng (AdamW từ torch.optim, và get_linear_schedule_with_warmap nếu có)\n                new_lines.append(line)\n                print(f\"  Dòng {{i + 1}} đã đúng định dạng.\")\n            else:\n                new_lines.append(line) # Giữ nguyên dòng\n\n        if patched_this_file:\n            with open(file_path, \"w\") as f:\n                f.writelines(new_lines)\n            print(f\"Hoàn tất vá lỗi import AdamW trong '{{file_path}}'.\")\n        else:\n            print(f\"Không tìm thấy dòng AdamW cần vá trong '{{file_path}}' (hoặc đã đúng định dạng).\")\n\n    except FileNotFoundError:\n        print(f\"Lỗi: Không tìm thấy file '{{file_path}}'. openprompt có thể chưa được cài đặt hoặc đường dẫn đã thay đổi.\")\n    except Exception as e:\n        print(f\"Đã xảy ra lỗi khi vá AdamW trong '{{file_path}}': {{e}}\")\nprint(\"\\\\nHoàn tất kiểm tra và vá tất cả các file openprompt liên quan đến AdamW.\")\n\"\"\"\nwith open(\"patch_openprompt_adamw.py\", \"w\") as f:\n    f.write(patch_adamw_content)\nos.system('python patch_openprompt_adamw.py')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-30T05:21:37.571903Z","iopub.execute_input":"2025-07-30T05:21:37.572135Z","iopub.status.idle":"2025-07-30T05:21:37.684082Z","shell.execute_reply.started":"2025-07-30T05:21:37.572116Z","shell.execute_reply":"2025-07-30T05:21:37.683500Z"}},"outputs":[{"name":"stdout","text":"\nĐang áp dụng bản vá AdamW cho openprompt (kiểm tra nhiều file)...\n\nĐang kiểm tra và vá file: {file_path}...\n  Đã vá dòng {i + 1}: '{original_line}' -> Sử dụng torch.optim.AdamW và transformers.get_linear_schedule_with_warmup.\nHoàn tất vá lỗi import AdamW trong '{file_path}'.\n\nĐang kiểm tra và vá file: {file_path}...\n  Đã vá dòng {i + 1}: '{original_line}' -> Sử dụng torch.optim.AdamW và transformers.get_linear_schedule_with_warmup.\nHoàn tất vá lỗi import AdamW trong '{file_path}'.\n\nĐang kiểm tra và vá file: {file_path}...\n  Đã vá dòng {i + 1}: '{original_line}' -> Sử dụng torch.optim.AdamW và transformers.get_linear_schedule_with_warmup.\nHoàn tất vá lỗi import AdamW trong '{file_path}'.\n\nHoàn tất kiểm tra và vá tất cả các file openprompt liên quan đến AdamW.\n","output_type":"stream"},{"execution_count":41,"output_type":"execute_result","data":{"text/plain":"0"},"metadata":{}}],"execution_count":41},{"cell_type":"code","source":"# BẢN VÁ CHO OPENPROMPT - SỬA LỖI IMPORT GENERATIONMIXIN\nprint(\"\\nĐang áp dụng bản vá GenerationMixin cho openprompt...\")\npatch_generationmixin_content = \"\"\"\nimport os\n\nfile_path = \"/usr/local/lib/python3.11/dist-packages/openprompt/pipeline_base.py\"\n\ntry:\n    # Đọc nội dung hiện tại của file\n    with open(file_path, \"r\") as f:\n        content = f.read()\n\n    # Thay thế dòng import cũ bằng dòng mới\n    if \"from transformers.generation_utils import GenerationMixin\" in content:\n        content = content.replace(\n            \"from transformers.generation_utils import GenerationMixin\",\n            \"from transformers import GenerationMixin\"\n        )\n        print(f\"Đã cập nhật import cho GenerationMixin trong file '{file_path}'.\")\n    else:\n        print(f\"Cảnh báo: Không tìm thấy dòng import GenerationMixin cũ trong '{file_path}'. File có thể đã được cập nhật hoặc cần kiểm tra thủ công.\")\n\n    # Ghi nội dung đã sửa đổi trở lại vào file\n    with open(file_path, \"w\") as f:\n        f.write(content)\n    print(\"Hoàn tất sửa lỗi import GenerationMixin.\")\nexcept FileNotFoundError:\n    print(f\"Lỗi: Không tìm thấy file '{file_path}'. openprompt có thể chưa được cài đặt hoặc đường dẫn đã thay đổi.\")\nexcept Exception as e:\n    print(f\"Đã xảy ra lỗi khi vá GenerationMixin: {e}\")\n\"\"\"\nwith open(\"patch_openprompt_generationmixin.py\", \"w\") as f:\n    f.write(patch_generationmixin_content)\nos.system('python patch_openprompt_generationmixin.py')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-30T05:21:37.684844Z","iopub.execute_input":"2025-07-30T05:21:37.685102Z","iopub.status.idle":"2025-07-30T05:21:37.794739Z","shell.execute_reply.started":"2025-07-30T05:21:37.685084Z","shell.execute_reply":"2025-07-30T05:21:37.794108Z"}},"outputs":[{"name":"stdout","text":"\nĐang áp dụng bản vá GenerationMixin cho openprompt...\nĐã cập nhật import cho GenerationMixin trong file '/usr/local/lib/python3.11/dist-packages/openprompt/pipeline_base.py'.\nHoàn tất sửa lỗi import GenerationMixin.\n","output_type":"stream"},{"execution_count":42,"output_type":"execute_result","data":{"text/plain":"0"},"metadata":{}}],"execution_count":42},{"cell_type":"code","source":"%%writefile /kaggle/working/DecT/src/dect_verbalizer.py\nimport torch\nimport torch.nn as nn\nfrom openprompt import Verbalizer\nfrom openprompt.utils.logging import logger\nimport json\nimport torch.nn.functional as F \nclass DecTVerbalizer(Verbalizer):\n    def __init__(self, \n                 tokenizer,\n                 classes,\n                 hidden_size=4096,  # Mistral 7B hidden size\n                 mid_dim=256,\n                 lr=1e-4,\n                 epochs=5,\n                 model_logits_weight=1.0):\n        \n        super().__init__(tokenizer=tokenizer, classes=classes)\n        self.hidden_size = hidden_size\n        self.mid_dim = mid_dim\n        self.lr = lr\n        self.epochs = epochs\n        self.model_logits_weight = model_logits_weight\n        \n        # Projection layer\n        self.head = nn.Linear(hidden_size, mid_dim)\n        \n        # Prototypes\n        self.prototypes = nn.Parameter(torch.randn(len(classes), mid_dim))\n        nn.init.xavier_uniform_(self.prototypes)\n        \n        # Prototype radii\n        self.radii = nn.Parameter(torch.ones(len(classes)))\n\n    def from_file(self, path: str):\n        \"\"\"Load label words from JSON file\"\"\"\n        try:\n            with open(path, 'r', encoding='utf-8') as f:\n                label_words = json.load(f)\n                \n                # Handle both index-based and class-name-based keys\n                self.label_words = []\n                for i, cls in enumerate(self.classes):\n                    if str(i) in label_words:\n                        self.label_words.append(label_words[str(i)])\n                    elif cls in label_words:\n                        self.label_words.append(label_words[cls])\n                    else:\n                        raise KeyError(f\"Missing label words for class {cls}\")\n                \n                logger.info(f\"Loaded label words from {path}\")\n                return self\n                \n        except Exception as e:\n            logger.error(f\"Error loading verbalizer: {str(e)}\")\n            raise\n\n    def project(self, hidden_states):\n        \"\"\"Project hidden states to prototype space\"\"\"\n        hidden_states = hidden_states.to(torch.float32)  # Cast to float32 to match linear layer dtype\n        return self.head(hidden_states)\n\n    def forward(self, hidden_states):\n        \"\"\"Compute similarity between embeddings and prototypes\"\"\"\n        projected = self.project(hidden_states)\n        projected = F.normalize(projected, p=2, dim=-1)\n        prototypes = F.normalize(self.prototypes, p=2, dim=-1)\n        return torch.matmul(projected, prototypes.t()) * torch.exp(self.radii)\n\n    def train_proto(self, model, train_loader, calib_loader=None):\n        \"\"\"Train the prototypes\"\"\"\n        self.train()\n        optimizer = torch.optim.Adam([\n            {'params': self.head.parameters()},\n            {'params': [self.prototypes, self.radii]}\n        ], lr=self.lr)\n        \n        device = next(model.parameters()).device\n        self.to(device)\n        \n        for epoch in range(self.epochs):\n            total_loss = 0.0\n            for batch in train_loader:\n                batch = batch.to(device)\n                \n                with torch.no_grad():\n                    outputs = model.prompt_model(batch)\n                    hidden_states = outputs.hidden_states[-1]\n                    seq_lens = batch.attention_mask.sum(dim=1) - 1\n                    h = hidden_states[torch.arange(hidden_states.size(0)), seq_lens]\n                \n                logits = self(h)\n                loss = F.cross_entropy(logits/0.1, batch.label, label_smoothing=0.1)\n                \n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n                \n                total_loss += loss.item()\n            \n            logger.info(f\"Epoch {epoch+1}/{self.epochs} - Loss: {total_loss/len(train_loader):.4f}\")\n        \n        self.eval()\n        return self","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-30T05:21:37.795560Z","iopub.execute_input":"2025-07-30T05:21:37.795849Z","iopub.status.idle":"2025-07-30T05:21:37.802277Z","shell.execute_reply.started":"2025-07-30T05:21:37.795824Z","shell.execute_reply":"2025-07-30T05:21:37.801694Z"}},"outputs":[{"name":"stdout","text":"Overwriting /kaggle/working/DecT/src/dect_verbalizer.py\n","output_type":"stream"}],"execution_count":43},{"cell_type":"code","source":"# Dán toàn bộ code này vào cell đã dùng để tạo template/verbalizer\n\nimport os\nimport json\n\nprint(\" BẮT ĐẦU: Tạo lại tất cả các file template và verbalizer với các KEY ĐÃ ĐỒNG BỘ...\")\n\nconfigs = {\n    # --- Phân loại câu đơn ---\n    \"agnews\": (\n        'Topic: {\"mask\"}. Content: {\"placeholder\":\"text_a\"} {\"placeholder\":\"text_b\"}',\n        { \"0\": [\"World\"], \"1\": [\"Sports\"], \"2\": [\"Business\"], \"3\": [\"Technology\", \"Science\"] }\n    ),\n    \"imdb\": (\n        '{\"placeholder\":\"text_a\"} . This movie is {\"mask\"}.',\n        { \"0\": [\"bad\", \"terrible\"], \"1\": [\"good\", \"great\"] }\n    ),\n    \"sst2\": (\n        'Classify the sentiment of this text: {\"placeholder\":\"text_a\"}. Sentiment: {\"mask\"}.',\n        {\n            \"0\": [\"negative\"],\n            \"1\": [\"positive\"]\n        }\n    ),\n    \"yahoo\": (\n        'Topic: {\"mask\"}. Content: {\"placeholder\":\"text_a\"} {\"placeholder\":\"text_b\"}',\n        {\n            \"0\": [\"Society\", \"Culture\"], \"1\": [\"Science\", \"Mathematics\"], \"2\": [\"Health\"],\n            \"3\": [\"Education\", \"Reference\"], \"4\": [\"Computers\", \"Internet\"], \"5\": [\"Sports\"],\n            \"6\": [\"Business\", \"Finance\"], \"7\": [\"Entertainment\", \"Music\"],\n            \"8\": [\"Family\", \"Relationships\"], \"9\": [\"Politics\", \"Government\"]\n        }\n    ),\n    \"yelp\": (\n        'Review: {\"placeholder\":\"text_a\"} sentiment: {\"mask\"}.',\n        { \"1\": [\"bad\", \"terrible\"], \"2\": [\"good\", \"great\"] }\n    ),\n    \"dbpedia\": (\n        'This piece of text is about {\"mask\"}. {\"placeholder\":\"text_a\"} {\"placeholder\":\"text_b\"}',\n        {\n            \"0\": [\"Company\"], \"1\": [\"School\"], \"2\": [\"Artist\"], \"3\": [\"Athlete\"],\n            \"4\": [\"Office Holder\", \"Politician\"], \"5\": [\"Mean of Transportation\", \"Transport\"],\n            \"6\": [\"Building\"], \"7\": [\"Natural Place\", \"River\"], \"8\": [\"Village\"],\n            \"9\": [\"Animal\"], \"10\": [\"Plant\"], \"11\": [\"Album\"], \"12\": [\"Film\"], \"13\": [\"Written Work\", \"Book\"]\n        }\n    ),\n\n    # ==============================================================================\n    # == ĐÂY LÀ PHẦN SỬA LỖI QUAN TRỌNG CHO CÁC DATASET ENTAILMENT ===============\n    # ==============================================================================\n    \"rte\": (\n        '{\"placeholder\":\"text_a\"} ? {\"mask\"}, {\"placeholder\":\"text_b\"}',\n        { \"not_entailment\": [\"No\"], \"entailment\": [\"Yes\"] } # Key khớp với RteProcessor\n    ),\n    \"snli\": (\n        '{\"placeholder\":\"text_a\"} ? {\"mask\"}, {\"placeholder\":\"text_b\"}',\n        { \"contradiction\": [\"contradiction\"], \"entailment\": [\"entailment\"], \"neutral\": [\"neutral\"] } # Key khớp với SnliProcessor\n    ),\n    \"mnli-m\": (\n        '{\"placeholder\":\"text_a\"} ? {\"mask\"}, {\"placeholder\":\"text_b\"}',\n        { \"contradiction\": [\"contradiction\"], \"entailment\": [\"entailment\"], \"neutral\": [\"neutral\"] } # Key khớp với MnlimProcessor\n    ),\n    \"mnli-mm\": (\n        '{\"placeholder\":\"text_a\"} ? {\"mask\"}, {\"placeholder\":\"text_b\"}',\n        { \"contradiction\": [\"contradiction\"], \"entailment\": [\"entailment\"], \"neutral\": [\"neutral\"] } # Key khớp với MnlimmProcessor\n    ),\n    # ==============================================================================\n\n    # --- Nhận dạng thực thể ---\n    \"fewnerd\": (\n        'Find the entity in the following text. The entity is a {\"mask\"}. Text: {\"placeholder\":\"text_a\"}',\n        {\n            \"art-broadcastprogram\": [\"broadcast program\"], \"art-film\": [\"film\"], \"art-music\": [\"music\"],\n            \"art-other\": [\"art\"], \"art-painting\": [\"painting\"], \"art-writtenart\": [\"written art\"],\n            \"building-airport\": [\"airport\"], \"building-hospital\": [\"hospital\"], \"building-hotel\": [\"hotel\"],\n            \"building-library\": [\"library\"], \"building-other\": [\"building\"], \"building-restaurant\": [\"restaurant\"],\n            \"building-sportsfacility\": [\"sports facility\"], \"building-theater\": [\"theater\"],\n            \"event-attack/battle/war/militaryconflict\": [\"military conflict\"], \"event-disaster\": [\"disaster\"],\n            \"event-election\": [\"election\"], \"event-other\": [\"event\"], \"event-protest\": [\"protest\"],\n            \"event-sportsevent\": [\"sports event\"], \"location-GPE\": [\"geopolitical entity\"],\n            \"location-bodiesofwater\": [\"body of water\"], \"location-island\": [\"island\"], \"location-mountain\": [\"mountain\"],\n            \"location-other\": [\"location\"], \"location-park\": [\"park\"], \"location-road/railway/highway/transit\": [\"road\"],\n            \"organization-company\": [\"company\"], \"organization-education\": [\"education organization\"],\n            \"organization-government/governmentagency\": [\"government agency\"], \"organization-media/newspaper\": [\"media organization\"],\n            \"organization-other\": [\"organization\"], \"organization-politicalparty\": [\"political party\"],\n            \"organization-religion\": [\"religious organization\"], \"organization-showorganization\": [\"show organization\"],\n            \"organization-sportsleague\": [\"sports league\"], \"organization-sportsteam\": [\"sports team\"],\n            \"other-astronomything\": [\"astronomical object\"], \"other-award\": [\"award\"],\n            \"other-biologything\": [\"biological thing\"], \"other-chemicalthing\": [\"chemical thing\"],\n            \"other-currency\": [\"currency\"], \"other-disease\": [\"disease\"], \"other-educationaldegree\": [\"educational degree\"],\n            \"other-god\": [\"god\"], \"other-language\": [\"language\"], \"other-law\": [\"law\"],\n            \"other-livingthing\": [\"living thing\"], \"other-medical\": [\"medical\"],\n            \"person-actor\": [\"actor\"], \"person-artist/author\": [\"artist\"], \"person-athlete\": [\"athlete\"],\n            \"person-director\": [\"director\"], \"person-other\": [\"person\"], \"person-politician\": [\"politician\"],\n            \"person-scholar\": [\"scholar\"], \"person-soldier\": [\"soldier\"],\n            \"product-airplane\": [\"airplane\"], \"product-car\": [\"car\"], \"product-food\": [\"food\"],\n            \"product-game\": [\"game\"], \"product-other\": [\"product\"], \"product-ship\": [\"ship\"],\n            \"product-software\": [\"software\"], \"product-train\": [\"train\"], \"product-weapon\": [\"weapon\"]\n        }\n    )\n}\n\n# --- Vòng lặp để tạo file ---\nbase_path = \"/kaggle/working/DecT/scripts\"\nfor dataset_name, (template_content, verbalizer_dict) in configs.items():\n    dir_name = dataset_name.split('-')[0]\n    dataset_path = os.path.join(base_path, dir_name)\n    os.makedirs(dataset_path, exist_ok=True)\n    \n    template_path = os.path.join(dataset_path, \"manual_template.txt\")\n    with open(template_path, 'w') as f:\n        f.write(template_content)\n    \n    verbalizer_path = os.path.join(dataset_path, \"manual_verbalizer.json\")\n    with open(verbalizer_path, 'w') as f:\n        json.dump(verbalizer_dict, f, indent=4)\n    print(f\"✅ Đã tạo lại cấu hình cho: {dir_name}\")\n\nprint(\"🎉 HOÀN TẤT: Tạo lại tất cả các file template và verbalizer!\")\nprint(\"-\" * 50)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-30T05:21:37.803046Z","iopub.execute_input":"2025-07-30T05:21:37.803287Z","iopub.status.idle":"2025-07-30T05:21:37.827595Z","shell.execute_reply.started":"2025-07-30T05:21:37.803267Z","shell.execute_reply":"2025-07-30T05:21:37.826867Z"}},"outputs":[{"name":"stdout","text":" BẮT ĐẦU: Tạo lại tất cả các file template và verbalizer với các KEY ĐÃ ĐỒNG BỘ...\n✅ Đã tạo lại cấu hình cho: agnews\n✅ Đã tạo lại cấu hình cho: imdb\n✅ Đã tạo lại cấu hình cho: sst2\n✅ Đã tạo lại cấu hình cho: yahoo\n✅ Đã tạo lại cấu hình cho: yelp\n✅ Đã tạo lại cấu hình cho: dbpedia\n✅ Đã tạo lại cấu hình cho: rte\n✅ Đã tạo lại cấu hình cho: snli\n✅ Đã tạo lại cấu hình cho: mnli\n✅ Đã tạo lại cấu hình cho: mnli\n✅ Đã tạo lại cấu hình cho: fewnerd\n🎉 HOÀN TẤT: Tạo lại tất cả các file template và verbalizer!\n--------------------------------------------------\n","output_type":"stream"}],"execution_count":44},{"cell_type":"code","source":"%%writefile /kaggle/working/DecT/src/run_dect.py\nimport os\nimport sys\nsys.path.append('.')\n\nimport argparse\nfrom process_data import load_dataset\nfrom dect_verbalizer import DecTVerbalizer\nfrom dect_trainer import DecTRunner\nfrom openprompt.prompts import ManualTemplate\nfrom openprompt.pipeline_base import PromptForClassification\nfrom openprompt.utils.reproduciblity import set_seed\nfrom openprompt import PromptDataLoader\nfrom openprompt.data_utils import FewShotSampler\nfrom openprompt.data_utils.utils import InputExample\n\nimport torch\nfrom typing import Dict, Optional\nfrom transformers import (\n    AutoConfig, AutoTokenizer, AutoModelForCausalLM,\n    BitsAndBytesConfig\n)\nfrom openprompt.plms.lm import LMTokenizerWrapper \nfrom openprompt.plms import _MODEL_CLASSES, load_plm\n\ndef get_last_hidden_state(hidden_states: torch.Tensor, attention_mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n    if attention_mask is None:\n        return hidden_states[:, -1]\n    \n    sequence_lengths = attention_mask.sum(dim=1) - 1\n    batch_size = hidden_states.shape[0]\n    batch_indices = torch.arange(batch_size, device=hidden_states.device)\n    last_hidden_state = hidden_states[batch_indices, sequence_lengths]\n    return last_hidden_state\n\nclass PromptForCausalLMClassification(PromptForClassification):\n    def forward(self, batch: Dict) -> torch.Tensor:\n        outputs = self.plm(**batch)\n        hidden_states = outputs.hidden_states[-1]\n        representation = get_last_hidden_state(hidden_states, batch['attention_mask'])\n        logits = self.verbalizer(representation)\n        return logits\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.float16,\n    bnb_4bit_use_double_quant=False,\n)\n\n_MODEL_CLASSES['auto'] = (AutoModelForCausalLM, AutoTokenizer, AutoConfig, LMTokenizerWrapper)\n\nparser = argparse.ArgumentParser(description=\"DecT runner with extended LLM support\")\nparser.add_argument(\"--model\", type=str, default='roberta', help=\"PLM key: roberta or auto for LLMs\")\nparser.add_argument(\"--model_name_or_path\", type=str, default='roberta-large', help=\"HuggingFace repo or local path\")\nparser.add_argument(\"--shot\", type=int, default=1, help=\"shots per class\")\nparser.add_argument(\"--seed\", type=int, default=0, help=\"sampling seed\")\nparser.add_argument(\"--template_id\", type=int, default=0, help=\"template choice index\")\nparser.add_argument(\"--dataset\", type=str, default='sst2', help=\"dataset name\")\nparser.add_argument(\"--max_epochs\", type=int, default=30, help=\"training epochs\")\nparser.add_argument(\"--batch_size\", type=int, default=32, help=\"batch size\")\nparser.add_argument(\"--proto_dim\", type=int, default=128, help=\"prototype dimension\")\nparser.add_argument(\"--model_logits_weight\", type=float, default=None, help=\"override logits weight\")\nparser.add_argument(\"--lr\", type=float, default=0.01, help=\"learning rate for DecT\")\nargs = parser.parse_args()\n\ndef build_dataloader(dataset, template, verbalizer, tokenizer, tokenizer_wrapper_class, batch_size):\n    return PromptDataLoader(\n        dataset=dataset, template=template, verbalizer=verbalizer, tokenizer=tokenizer,\n        tokenizer_wrapper_class=tokenizer_wrapper_class, batch_size=batch_size,\n        max_seq_length=256, decoder_max_length=3, truncate_method=\"head\"\n    )\n\ndef main():\n    data_path = args.dataset.split('-')[0]\n    if args.model_logits_weight is None:\n        args.model_logits_weight = 1.0 / args.shot\n\n    train_ds, valid_ds, test_ds, Processor = load_dataset(args.dataset)\n    print(f\"DEBUG: Processor.labels for {args.dataset}: {Processor.labels}\")\n    \n    print(f\"Using full test set with size: {len(test_ds)}\")\n    \n    sampler = FewShotSampler(num_examples_per_label=args.shot, also_sample_dev=True, num_examples_per_label_dev=args.shot)\n    train_sampled, valid_sampled = sampler(train_dataset=train_ds, valid_dataset=valid_ds, seed=args.seed)\n\n    set_seed(args.seed)\n\n    token = os.getenv('HF_TOKEN', None)\n    if args.model == 'auto':\n        tokenizer = AutoTokenizer.from_pretrained(args.model_name_or_path, use_fast=False, token=token)\n        if tokenizer.pad_token is None:\n            tokenizer.pad_token = tokenizer.eos_token\n        config = AutoConfig.from_pretrained(args.model_name_or_path, output_hidden_states=True, token=token)\n        model_for_prompt_model = AutoModelForCausalLM.from_pretrained(\n            args.model_name_or_path, config=config, quantization_config=bnb_config, device_map=\"auto\", token=token\n        )\n        actual_plm_wrapper_class = LMTokenizerWrapper\n        PromptPipeline = PromptForCausalLMClassification\n    else:\n        model_for_prompt_model, tokenizer, config, actual_plm_wrapper_class = load_plm(args.model, args.model_name_or_path)\n        PromptPipeline = PromptForClassification\n\n    set_seed(args.seed)\n\n    template = ManualTemplate(tokenizer=tokenizer).from_file(\n        f\"scripts/{data_path}/manual_template.txt\", choice=args.template_id\n    )\n    verbalizer = DecTVerbalizer(\n        tokenizer=tokenizer, classes=Processor.labels,\n        hidden_size=config.hidden_size, lr=args.lr, mid_dim=args.proto_dim,\n        epochs=args.max_epochs, model_logits_weight=args.model_logits_weight\n    ).from_file(f\"scripts/{data_path}/manual_verbalizer.json\")\n    \n    prompt_model = PromptPipeline(\n        plm=model_for_prompt_model, template=template, verbalizer=verbalizer, freeze_plm=True\n    )\n\n    train_loader = build_dataloader(train_sampled, template, verbalizer, tokenizer, actual_plm_wrapper_class, args.batch_size)\n    test_loader = build_dataloader(test_ds, template, verbalizer, tokenizer, actual_plm_wrapper_class, args.batch_size)\n\n    calib_input_examples = [InputExample(guid='0', text_a='A sample sentence.', label=0)]\n    calib_loader = PromptDataLoader(\n        dataset=calib_input_examples, template=template, tokenizer=tokenizer,\n        tokenizer_wrapper_class=actual_plm_wrapper_class, max_seq_length=256, batch_size=1\n    )\n    \n    runner = DecTRunner(\n        model=prompt_model,\n        train_dataloader=train_loader,\n        valid_dataloader=None,\n        test_dataloader=test_loader,\n        calibrate_dataloader=calib_loader\n    )\n    acc = runner.run()\n    print(f\"Dataset: {args.dataset} | Shot: {args.shot} | Acc: {acc['accuracy']*100:.2f}\")\n\nif __name__ == '__main__':\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-30T05:21:37.828489Z","iopub.execute_input":"2025-07-30T05:21:37.828754Z","iopub.status.idle":"2025-07-30T05:21:37.845114Z","shell.execute_reply.started":"2025-07-30T05:21:37.828732Z","shell.execute_reply":"2025-07-30T05:21:37.844417Z"}},"outputs":[{"name":"stdout","text":"Overwriting /kaggle/working/DecT/src/run_dect.py\n","output_type":"stream"}],"execution_count":45},{"cell_type":"code","source":"%%writefile /kaggle/working/DecT/src/dect_trainer.py\nimport os\nimport torch\nfrom tqdm import tqdm\nfrom typing import Optional, Dict\nfrom sklearn.metrics import accuracy_score\nfrom openprompt.pipeline_base import PromptForClassification\nfrom openprompt import PromptDataLoader\nfrom openprompt.utils.logging import logger\n\nclass DecTRunner:\n    \"\"\"Runner for DecT with Mistral 7B support\"\"\"\n    \n    def __init__(self,\n                 model: PromptForClassification,\n                 train_dataloader: Optional[PromptDataLoader] = None,\n                 valid_dataloader: Optional[PromptDataLoader] = None,\n                 test_dataloader: Optional[PromptDataLoader] = None,\n                 calibrate_dataloader: Optional[PromptDataLoader] = None):\n        \n        self.model = model.cuda()\n        self.train_dataloader = train_dataloader\n        self.valid_dataloader = valid_dataloader \n        self.test_dataloader = test_dataloader\n        self.calibrate_dataloader = calibrate_dataloader\n        self.loss_fn = torch.nn.CrossEntropyLoss()\n\n    def fit(self, ckpt: Optional[str] = None):\n        \"\"\"Train the verbalizer prototypes\"\"\"\n        if ckpt:\n            logger.warning(\"Checkpoint loading not implemented, training from scratch\")\n            \n        # Train prototypes\n        self.model.verbalizer.train_proto(\n            self.model,\n            self.train_dataloader,\n            self.calibrate_dataloader\n        )\n        return self\n\n    def evaluate(self, dataloader: PromptDataLoader) -> Dict:\n        \"\"\"Evaluate on a dataloader\"\"\"\n        self.model.eval()\n        all_preds, all_labels = [], []\n        \n        with torch.no_grad():\n            for batch in tqdm(dataloader, desc=\"Evaluating\"):\n                batch = batch.cuda()\n                \n                # Get hidden states\n                outputs = self.model.prompt_model(batch)\n                hidden_states = outputs.hidden_states[-1]\n                \n                # Get last token hidden states\n                seq_lens = batch.attention_mask.sum(dim=1) - 1\n                h = hidden_states[torch.arange(hidden_states.size(0)), seq_lens]\n                \n                # Get predictions\n                logits = self.model.verbalizer(h)\n                preds = torch.argmax(logits, dim=-1)\n                \n                all_preds.extend(preds.cpu().tolist())\n                all_labels.extend(batch.label.cpu().tolist())\n        \n        acc = accuracy_score(all_labels, all_preds)\n        return {\"accuracy\": acc}\n\n    def test(self) -> Dict:\n        \"\"\"Test on the test set\"\"\"\n        if self.test_dataloader is None:\n            raise ValueError(\"No test dataloader provided\")\n        return self.evaluate(self.test_dataloader)\n\n    def run(self) -> Dict:\n        \"\"\"Full training and evaluation pipeline\"\"\"\n        self.fit()\n        results = self.test()\n        return results","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-30T05:21:37.845878Z","iopub.execute_input":"2025-07-30T05:21:37.846129Z","iopub.status.idle":"2025-07-30T05:21:37.864215Z","shell.execute_reply.started":"2025-07-30T05:21:37.846106Z","shell.execute_reply":"2025-07-30T05:21:37.863708Z"}},"outputs":[{"name":"stdout","text":"Overwriting /kaggle/working/DecT/src/dect_trainer.py\n","output_type":"stream"}],"execution_count":46},{"cell_type":"markdown","source":"## Agnews","metadata":{}},{"cell_type":"code","source":"!python src/run_dect.py \\\n--model auto \\\n--model_name_or_path mistralai/Mistral-7B-Instruct-v0.2 \\\n--dataset agnews \\\n--max_epochs 30 \\\n--batch_size 4 \\\n--proto_dim 256 \\\n--lr 1e-4 \\\n--seed 42\n","metadata":{"execution":{"iopub.status.busy":"2025-07-29T00:01:07.855751Z","iopub.execute_input":"2025-07-29T00:01:07.856461Z","iopub.status.idle":"2025-07-29T01:19:21.212038Z","shell.execute_reply.started":"2025-07-29T00:01:07.856444Z","shell.execute_reply":"2025-07-29T01:19:21.210740Z"},"trusted":true},"outputs":[{"name":"stdout","text":"DEBUG: Processor.labels for agnews: ['World', 'Sports', 'Business', 'Tech']\nUsing full test set with size: 7600\ntokenizer_config.json: 100%|███████████████| 2.10k/2.10k [00:00<00:00, 15.5MB/s]\ntokenizer.model: 100%|████████████████████████| 493k/493k [00:00<00:00, 946kB/s]\nspecial_tokens_map.json: 100%|█████████████████| 414/414 [00:00<00:00, 2.39MB/s]\ntokenizer.json: 100%|██████████████████████| 1.80M/1.80M [00:00<00:00, 5.50MB/s]\nnormalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.\nconfig.json: 100%|█████████████████████████████| 596/596 [00:00<00:00, 4.03MB/s]\nmodel.safetensors.index.json: 100%|█████████| 25.1k/25.1k [00:00<00:00, 104MB/s]\nFetching 3 files:   0%|                                   | 0/3 [00:00<?, ?it/s]\nmodel-00002-of-00003.safetensors:   0%|             | 0.00/5.00G [00:00<?, ?B/s]\u001b[A\n\nmodel-00001-of-00003.safetensors:   0%|             | 0.00/4.94G [00:00<?, ?B/s]\u001b[A\u001b[A\n\n\nmodel-00003-of-00003.safetensors:   0%|             | 0.00/4.54G [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n\n\nmodel-00003-of-00003.safetensors:   0%|    | 816k/4.54G [00:01<1:45:57, 714kB/s]\u001b[A\u001b[A\u001b[A\n\nmodel-00001-of-00003.safetensors:   0%|    | 711k/4.94G [00:01<2:28:16, 556kB/s]\u001b[A\u001b[A\nmodel-00002-of-00003.safetensors:   0%|    | 754k/5.00G [00:01<2:32:19, 547kB/s]\u001b[A\n\n\nmodel-00003-of-00003.safetensors:   0%|    | 2.38M/4.54G [00:01<45:18, 1.67MB/s]\u001b[A\u001b[A\u001b[A\nmodel-00002-of-00003.safetensors:   0%|    | 994k/5.00G [00:01<2:27:14, 566kB/s]\u001b[A\n\nmodel-00001-of-00003.safetensors:   0%|  | 2.14M/4.94G [00:01<1:05:06, 1.26MB/s]\u001b[A\u001b[A\nmodel-00002-of-00003.safetensors:   0%|  | 2.34M/5.00G [00:02<1:08:47, 1.21MB/s]\u001b[A\n\n\nmodel-00003-of-00003.safetensors:   0%|    | 4.59M/4.54G [00:02<35:34, 2.13MB/s]\u001b[A\u001b[A\u001b[A\nmodel-00002-of-00003.safetensors:   0%|    | 11.5M/5.00G [00:03<17:24, 4.78MB/s]\u001b[A\nmodel-00002-of-00003.safetensors:   1%|    | 39.2M/5.00G [00:04<06:01, 13.7MB/s]\u001b[A\nmodel-00002-of-00003.safetensors:   1%|    | 66.8M/5.00G [00:06<05:29, 15.0MB/s]\u001b[A\nmodel-00002-of-00003.safetensors:   2%|     | 117M/5.00G [00:07<03:22, 24.1MB/s]\u001b[A\n\nmodel-00001-of-00003.safetensors:   1%|    | 33.7M/4.94G [00:08<17:56, 4.56MB/s]\u001b[A\u001b[A\nmodel-00002-of-00003.safetensors:   3%|▏    | 173M/5.00G [00:08<02:16, 35.4MB/s]\u001b[A\n\nmodel-00001-of-00003.safetensors:   1%|    | 36.0M/4.94G [00:10<23:47, 3.44MB/s]\u001b[A\u001b[A\nmodel-00002-of-00003.safetensors:   4%|▏    | 191M/5.00G [00:10<03:45, 21.4MB/s]\u001b[A\nmodel-00002-of-00003.safetensors:   6%|▎    | 290M/5.00G [00:11<01:52, 41.9MB/s]\u001b[A\nmodel-00002-of-00003.safetensors:   8%|▍    | 381M/5.00G [00:12<01:28, 52.2MB/s]\u001b[A\n\n\nmodel-00003-of-00003.safetensors:   0%|    | 4.59M/4.54G [00:13<35:34, 2.13MB/s]\u001b[A\u001b[A\u001b[A\nmodel-00002-of-00003.safetensors:  10%|▌    | 516M/5.00G [00:14<01:24, 53.3MB/s]\u001b[A\n\nmodel-00001-of-00003.safetensors:   1%|    | 54.4M/4.94G [00:15<22:46, 3.58MB/s]\u001b[A\u001b[A\nmodel-00002-of-00003.safetensors:  11%|▌    | 568M/5.00G [00:15<01:19, 56.1MB/s]\u001b[A\n\n\nmodel-00003-of-00003.safetensors:   2%|    | 71.7M/4.54G [00:16<16:02, 4.64MB/s]\u001b[A\u001b[A\u001b[A\n\nmodel-00001-of-00003.safetensors:   2%|    | 96.1M/4.94G [00:16<09:55, 8.14MB/s]\u001b[A\u001b[A\nmodel-00002-of-00003.safetensors:  12%|▌    | 616M/5.00G [00:16<01:20, 54.4MB/s]\u001b[A\nmodel-00002-of-00003.safetensors:  15%|▋    | 735M/5.00G [00:17<00:50, 84.2MB/s]\u001b[A\n\n\nmodel-00003-of-00003.safetensors:   3%|▏    | 139M/4.54G [00:18<07:52, 9.32MB/s]\u001b[A\u001b[A\u001b[A\nmodel-00002-of-00003.safetensors:  17%|▊    | 869M/5.00G [00:18<00:47, 86.4MB/s]\u001b[A\n\n\nmodel-00003-of-00003.safetensors:   5%|▏    | 206M/4.54G [00:18<04:27, 16.2MB/s]\u001b[A\u001b[A\u001b[A\nmodel-00002-of-00003.safetensors:  20%|█▏    | 978M/5.00G [00:18<00:36, 109MB/s]\u001b[A\n\n\nmodel-00003-of-00003.safetensors:   6%|▎    | 273M/4.54G [00:19<03:00, 23.6MB/s]\u001b[A\u001b[A\u001b[A\n\nmodel-00001-of-00003.safetensors:   3%|▏    | 163M/4.94G [00:19<06:11, 12.9MB/s]\u001b[A\u001b[A\nmodel-00002-of-00003.safetensors:  22%|█    | 1.09G/5.00G [00:19<00:32, 121MB/s]\u001b[A\nmodel-00002-of-00003.safetensors:  24%|█▏   | 1.22G/5.00G [00:20<00:26, 145MB/s]\u001b[A\n\n\nmodel-00003-of-00003.safetensors:   7%|▎    | 340M/4.54G [00:20<02:18, 30.4MB/s]\u001b[A\u001b[A\u001b[A\n\nmodel-00001-of-00003.safetensors:   5%|▏    | 230M/4.94G [00:20<03:53, 20.2MB/s]\u001b[A\u001b[A\n\n\nmodel-00003-of-00003.safetensors:   9%|▍    | 406M/4.54G [00:22<02:11, 31.4MB/s]\u001b[A\u001b[A\u001b[A\nmodel-00002-of-00003.safetensors:  27%|█   | 1.34G/5.00G [00:23<00:44, 82.0MB/s]\u001b[A\n\n\nmodel-00003-of-00003.safetensors:  10%|▌    | 473M/4.54G [00:23<01:46, 38.3MB/s]\u001b[A\u001b[A\u001b[A\n\nmodel-00001-of-00003.safetensors:   7%|▎    | 346M/4.94G [00:23<02:52, 26.6MB/s]\u001b[A\u001b[A\n\n\nmodel-00003-of-00003.safetensors:  15%|▋    | 674M/4.54G [00:24<00:48, 79.3MB/s]\u001b[A\u001b[A\u001b[A\n\n\nmodel-00003-of-00003.safetensors:  18%|▉    | 809M/4.54G [00:25<00:46, 80.9MB/s]\u001b[A\u001b[A\u001b[A\nmodel-00002-of-00003.safetensors:  28%|█   | 1.41G/5.00G [00:27<01:19, 45.0MB/s]\u001b[A\n\n\nmodel-00003-of-00003.safetensors:  21%|█    | 943M/4.54G [00:28<00:52, 68.6MB/s]\u001b[A\u001b[A\u001b[A\nmodel-00002-of-00003.safetensors:  29%|█▏  | 1.47G/5.00G [00:28<01:12, 48.6MB/s]\u001b[A\n\nmodel-00001-of-00003.safetensors:   8%|▍    | 413M/4.94G [00:28<03:39, 20.6MB/s]\u001b[A\u001b[A\nmodel-00002-of-00003.safetensors:  31%|█▏  | 1.54G/5.00G [00:29<01:13, 47.2MB/s]\u001b[A\n\n\nmodel-00003-of-00003.safetensors:  24%|▉   | 1.08G/4.54G [00:29<00:48, 71.1MB/s]\u001b[A\u001b[A\u001b[A\nmodel-00002-of-00003.safetensors:  34%|█▎  | 1.68G/5.00G [00:31<00:54, 61.3MB/s]\u001b[A\n\nmodel-00001-of-00003.safetensors:  10%|▍    | 480M/4.94G [00:31<03:26, 21.6MB/s]\u001b[A\u001b[A\n\n\nmodel-00003-of-00003.safetensors:  25%|█   | 1.14G/4.54G [00:31<00:58, 58.3MB/s]\u001b[A\u001b[A\u001b[A\n\n\nmodel-00003-of-00003.safetensors:  27%|█   | 1.21G/4.54G [00:32<00:54, 60.9MB/s]\u001b[A\u001b[A\u001b[A\n\n\nmodel-00003-of-00003.safetensors:  28%|█   | 1.28G/4.54G [00:33<00:44, 73.4MB/s]\u001b[A\u001b[A\u001b[A\n\nmodel-00001-of-00003.safetensors:  11%|▌    | 547M/4.94G [00:33<02:56, 24.9MB/s]\u001b[A\u001b[A\n\n\nmodel-00003-of-00003.safetensors:  30%|█▏  | 1.34G/4.54G [00:33<00:35, 89.8MB/s]\u001b[A\u001b[A\u001b[A\n\nmodel-00001-of-00003.safetensors:  12%|▌    | 614M/4.94G [00:33<02:15, 31.9MB/s]\u001b[A\u001b[A\nmodel-00002-of-00003.safetensors:  35%|█▍  | 1.75G/5.00G [00:34<01:14, 43.8MB/s]\u001b[A\n\n\nmodel-00003-of-00003.safetensors:  31%|█▏  | 1.41G/4.54G [00:34<00:35, 87.6MB/s]\u001b[A\u001b[A\u001b[A\nmodel-00002-of-00003.safetensors:  36%|█▍  | 1.81G/5.00G [00:34<01:00, 53.0MB/s]\u001b[A\n\n\nmodel-00003-of-00003.safetensors:  33%|█▎  | 1.48G/4.54G [00:35<00:37, 81.2MB/s]\u001b[A\u001b[A\u001b[A\nmodel-00002-of-00003.safetensors:  37%|█▍  | 1.85G/5.00G [00:35<01:04, 48.5MB/s]\u001b[A\n\nmodel-00001-of-00003.safetensors:  14%|▋    | 681M/4.94G [00:35<02:11, 32.4MB/s]\u001b[A\u001b[A\n\n\nmodel-00003-of-00003.safetensors:  34%|█▎  | 1.54G/4.54G [00:38<01:08, 43.4MB/s]\u001b[A\u001b[A\u001b[A\n\nmodel-00001-of-00003.safetensors:  16%|▊    | 816M/4.94G [00:38<01:46, 38.6MB/s]\u001b[A\u001b[A\n\n\nmodel-00003-of-00003.safetensors:  35%|█▍  | 1.61G/4.54G [00:38<00:50, 58.4MB/s]\u001b[A\u001b[A\u001b[A\n\n\nmodel-00003-of-00003.safetensors:  37%|█▍  | 1.68G/4.54G [00:39<00:46, 61.4MB/s]\u001b[A\u001b[A\u001b[A\n\nmodel-00001-of-00003.safetensors:  18%|▉    | 883M/4.94G [00:40<01:42, 39.7MB/s]\u001b[A\u001b[A\n\n\nmodel-00003-of-00003.safetensors:  38%|█▌  | 1.75G/4.54G [00:40<00:38, 72.8MB/s]\u001b[A\u001b[A\u001b[A\nmodel-00002-of-00003.safetensors:  38%|█▌  | 1.92G/5.00G [00:40<01:45, 29.3MB/s]\u001b[A\n\n\nmodel-00003-of-00003.safetensors:  40%|█▌  | 1.81G/4.54G [00:42<00:51, 53.4MB/s]\u001b[A\u001b[A\u001b[A\n\n\nmodel-00003-of-00003.safetensors:  41%|█▋  | 1.88G/4.54G [00:42<00:37, 70.8MB/s]\u001b[A\u001b[A\u001b[A\n\nmodel-00001-of-00003.safetensors:  19%|▉    | 950M/4.94G [00:43<02:08, 31.1MB/s]\u001b[A\u001b[A\n\n\nmodel-00003-of-00003.safetensors:  44%|█▊  | 2.01G/4.54G [00:43<00:29, 86.0MB/s]\u001b[A\u001b[A\u001b[A\nmodel-00002-of-00003.safetensors:  41%|█▋  | 2.05G/5.00G [00:43<01:29, 33.0MB/s]\u001b[A\nmodel-00002-of-00003.safetensors:  41%|█▋  | 2.06G/5.00G [00:44<01:29, 32.9MB/s]\u001b[A\n\nmodel-00001-of-00003.safetensors:  21%|▊   | 1.02G/4.94G [00:44<01:38, 39.7MB/s]\u001b[A\u001b[A\nmodel-00002-of-00003.safetensors:  42%|█▋  | 2.10G/5.00G [00:44<01:24, 34.2MB/s]\u001b[A\n\n\nmodel-00003-of-00003.safetensors:  47%|█▉  | 2.15G/4.54G [00:45<00:27, 87.6MB/s]\u001b[A\u001b[A\u001b[A\n\nmodel-00001-of-00003.safetensors:  22%|▉   | 1.08G/4.94G [00:45<01:27, 44.0MB/s]\u001b[A\u001b[A\n\n\nmodel-00003-of-00003.safetensors:  49%|█▉  | 2.22G/4.54G [00:45<00:23, 97.3MB/s]\u001b[A\u001b[A\u001b[A\n\nmodel-00001-of-00003.safetensors:  23%|▉   | 1.15G/4.94G [00:45<01:12, 52.1MB/s]\u001b[A\u001b[A\n\nmodel-00001-of-00003.safetensors:  24%|▉   | 1.21G/4.94G [00:47<01:13, 51.0MB/s]\u001b[A\u001b[A\n\n\nmodel-00003-of-00003.safetensors:  50%|██  | 2.28G/4.54G [00:48<00:38, 58.6MB/s]\u001b[A\u001b[A\u001b[A\n\n\nmodel-00003-of-00003.safetensors:  52%|██  | 2.35G/4.54G [00:48<00:33, 65.9MB/s]\u001b[A\u001b[A\u001b[A\n\n\nmodel-00003-of-00003.safetensors:  53%|██▏ | 2.42G/4.54G [00:49<00:30, 70.8MB/s]\u001b[A\u001b[A\u001b[A\n\nmodel-00001-of-00003.safetensors:  26%|█   | 1.28G/4.94G [00:50<01:38, 37.1MB/s]\u001b[A\u001b[A\n\n\nmodel-00003-of-00003.safetensors:  55%|██▏ | 2.48G/4.54G [00:50<00:28, 71.0MB/s]\u001b[A\u001b[A\u001b[A\nmodel-00002-of-00003.safetensors:  43%|█▋  | 2.17G/5.00G [00:50<02:16, 20.8MB/s]\u001b[A\n\n\nmodel-00003-of-00003.safetensors:  56%|██▏ | 2.53G/4.54G [00:51<00:29, 68.2MB/s]\u001b[A\u001b[A\u001b[A\n\n\nmodel-00003-of-00003.safetensors:  57%|██▎ | 2.60G/4.54G [00:52<00:27, 71.3MB/s]\u001b[A\u001b[A\u001b[A\n\nmodel-00001-of-00003.safetensors:  27%|█   | 1.35G/4.94G [00:53<01:56, 30.9MB/s]\u001b[A\u001b[A\nmodel-00002-of-00003.safetensors:  45%|█▊  | 2.26G/5.00G [00:53<01:51, 24.6MB/s]\u001b[A\n\n\nmodel-00003-of-00003.safetensors:  59%|██▎ | 2.66G/4.54G [00:53<00:31, 59.6MB/s]\u001b[A\u001b[A\u001b[A\n\n\nmodel-00003-of-00003.safetensors:  60%|██▍ | 2.73G/4.54G [00:54<00:24, 73.0MB/s]\u001b[A\u001b[A\u001b[A\n\nmodel-00001-of-00003.safetensors:  29%|█▏  | 1.41G/4.94G [00:54<01:41, 34.8MB/s]\u001b[A\u001b[A\n\n\nmodel-00003-of-00003.safetensors:  62%|██▍ | 2.80G/4.54G [00:54<00:20, 83.4MB/s]\u001b[A\u001b[A\u001b[A\nmodel-00002-of-00003.safetensors:  46%|█▊  | 2.32G/5.00G [00:55<01:44, 25.6MB/s]\u001b[A\nmodel-00002-of-00003.safetensors:  47%|█▊  | 2.33G/5.00G [00:55<01:44, 25.7MB/s]\u001b[A\n\n\nmodel-00003-of-00003.safetensors:  63%|██▌ | 2.87G/4.54G [00:56<00:27, 62.0MB/s]\u001b[A\u001b[A\u001b[A\n\nmodel-00001-of-00003.safetensors:  30%|█▏  | 1.48G/4.94G [00:57<01:51, 31.1MB/s]\u001b[A\u001b[A\nmodel-00002-of-00003.safetensors:  47%|█▉  | 2.36G/5.00G [00:57<01:45, 25.0MB/s]\u001b[A\n\n\nmodel-00003-of-00003.safetensors:  65%|██▌ | 2.93G/4.54G [00:57<00:28, 57.0MB/s]\u001b[A\u001b[A\u001b[A\n\n\nmodel-00003-of-00003.safetensors:  66%|██▋ | 3.00G/4.54G [00:58<00:24, 63.4MB/s]\u001b[A\u001b[A\u001b[A\nmodel-00002-of-00003.safetensors:  47%|█▉  | 2.37G/5.00G [00:58<02:08, 20.4MB/s]\u001b[A\n\n\nmodel-00003-of-00003.safetensors:  68%|██▋ | 3.06G/4.54G [00:58<00:18, 81.1MB/s]\u001b[A\u001b[A\u001b[A\n\nmodel-00001-of-00003.safetensors:  31%|█▎  | 1.54G/4.94G [00:58<01:43, 33.0MB/s]\u001b[A\u001b[A\n\nmodel-00001-of-00003.safetensors:  33%|█▎  | 1.61G/4.94G [00:59<01:12, 45.8MB/s]\u001b[A\u001b[A\nmodel-00002-of-00003.safetensors:  48%|█▉  | 2.40G/5.00G [00:59<01:53, 22.9MB/s]\u001b[A\n\nmodel-00001-of-00003.safetensors:  34%|█▎  | 1.68G/4.94G [00:59<00:58, 55.4MB/s]\u001b[A\u001b[A\n\nmodel-00001-of-00003.safetensors:  35%|█▍  | 1.75G/4.94G [00:59<00:43, 73.1MB/s]\u001b[A\u001b[A\nmodel-00002-of-00003.safetensors:  49%|█▉  | 2.47G/5.00G [00:59<01:08, 37.0MB/s]\u001b[A\nmodel-00002-of-00003.safetensors:  50%|█▉  | 2.48G/5.00G [01:00<01:05, 38.6MB/s]\u001b[A\n\nmodel-00001-of-00003.safetensors:  37%|█▍  | 1.81G/4.94G [01:00<00:42, 74.1MB/s]\u001b[A\u001b[A\n\n\nmodel-00003-of-00003.safetensors:  69%|██▊ | 3.13G/4.54G [01:01<00:30, 46.3MB/s]\u001b[A\u001b[A\u001b[A\nmodel-00002-of-00003.safetensors:  50%|██  | 2.52G/5.00G [01:01<01:17, 32.1MB/s]\u001b[A\n\nmodel-00001-of-00003.safetensors:  38%|█▌  | 1.88G/4.94G [01:02<00:47, 65.1MB/s]\u001b[A\u001b[A\nmodel-00002-of-00003.safetensors:  51%|██  | 2.53G/5.00G [01:02<01:22, 29.9MB/s]\u001b[A\nmodel-00002-of-00003.safetensors:  51%|██  | 2.55G/5.00G [01:02<01:08, 35.6MB/s]\u001b[A\n\nmodel-00001-of-00003.safetensors:  39%|█▌  | 1.95G/4.94G [01:03<00:53, 56.0MB/s]\u001b[A\u001b[A\nmodel-00002-of-00003.safetensors:  52%|██  | 2.58G/5.00G [01:03<01:19, 30.6MB/s]\u001b[A\n\nmodel-00001-of-00003.safetensors:  41%|█▋  | 2.01G/4.94G [01:04<00:49, 59.4MB/s]\u001b[A\u001b[A\nmodel-00002-of-00003.safetensors:  53%|██  | 2.64G/5.00G [01:04<00:56, 41.5MB/s]\u001b[A\nmodel-00002-of-00003.safetensors:  53%|██▏ | 2.66G/5.00G [01:05<00:53, 43.8MB/s]\u001b[A\n\nmodel-00001-of-00003.safetensors:  42%|█▋  | 2.08G/4.94G [01:05<00:48, 58.9MB/s]\u001b[A\u001b[A\n\n\nmodel-00003-of-00003.safetensors:  70%|██▊ | 3.20G/4.54G [01:05<00:45, 29.8MB/s]\u001b[A\u001b[A\u001b[A\nmodel-00002-of-00003.safetensors:  53%|██▏ | 2.67G/5.00G [01:06<01:17, 30.1MB/s]\u001b[A\nmodel-00002-of-00003.safetensors:  54%|██▏ | 2.70G/5.00G [01:06<01:07, 34.2MB/s]\u001b[A\n\nmodel-00001-of-00003.safetensors:  43%|█▋  | 2.15G/4.94G [01:07<00:55, 50.2MB/s]\u001b[A\u001b[A\nmodel-00002-of-00003.safetensors:  55%|██▏ | 2.76G/5.00G [01:07<00:51, 43.2MB/s]\u001b[A\nmodel-00002-of-00003.safetensors:  57%|██▎ | 2.85G/5.00G [01:08<00:31, 68.7MB/s]\u001b[A\n\nmodel-00001-of-00003.safetensors:  45%|█▊  | 2.22G/4.94G [01:08<00:54, 49.8MB/s]\u001b[A\u001b[A\nmodel-00002-of-00003.safetensors:  59%|██▎ | 2.94G/5.00G [01:09<00:24, 82.9MB/s]\u001b[A\n\nmodel-00001-of-00003.safetensors:  46%|█▊  | 2.28G/4.94G [01:09<00:46, 56.8MB/s]\u001b[A\u001b[A\nmodel-00002-of-00003.safetensors:  59%|██▍ | 2.97G/5.00G [01:09<00:26, 75.8MB/s]\u001b[A\n\nmodel-00001-of-00003.safetensors:  48%|█▉  | 2.35G/4.94G [01:10<00:35, 73.9MB/s]\u001b[A\u001b[A\nmodel-00002-of-00003.safetensors:  60%|██▍ | 3.01G/5.00G [01:10<00:24, 82.7MB/s]\u001b[A\n\n\nmodel-00003-of-00003.safetensors:  72%|██▉ | 3.27G/4.54G [01:10<00:58, 21.6MB/s]\u001b[A\u001b[A\u001b[A\n\n\nmodel-00003-of-00003.safetensors:  73%|██▉ | 3.33G/4.54G [01:11<00:40, 29.6MB/s]\u001b[A\u001b[A\u001b[A\nmodel-00002-of-00003.safetensors:  62%|██▍ | 3.08G/5.00G [01:11<00:28, 67.4MB/s]\u001b[A\n\nmodel-00001-of-00003.safetensors:  49%|█▉  | 2.42G/4.94G [01:13<01:00, 41.9MB/s]\u001b[A\u001b[A\n\nmodel-00001-of-00003.safetensors:  50%|██  | 2.48G/4.94G [01:13<00:43, 56.1MB/s]\u001b[A\u001b[A\n\n\nmodel-00003-of-00003.safetensors:  75%|██▉ | 3.40G/4.54G [01:13<00:40, 28.3MB/s]\u001b[A\u001b[A\u001b[A\n\nmodel-00001-of-00003.safetensors:  52%|██  | 2.55G/4.94G [01:14<00:36, 66.4MB/s]\u001b[A\u001b[A\n\nmodel-00001-of-00003.safetensors:  53%|██  | 2.62G/4.94G [01:15<00:43, 53.6MB/s]\u001b[A\u001b[A\n\nmodel-00001-of-00003.safetensors:  54%|██▏ | 2.68G/4.94G [01:16<00:39, 57.9MB/s]\u001b[A\u001b[A\n\nmodel-00001-of-00003.safetensors:  56%|██▏ | 2.75G/4.94G [01:17<00:30, 71.3MB/s]\u001b[A\u001b[A\n\n\nmodel-00003-of-00003.safetensors:  76%|███ | 3.47G/4.54G [01:17<00:44, 24.1MB/s]\u001b[A\u001b[A\u001b[A\n\nmodel-00001-of-00003.safetensors:  57%|██▎ | 2.82G/4.94G [01:17<00:24, 85.2MB/s]\u001b[A\u001b[A\n\nmodel-00001-of-00003.safetensors:  58%|██▉  | 2.89G/4.94G [01:17<00:18, 114MB/s]\u001b[A\u001b[A\nmodel-00002-of-00003.safetensors:  62%|██▍ | 3.10G/5.00G [01:17<01:42, 18.5MB/s]\u001b[A\n\n\nmodel-00003-of-00003.safetensors:  78%|███ | 3.53G/4.54G [01:18<00:31, 32.1MB/s]\u001b[A\u001b[A\u001b[A\nmodel-00002-of-00003.safetensors:  63%|██▌ | 3.13G/5.00G [01:18<01:29, 21.0MB/s]\u001b[A\n\nmodel-00001-of-00003.safetensors:  60%|██▉  | 2.95G/4.94G [01:18<00:18, 108MB/s]\u001b[A\u001b[A\n\nmodel-00001-of-00003.safetensors:  61%|███  | 3.02G/4.94G [01:18<00:14, 128MB/s]\u001b[A\u001b[A\nmodel-00002-of-00003.safetensors:  63%|██▌ | 3.14G/5.00G [01:19<01:30, 20.4MB/s]\u001b[A\n\nmodel-00001-of-00003.safetensors:  62%|███  | 3.09G/4.94G [01:19<00:13, 143MB/s]\u001b[A\u001b[A\nmodel-00002-of-00003.safetensors:  64%|██▌ | 3.21G/5.00G [01:19<00:51, 34.7MB/s]\u001b[A\n\nmodel-00001-of-00003.safetensors:  64%|███▏ | 3.15G/4.94G [01:19<00:14, 124MB/s]\u001b[A\u001b[A\nmodel-00002-of-00003.safetensors:  66%|██▌ | 3.28G/5.00G [01:20<00:34, 49.3MB/s]\u001b[A\nmodel-00002-of-00003.safetensors:  67%|██▋ | 3.34G/5.00G [01:20<00:24, 67.3MB/s]\u001b[A\nmodel-00002-of-00003.safetensors:  67%|██▋ | 3.37G/5.00G [01:21<00:25, 62.8MB/s]\u001b[A\n\nmodel-00001-of-00003.safetensors:  65%|██▌ | 3.22G/4.94G [01:21<00:19, 90.1MB/s]\u001b[A\u001b[A\n\n\nmodel-00003-of-00003.safetensors:  79%|███▏| 3.60G/4.54G [01:22<00:38, 24.6MB/s]\u001b[A\u001b[A\u001b[A\nmodel-00002-of-00003.safetensors:  69%|██▋ | 3.44G/5.00G [01:23<00:36, 43.4MB/s]\u001b[A\nmodel-00002-of-00003.safetensors:  70%|██▊ | 3.48G/5.00G [01:24<00:33, 45.6MB/s]\u001b[A\nmodel-00002-of-00003.safetensors:  71%|██▊ | 3.54G/5.00G [01:25<00:30, 47.8MB/s]\u001b[A\n\n\nmodel-00003-of-00003.safetensors:  81%|███▏| 3.67G/4.54G [01:25<00:39, 22.2MB/s]\u001b[A\u001b[A\u001b[A\nmodel-00002-of-00003.safetensors:  72%|██▉ | 3.61G/5.00G [01:26<00:27, 51.0MB/s]\u001b[A\nmodel-00002-of-00003.safetensors:  73%|██▉ | 3.65G/5.00G [01:26<00:23, 57.8MB/s]\u001b[A\nmodel-00002-of-00003.safetensors:  74%|██▉ | 3.69G/5.00G [01:26<00:17, 73.0MB/s]\u001b[A\n\nmodel-00001-of-00003.safetensors:  67%|██▋ | 3.29G/4.94G [01:27<00:57, 28.7MB/s]\u001b[A\u001b[A\n\n\nmodel-00003-of-00003.safetensors:  82%|███▎| 3.74G/4.54G [01:27<00:32, 25.0MB/s]\u001b[A\u001b[A\u001b[A\nmodel-00002-of-00003.safetensors:  75%|██▉ | 3.73G/5.00G [01:27<00:20, 62.8MB/s]\u001b[A\nmodel-00002-of-00003.safetensors:  76%|███ | 3.78G/5.00G [01:28<00:15, 77.8MB/s]\u001b[A\n\n\nmodel-00003-of-00003.safetensors:  84%|███▎| 3.80G/4.54G [01:28<00:23, 31.6MB/s]\u001b[A\u001b[A\u001b[A\nmodel-00002-of-00003.safetensors:  76%|███ | 3.82G/5.00G [01:28<00:16, 72.7MB/s]\u001b[A\n\n\nmodel-00003-of-00003.safetensors:  85%|███▍| 3.87G/4.54G [01:29<00:17, 38.0MB/s]\u001b[A\u001b[A\u001b[A\nmodel-00002-of-00003.safetensors:  77%|███ | 3.86G/5.00G [01:29<00:19, 58.9MB/s]\u001b[A\n\n\nmodel-00003-of-00003.safetensors:  87%|███▍| 3.94G/4.54G [01:30<00:12, 46.7MB/s]\u001b[A\u001b[A\u001b[A\n\nmodel-00001-of-00003.safetensors:  68%|██▋ | 3.36G/4.94G [01:30<01:02, 25.3MB/s]\u001b[A\u001b[A\n\n\nmodel-00003-of-00003.safetensors:  88%|███▌| 4.00G/4.54G [01:30<00:09, 59.4MB/s]\u001b[A\u001b[A\u001b[A\n\n\nmodel-00003-of-00003.safetensors:  90%|███▌| 4.07G/4.54G [01:30<00:05, 81.6MB/s]\u001b[A\u001b[A\u001b[A\n\nmodel-00001-of-00003.safetensors:  69%|██▊ | 3.42G/4.94G [01:31<00:47, 32.1MB/s]\u001b[A\u001b[A\n\nmodel-00001-of-00003.safetensors:  71%|██▊ | 3.49G/4.94G [01:31<00:35, 41.4MB/s]\u001b[A\u001b[A\nmodel-00002-of-00003.safetensors:  78%|███▏| 3.91G/5.00G [01:31<00:26, 40.8MB/s]\u001b[A\n\n\nmodel-00003-of-00003.safetensors:  91%|███▋| 4.14G/4.54G [01:32<00:06, 65.7MB/s]\u001b[A\u001b[A\u001b[A\n\nmodel-00001-of-00003.safetensors:  72%|██▉ | 3.55G/4.94G [01:33<00:34, 40.1MB/s]\u001b[A\u001b[A\nmodel-00002-of-00003.safetensors:  80%|███▏| 3.98G/5.00G [01:34<00:30, 33.9MB/s]\u001b[A\n\n\nmodel-00003-of-00003.safetensors:  93%|███▋| 4.21G/4.54G [01:34<00:07, 44.6MB/s]\u001b[A\u001b[A\u001b[A\nmodel-00002-of-00003.safetensors:  81%|███▏| 4.05G/5.00G [01:35<00:24, 39.6MB/s]\u001b[A\nmodel-00002-of-00003.safetensors:  82%|███▎| 4.11G/5.00G [01:36<00:16, 52.9MB/s]\u001b[A\nmodel-00002-of-00003.safetensors:  84%|███▎| 4.18G/5.00G [01:36<00:13, 59.1MB/s]\u001b[A\n\nmodel-00001-of-00003.safetensors:  73%|██▉ | 3.60G/4.94G [01:37<00:49, 26.9MB/s]\u001b[A\u001b[A\n\nmodel-00001-of-00003.safetensors:  74%|██▉ | 3.67G/4.94G [01:37<00:35, 35.9MB/s]\u001b[A\u001b[A\nmodel-00002-of-00003.safetensors:  86%|███▍| 4.28G/5.00G [01:37<00:10, 71.3MB/s]\u001b[A\n\nmodel-00001-of-00003.safetensors:  76%|███ | 3.74G/4.94G [01:37<00:23, 50.6MB/s]\u001b[A\u001b[A\n\nmodel-00001-of-00003.safetensors:  77%|███ | 3.80G/4.94G [01:38<00:16, 69.2MB/s]\u001b[A\u001b[A\nmodel-00002-of-00003.safetensors:  87%|███▍| 4.35G/5.00G [01:38<00:08, 75.0MB/s]\u001b[A\nmodel-00002-of-00003.safetensors:  88%|███▌| 4.42G/5.00G [01:38<00:06, 96.4MB/s]\u001b[A\n\nmodel-00001-of-00003.safetensors:  78%|███▏| 3.87G/4.94G [01:38<00:14, 72.5MB/s]\u001b[A\u001b[A\nmodel-00002-of-00003.safetensors:  90%|████▍| 4.48G/5.00G [01:39<00:04, 120MB/s]\u001b[A\n\nmodel-00001-of-00003.safetensors:  80%|███▏| 3.94G/4.94G [01:39<00:14, 69.9MB/s]\u001b[A\u001b[A\n\nmodel-00001-of-00003.safetensors:  81%|███▏| 4.00G/4.94G [01:40<00:09, 95.4MB/s]\u001b[A\u001b[A\n\nmodel-00001-of-00003.safetensors:  82%|████ | 4.07G/4.94G [01:40<00:06, 128MB/s]\u001b[A\u001b[A\n\nmodel-00001-of-00003.safetensors:  84%|████▏| 4.14G/4.94G [01:40<00:05, 140MB/s]\u001b[A\u001b[A\n\nmodel-00001-of-00003.safetensors:  85%|███▍| 4.21G/4.94G [01:41<00:07, 92.7MB/s]\u001b[A\u001b[A\n\n\nmodel-00003-of-00003.safetensors:  94%|███▊| 4.27G/4.54G [01:41<00:12, 21.2MB/s]\u001b[A\u001b[A\u001b[A\n\n\nmodel-00003-of-00003.safetensors:  96%|███▊| 4.34G/4.54G [01:42<00:06, 29.2MB/s]\u001b[A\u001b[A\u001b[A\nmodel-00002-of-00003.safetensors:  91%|███▋| 4.55G/5.00G [01:42<00:09, 46.1MB/s]\u001b[A\nmodel-00002-of-00003.safetensors:  92%|███▋| 4.60G/5.00G [01:44<00:09, 42.3MB/s]\u001b[A\n\nmodel-00001-of-00003.safetensors:  86%|███▍| 4.27G/4.94G [01:44<00:13, 48.2MB/s]\u001b[A\u001b[A\n\nmodel-00001-of-00003.safetensors:  88%|███▌| 4.34G/4.94G [01:45<00:09, 60.9MB/s]\u001b[A\u001b[A\n\n\nmodel-00003-of-00003.safetensors:  97%|███▉| 4.41G/4.54G [01:45<00:05, 25.4MB/s]\u001b[A\u001b[A\u001b[A\nmodel-00002-of-00003.safetensors:  93%|███▋| 4.66G/5.00G [01:46<00:09, 34.0MB/s]\u001b[A\n\nmodel-00001-of-00003.safetensors:  89%|███▌| 4.41G/4.94G [01:47<00:12, 44.5MB/s]\u001b[A\u001b[A\n\nmodel-00001-of-00003.safetensors:  91%|███▌| 4.47G/4.94G [01:48<00:09, 51.9MB/s]\u001b[A\u001b[A\n\nmodel-00001-of-00003.safetensors:  92%|███▋| 4.54G/4.94G [01:48<00:06, 64.8MB/s]\u001b[A\u001b[A\n\nmodel-00001-of-00003.safetensors:  95%|████▋| 4.67G/4.94G [01:49<00:02, 113MB/s]\u001b[A\u001b[A\n\nmodel-00001-of-00003.safetensors:  96%|████▊| 4.74G/4.94G [01:49<00:01, 130MB/s]\u001b[A\u001b[A\n\nmodel-00001-of-00003.safetensors:  97%|████▊| 4.81G/4.94G [01:49<00:00, 155MB/s]\u001b[A\u001b[A\n\n\nmodel-00003-of-00003.safetensors:  99%|███▉| 4.47G/4.54G [01:49<00:03, 21.9MB/s]\u001b[A\u001b[A\u001b[A\nmodel-00002-of-00003.safetensors:  95%|███▊| 4.73G/5.00G [01:49<00:08, 30.1MB/s]\u001b[A\n\n\nmodel-00003-of-00003.safetensors: 100%|████| 4.54G/4.54G [01:50<00:00, 41.2MB/s]\u001b[A\u001b[A\u001b[A\n\nmodel-00002-of-00003.safetensors:  96%|███▊| 4.80G/5.00G [01:50<00:05, 37.8MB/s]\u001b[A\nmodel-00002-of-00003.safetensors:  97%|███▉| 4.87G/5.00G [01:50<00:02, 48.8MB/s]\u001b[A\n\nmodel-00001-of-00003.safetensors:  99%|███▉| 4.88G/4.94G [01:51<00:00, 92.7MB/s]\u001b[A\u001b[A\nmodel-00002-of-00003.safetensors:  99%|███▉| 4.93G/5.00G [01:51<00:01, 56.8MB/s]\u001b[A\n\nmodel-00001-of-00003.safetensors: 100%|████| 4.94G/4.94G [01:51<00:00, 44.2MB/s]\u001b[A\u001b[A\nFetching 3 files:  33%|████████▋                 | 1/3 [01:52<03:44, 112.09s/it]\nmodel-00002-of-00003.safetensors: 100%|████| 5.00G/5.00G [01:52<00:00, 44.3MB/s]\u001b[A\nFetching 3 files: 100%|███████████████████████████| 3/3 [01:53<00:00, 37.71s/it]\nThe following generation flags are not valid and may be ignored: ['output_hidden_states']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['output_hidden_states']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nLoading checkpoint shards: 100%|██████████████████| 3/3 [01:05<00:00, 21.96s/it]\ngeneration_config.json: 100%|███████████████████| 111/111 [00:00<00:00, 619kB/s]\ntokenizing: 4it [00:00, 573.52it/s]\ntokenizing: 7600it [00:02, 3544.90it/s]\ntokenizing: 1it [00:00, 2051.00it/s]\nEvaluating: 100%|█████████████████████████| 1900/1900 [1:13:24<00:00,  2.32s/it]\nDataset: agnews | Shot: 1 | Acc: 25.00\n","output_type":"stream"}],"execution_count":47},{"cell_type":"markdown","source":"## SST2","metadata":{}},{"cell_type":"code","source":"!python src/run_dect.py \\\n--model auto \\\n--model_name_or_path mistralai/Mistral-7B-Instruct-v0.2 \\\n--dataset sst2 \\\n--max_epochs 30 \\\n--batch_size 4 \\\n--proto_dim 256 \\\n--lr 1e-4 \\\n--seed 42\n\n","metadata":{"execution":{"iopub.status.busy":"2025-07-29T01:19:21.213870Z","iopub.execute_input":"2025-07-29T01:19:21.214322Z","iopub.status.idle":"2025-07-29T01:30:23.996331Z","shell.execute_reply.started":"2025-07-29T01:19:21.214273Z","shell.execute_reply":"2025-07-29T01:30:23.995389Z"},"trusted":true},"outputs":[{"name":"stdout","text":"DEBUG: Processor.labels for sst2: ['0', '1']\nUsing full test set with size: 872\nnormalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.\nThe following generation flags are not valid and may be ignored: ['output_hidden_states']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['output_hidden_states']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nLoading checkpoint shards: 100%|██████████████████| 3/3 [01:21<00:00, 27.33s/it]\ntokenizing: 2it [00:00, 1046.61it/s]\ntokenizing: 872it [00:00, 1528.17it/s]\ntokenizing: 1it [00:00, 1596.61it/s]\nEvaluating: 100%|█████████████████████████████| 218/218 [08:25<00:00,  2.32s/it]\nDataset: sst2 | Shot: 1 | Acc: 91.86\n","output_type":"stream"}],"execution_count":48},{"cell_type":"markdown","source":"## IMDB","metadata":{}},{"cell_type":"code","source":"!python src/run_dect.py \\\n--model auto \\\n--model_name_or_path mistralai/Mistral-7B-Instruct-v0.2 \\\n--dataset imdb \\\n--max_epochs 30 \\\n--batch_size 4 \\\n--proto_dim 256 \\\n--lr 1e-4 \\\n--seed 42\n","metadata":{"execution":{"iopub.status.busy":"2025-07-29T01:30:23.997529Z","iopub.execute_input":"2025-07-29T01:30:23.998451Z","iopub.status.idle":"2025-07-29T05:35:31.174241Z","shell.execute_reply.started":"2025-07-29T01:30:23.998413Z","shell.execute_reply":"2025-07-29T05:35:31.173100Z"},"trusted":true},"outputs":[{"name":"stdout","text":"DEBUG: Processor.labels for imdb: ['negative', 'positive']\nUsing full test set with size: 25000\nnormalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.\nThe following generation flags are not valid and may be ignored: ['output_hidden_states']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['output_hidden_states']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nLoading checkpoint shards: 100%|██████████████████| 3/3 [01:22<00:00, 27.48s/it]\ntokenizing: 2it [00:00, 414.81it/s]\ntokenizing: 25000it [00:53, 471.29it/s]\ntokenizing: 1it [00:00, 1960.87it/s]\nEvaluating: 100%|█████████████████████████| 6250/6250 [4:01:37<00:00,  2.32s/it]\nDataset: imdb | Shot: 1 | Acc: 64.23\n","output_type":"stream"}],"execution_count":49},{"cell_type":"markdown","source":"## mnli","metadata":{}},{"cell_type":"code","source":"!python src/run_dect.py \\\n--model auto \\\n--model_name_or_path mistralai/Mistral-7B-Instruct-v0.2 \\\n--dataset mnli-m \\\n--max_epochs 30 \\\n--batch_size 4 \\\n--proto_dim 256 \\\n--lr 1e-4 \\\n--seed 42\n\n# For other datasets, just change the --dataset parameter","metadata":{"execution":{"iopub.status.busy":"2025-07-29T16:06:45.437756Z","iopub.execute_input":"2025-07-29T16:06:45.438011Z","iopub.status.idle":"2025-07-29T17:46:17.823622Z","shell.execute_reply.started":"2025-07-29T16:06:45.437978Z","shell.execute_reply":"2025-07-29T17:46:17.822424Z"},"trusted":true},"outputs":[{"name":"stdout","text":"DEBUG: Processor.labels for mnli-m: ['contradiction', 'entailment', 'neutral']\nUsing full test set with size: 9815\ntokenizer_config.json: 100%|███████████████| 2.10k/2.10k [00:00<00:00, 12.7MB/s]\ntokenizer.model: 100%|████████████████████████| 493k/493k [00:01<00:00, 450kB/s]\nspecial_tokens_map.json: 100%|█████████████████| 414/414 [00:00<00:00, 3.11MB/s]\ntokenizer.json: 100%|██████████████████████| 1.80M/1.80M [00:00<00:00, 2.49MB/s]\nnormalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.\nconfig.json: 100%|█████████████████████████████| 596/596 [00:00<00:00, 5.01MB/s]\nmodel.safetensors.index.json: 100%|████████| 25.1k/25.1k [00:00<00:00, 65.2MB/s]\nFetching 3 files:   0%|                                   | 0/3 [00:00<?, ?it/s]\nmodel-00003-of-00003.safetensors:   0%|             | 0.00/4.54G [00:00<?, ?B/s]\u001b[A\n\nmodel-00002-of-00003.safetensors:   0%|             | 0.00/5.00G [00:00<?, ?B/s]\u001b[A\u001b[A\n\n\nmodel-00001-of-00003.safetensors:   0%|             | 0.00/4.94G [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\nmodel-00003-of-00003.safetensors:   0%|    | 816k/4.54G [00:02<3:09:28, 399kB/s]\u001b[A\n\nmodel-00002-of-00003.safetensors:   0%|    | 754k/5.00G [00:02<4:27:24, 312kB/s]\u001b[A\u001b[A\n\n\nmodel-00001-of-00003.safetensors:   0%|    | 711k/4.94G [00:02<5:06:39, 269kB/s]\u001b[A\u001b[A\u001b[A\nmodel-00003-of-00003.safetensors:   0%|  | 2.38M/4.54G [00:02<1:14:52, 1.01MB/s]\u001b[A\nmodel-00003-of-00003.safetensors:   2%|    | 69.4M/4.54G [00:02<01:43, 43.0MB/s]\u001b[A\n\nmodel-00002-of-00003.safetensors:   0%|    | 994k/5.00G [00:03<4:10:50, 332kB/s]\u001b[A\u001b[A\n\nmodel-00002-of-00003.safetensors:   0%|   | 2.34M/5.00G [00:03<1:23:51, 993kB/s]\u001b[A\u001b[A\n\n\nmodel-00001-of-00003.safetensors:   0%|   | 2.14M/4.94G [00:03<1:49:33, 752kB/s]\u001b[A\u001b[A\u001b[A\nmodel-00003-of-00003.safetensors:   3%|▏    | 139M/4.54G [00:04<01:49, 40.1MB/s]\u001b[A\n\nmodel-00002-of-00003.safetensors:   0%|    | 20.3M/5.00G [00:05<14:41, 5.65MB/s]\u001b[A\u001b[A\n\nmodel-00002-of-00003.safetensors:   1%|    | 45.9M/5.00G [00:05<05:37, 14.7MB/s]\u001b[A\u001b[A\n\nmodel-00002-of-00003.safetensors:   2%|    | 81.4M/5.00G [00:06<03:34, 23.0MB/s]\u001b[A\u001b[A\n\nmodel-00002-of-00003.safetensors:   2%|     | 123M/5.00G [00:08<03:45, 21.6MB/s]\u001b[A\u001b[A\n\nmodel-00002-of-00003.safetensors:   3%|▏    | 160M/5.00G [00:10<03:35, 22.5MB/s]\u001b[A\u001b[A\n\nmodel-00002-of-00003.safetensors:   5%|▏    | 233M/5.00G [00:12<02:52, 27.6MB/s]\u001b[A\u001b[A\n\n\nmodel-00001-of-00003.safetensors:   0%|    | 20.5M/4.94G [00:12<45:16, 1.81MB/s]\u001b[A\u001b[A\u001b[A\n\nmodel-00002-of-00003.safetensors:   6%|▎    | 281M/5.00G [00:13<02:33, 30.8MB/s]\u001b[A\u001b[A\n\n\nmodel-00001-of-00003.safetensors:   1%|    | 54.4M/4.94G [00:14<15:42, 5.19MB/s]\u001b[A\u001b[A\u001b[A\n\nmodel-00002-of-00003.safetensors:   6%|▎    | 323M/5.00G [00:14<02:32, 30.6MB/s]\u001b[A\u001b[A\n\nmodel-00002-of-00003.safetensors:   9%|▍    | 441M/5.00G [00:15<01:20, 56.5MB/s]\u001b[A\u001b[A\n\nmodel-00002-of-00003.safetensors:  11%|▌    | 566M/5.00G [00:16<01:04, 69.2MB/s]\u001b[A\u001b[A\n\n\nmodel-00001-of-00003.safetensors:   2%|    | 96.1M/4.94G [00:17<10:43, 7.53MB/s]\u001b[A\u001b[A\u001b[A\nmodel-00003-of-00003.safetensors:   6%|▎    | 273M/4.54G [00:17<05:02, 14.1MB/s]\u001b[A\n\nmodel-00002-of-00003.safetensors:  12%|▌    | 600M/5.00G [00:18<01:38, 44.7MB/s]\u001b[A\u001b[A\nmodel-00003-of-00003.safetensors:   7%|▎    | 340M/4.54G [00:19<03:51, 18.2MB/s]\u001b[A\nmodel-00003-of-00003.safetensors:  10%|▌    | 474M/4.54G [00:21<02:27, 27.7MB/s]\u001b[A\n\nmodel-00002-of-00003.safetensors:  14%|▋    | 700M/5.00G [00:21<01:36, 44.5MB/s]\u001b[A\u001b[A\n\nmodel-00002-of-00003.safetensors:  16%|▊    | 812M/5.00G [00:22<01:11, 58.9MB/s]\u001b[A\u001b[A\n\n\nmodel-00001-of-00003.safetensors:   3%|▏    | 163M/4.94G [00:22<07:49, 10.2MB/s]\u001b[A\u001b[A\u001b[A\nmodel-00003-of-00003.safetensors:  12%|▌    | 541M/4.54G [00:22<02:09, 30.9MB/s]\u001b[A\nmodel-00003-of-00003.safetensors:  15%|▋    | 675M/4.54G [00:23<01:19, 48.3MB/s]\u001b[A\n\nmodel-00002-of-00003.safetensors:  18%|▉    | 904M/5.00G [00:23<01:09, 58.9MB/s]\u001b[A\u001b[A\nmodel-00003-of-00003.safetensors:  18%|▉    | 809M/4.54G [00:23<00:56, 66.5MB/s]\u001b[A\n\n\nmodel-00001-of-00003.safetensors:   5%|▏    | 230M/4.94G [00:24<05:24, 14.5MB/s]\u001b[A\u001b[A\u001b[A\nmodel-00003-of-00003.safetensors:  22%|█    | 1.01G/4.54G [00:24<00:34, 103MB/s]\u001b[A\n\nmodel-00002-of-00003.safetensors:  19%|▉    | 928M/5.00G [00:25<01:24, 47.9MB/s]\u001b[A\u001b[A\nmodel-00003-of-00003.safetensors:  24%|▉   | 1.08G/4.54G [00:25<00:40, 85.7MB/s]\u001b[A\n\nmodel-00002-of-00003.safetensors:  20%|▉    | 995M/5.00G [00:26<01:16, 52.1MB/s]\u001b[A\u001b[A\nmodel-00003-of-00003.safetensors:  27%|█▎   | 1.21G/4.54G [00:26<00:27, 120MB/s]\u001b[A\n\n\nmodel-00001-of-00003.safetensors:   6%|▎    | 297M/4.94G [00:26<04:14, 18.3MB/s]\u001b[A\u001b[A\u001b[A\nmodel-00003-of-00003.safetensors:  28%|█▍   | 1.28G/4.54G [00:26<00:27, 120MB/s]\u001b[A\nmodel-00003-of-00003.safetensors:  31%|█▌   | 1.41G/4.54G [00:27<00:24, 125MB/s]\u001b[A\nmodel-00003-of-00003.safetensors:  33%|█▋   | 1.48G/4.54G [00:27<00:21, 140MB/s]\u001b[A\n\n\nmodel-00001-of-00003.safetensors:   7%|▎    | 364M/4.94G [00:28<03:27, 22.1MB/s]\u001b[A\u001b[A\u001b[A\n\n\nmodel-00001-of-00003.safetensors:   9%|▍    | 431M/4.94G [00:29<02:27, 30.6MB/s]\u001b[A\u001b[A\u001b[A\n\nmodel-00002-of-00003.safetensors:  23%|▉   | 1.13G/5.00G [00:29<01:21, 47.8MB/s]\u001b[A\u001b[A\n\n\nmodel-00001-of-00003.safetensors:  10%|▍    | 492M/4.94G [00:29<01:51, 39.7MB/s]\u001b[A\u001b[A\u001b[A\nmodel-00003-of-00003.safetensors:  34%|█▎  | 1.55G/4.54G [00:29<00:34, 87.3MB/s]\u001b[A\n\n\nmodel-00001-of-00003.safetensors:  11%|▌    | 559M/4.94G [00:29<01:19, 55.3MB/s]\u001b[A\u001b[A\u001b[A\nmodel-00003-of-00003.safetensors:  36%|█▍  | 1.61G/4.54G [00:30<00:31, 92.1MB/s]\u001b[A\n\n\nmodel-00001-of-00003.safetensors:  13%|▋    | 626M/4.94G [00:30<01:10, 61.1MB/s]\u001b[A\u001b[A\u001b[A\nmodel-00003-of-00003.safetensors:  37%|█▊   | 1.68G/4.54G [00:30<00:28, 102MB/s]\u001b[A\n\n\nmodel-00001-of-00003.safetensors:  14%|▋    | 693M/4.94G [00:31<01:04, 66.2MB/s]\u001b[A\u001b[A\u001b[A\n\n\nmodel-00001-of-00003.safetensors:  15%|▊    | 761M/4.94G [00:31<00:46, 90.6MB/s]\u001b[A\u001b[A\u001b[A\nmodel-00003-of-00003.safetensors:  38%|█▌  | 1.75G/4.54G [00:31<00:31, 89.9MB/s]\u001b[A\n\n\nmodel-00001-of-00003.safetensors:  17%|█     | 828M/4.94G [00:31<00:34, 121MB/s]\u001b[A\u001b[A\u001b[A\nmodel-00003-of-00003.safetensors:  40%|█▌  | 1.81G/4.54G [00:32<00:34, 78.9MB/s]\u001b[A\n\n\nmodel-00001-of-00003.safetensors:  18%|▉    | 895M/4.94G [00:32<00:43, 92.4MB/s]\u001b[A\u001b[A\u001b[A\n\nmodel-00002-of-00003.safetensors:  25%|▉   | 1.24G/5.00G [00:33<01:44, 36.0MB/s]\u001b[A\u001b[A\nmodel-00003-of-00003.safetensors:  41%|█▋  | 1.88G/4.54G [00:33<00:33, 79.4MB/s]\u001b[A\n\nmodel-00002-of-00003.safetensors:  25%|█   | 1.26G/5.00G [00:34<01:51, 33.6MB/s]\u001b[A\u001b[A\nmodel-00003-of-00003.safetensors:  43%|█▋  | 1.95G/4.54G [00:34<00:36, 71.9MB/s]\u001b[A\n\n\nmodel-00001-of-00003.safetensors:  19%|▉    | 961M/4.94G [00:36<01:38, 40.4MB/s]\u001b[A\u001b[A\u001b[A\n\n\nmodel-00001-of-00003.safetensors:  22%|▉   | 1.10G/4.94G [00:37<01:02, 61.1MB/s]\u001b[A\u001b[A\u001b[A\n\nmodel-00002-of-00003.safetensors:  27%|█   | 1.33G/5.00G [00:37<02:10, 28.1MB/s]\u001b[A\u001b[A\nmodel-00003-of-00003.safetensors:  44%|█▊  | 2.02G/4.54G [00:38<01:02, 40.5MB/s]\u001b[A\n\n\nmodel-00001-of-00003.safetensors:  24%|▉   | 1.16G/4.94G [00:38<00:56, 67.0MB/s]\u001b[A\u001b[A\u001b[A\nmodel-00003-of-00003.safetensors:  46%|█▊  | 2.08G/4.54G [00:38<00:45, 53.8MB/s]\u001b[A\n\n\nmodel-00001-of-00003.safetensors:  25%|▉   | 1.23G/4.94G [00:38<00:46, 80.5MB/s]\u001b[A\u001b[A\u001b[A\nmodel-00003-of-00003.safetensors:  47%|█▉  | 2.15G/4.54G [00:39<00:38, 62.6MB/s]\u001b[A\n\n\nmodel-00001-of-00003.safetensors:  26%|█   | 1.30G/4.94G [00:39<00:47, 77.4MB/s]\u001b[A\u001b[A\u001b[A\n\n\nmodel-00001-of-00003.safetensors:  28%|█   | 1.37G/4.94G [00:40<00:40, 89.1MB/s]\u001b[A\u001b[A\u001b[A\nmodel-00003-of-00003.safetensors:  49%|█▉  | 2.22G/4.54G [00:40<00:42, 54.6MB/s]\u001b[A\n\n\nmodel-00001-of-00003.safetensors:  29%|█▏  | 1.43G/4.94G [00:40<00:38, 90.2MB/s]\u001b[A\u001b[A\u001b[A\n\nmodel-00002-of-00003.safetensors:  28%|█   | 1.39G/5.00G [00:40<02:17, 26.2MB/s]\u001b[A\u001b[A\n\n\nmodel-00001-of-00003.safetensors:  30%|█▏  | 1.50G/4.94G [00:41<00:38, 88.3MB/s]\u001b[A\u001b[A\u001b[A\nmodel-00003-of-00003.safetensors:  50%|██  | 2.28G/4.54G [00:42<00:50, 44.8MB/s]\u001b[A\n\nmodel-00002-of-00003.safetensors:  29%|█▏  | 1.46G/5.00G [00:43<02:12, 26.7MB/s]\u001b[A\u001b[A\nmodel-00003-of-00003.safetensors:  52%|██  | 2.35G/4.54G [00:44<00:50, 43.5MB/s]\u001b[A\nmodel-00003-of-00003.safetensors:  53%|██▏ | 2.42G/4.54G [00:46<00:48, 43.8MB/s]\u001b[A\n\n\nmodel-00001-of-00003.safetensors:  32%|█▎  | 1.57G/4.94G [00:46<01:34, 35.8MB/s]\u001b[A\u001b[A\u001b[A\n\nmodel-00002-of-00003.safetensors:  31%|█▏  | 1.53G/5.00G [00:46<02:20, 24.8MB/s]\u001b[A\u001b[A\nmodel-00003-of-00003.safetensors:  55%|██▏ | 2.49G/4.54G [00:47<00:47, 43.3MB/s]\u001b[A\nmodel-00003-of-00003.safetensors:  56%|██▏ | 2.55G/4.54G [00:48<00:38, 52.3MB/s]\u001b[A\n\nmodel-00002-of-00003.safetensors:  32%|█▎  | 1.59G/5.00G [00:49<02:18, 24.6MB/s]\u001b[A\u001b[A\n\n\nmodel-00001-of-00003.safetensors:  34%|█▍  | 1.70G/4.94G [00:49<01:23, 38.9MB/s]\u001b[A\u001b[A\u001b[A\n\nmodel-00002-of-00003.safetensors:  33%|█▎  | 1.66G/5.00G [00:49<01:37, 34.1MB/s]\u001b[A\u001b[A\n\n\nmodel-00001-of-00003.safetensors:  39%|█▌  | 1.90G/4.94G [00:49<00:41, 74.0MB/s]\u001b[A\u001b[A\u001b[A\nmodel-00003-of-00003.safetensors:  58%|██▎ | 2.62G/4.54G [00:49<00:38, 50.1MB/s]\u001b[A\n\nmodel-00002-of-00003.safetensors:  34%|█▎  | 1.72G/5.00G [00:50<01:26, 38.1MB/s]\u001b[A\u001b[A\nmodel-00003-of-00003.safetensors:  59%|██▎ | 2.69G/4.54G [00:50<00:32, 57.1MB/s]\u001b[A\n\n\nmodel-00001-of-00003.safetensors:  40%|█▌  | 1.97G/4.94G [00:50<00:43, 68.2MB/s]\u001b[A\u001b[A\u001b[A\n\nmodel-00002-of-00003.safetensors:  36%|█▍  | 1.79G/5.00G [00:51<01:11, 45.1MB/s]\u001b[A\u001b[A\n\nmodel-00002-of-00003.safetensors:  37%|█▍  | 1.85G/5.00G [00:52<00:59, 53.1MB/s]\u001b[A\u001b[A\n\nmodel-00002-of-00003.safetensors:  38%|█▌  | 1.92G/5.00G [00:52<00:45, 68.4MB/s]\u001b[A\u001b[A\n\nmodel-00002-of-00003.safetensors:  40%|█▌  | 1.99G/5.00G [00:52<00:36, 83.1MB/s]\u001b[A\u001b[A\n\nmodel-00002-of-00003.safetensors:  40%|█▌  | 2.00G/5.00G [00:53<00:39, 75.1MB/s]\u001b[A\u001b[A\n\n\nmodel-00001-of-00003.safetensors:  41%|█▋  | 2.04G/4.94G [00:54<01:04, 45.2MB/s]\u001b[A\u001b[A\u001b[A\n\nmodel-00002-of-00003.safetensors:  40%|█▌  | 2.02G/5.00G [00:54<00:53, 55.7MB/s]\u001b[A\u001b[A\n\n\nmodel-00001-of-00003.safetensors:  43%|█▋  | 2.10G/4.94G [00:54<00:56, 50.5MB/s]\u001b[A\u001b[A\u001b[A\n\nmodel-00002-of-00003.safetensors:  42%|█▋  | 2.09G/5.00G [00:55<01:01, 47.1MB/s]\u001b[A\u001b[A\n\nmodel-00002-of-00003.safetensors:  42%|█▋  | 2.12G/5.00G [00:57<01:11, 40.3MB/s]\u001b[A\u001b[A\nmodel-00003-of-00003.safetensors:  61%|██▍ | 2.75G/4.54G [00:57<01:13, 24.3MB/s]\u001b[A\n\n\nmodel-00001-of-00003.safetensors:  44%|█▊  | 2.17G/4.94G [00:57<01:05, 42.4MB/s]\u001b[A\u001b[A\u001b[A\nmodel-00003-of-00003.safetensors:  62%|██▍ | 2.82G/4.54G [00:57<00:55, 30.8MB/s]\u001b[A\n\nmodel-00002-of-00003.safetensors:  43%|█▋  | 2.13G/5.00G [00:58<01:38, 29.1MB/s]\u001b[A\u001b[A\n\nmodel-00002-of-00003.safetensors:  43%|█▋  | 2.14G/5.00G [00:58<01:32, 30.9MB/s]\u001b[A\u001b[A\n\nmodel-00002-of-00003.safetensors:  43%|█▋  | 2.15G/5.00G [00:58<01:28, 32.2MB/s]\u001b[A\u001b[A\n\n\nmodel-00001-of-00003.safetensors:  45%|█▊  | 2.22G/4.94G [00:59<01:14, 36.6MB/s]\u001b[A\u001b[A\u001b[A\nmodel-00003-of-00003.safetensors:  64%|██▌ | 2.89G/4.54G [01:00<00:53, 30.9MB/s]\u001b[A\nmodel-00003-of-00003.safetensors:  65%|██▌ | 2.95G/4.54G [01:00<00:37, 42.1MB/s]\u001b[A\nmodel-00003-of-00003.safetensors:  67%|██▋ | 3.02G/4.54G [01:00<00:28, 52.5MB/s]\u001b[A\n\n\nmodel-00001-of-00003.safetensors:  46%|█▊  | 2.29G/4.94G [01:00<01:09, 38.4MB/s]\u001b[A\u001b[A\u001b[A\n\nmodel-00002-of-00003.safetensors:  44%|█▋  | 2.18G/5.00G [01:01<02:27, 19.2MB/s]\u001b[A\u001b[A\nmodel-00003-of-00003.safetensors:  68%|██▋ | 3.09G/4.54G [01:01<00:22, 64.2MB/s]\u001b[A\nmodel-00003-of-00003.safetensors:  70%|██▊ | 3.16G/4.54G [01:01<00:16, 83.4MB/s]\u001b[A\n\nmodel-00002-of-00003.safetensors:  45%|█▊  | 2.24G/5.00G [01:02<01:26, 31.8MB/s]\u001b[A\u001b[A\nmodel-00003-of-00003.safetensors:  71%|██▊ | 3.22G/4.54G [01:02<00:16, 79.9MB/s]\u001b[A\n\nmodel-00002-of-00003.safetensors:  46%|█▊  | 2.30G/5.00G [01:02<01:04, 42.2MB/s]\u001b[A\u001b[A\n\nmodel-00002-of-00003.safetensors:  46%|█▊  | 2.32G/5.00G [01:03<00:58, 45.8MB/s]\u001b[A\u001b[A\nmodel-00003-of-00003.safetensors:  72%|██▉ | 3.29G/4.54G [01:03<00:15, 81.7MB/s]\u001b[A\n\nmodel-00002-of-00003.safetensors:  47%|█▊  | 2.33G/5.00G [01:03<00:57, 46.6MB/s]\u001b[A\u001b[A\nmodel-00003-of-00003.safetensors:  74%|███▋ | 3.36G/4.54G [01:03<00:10, 108MB/s]\u001b[A\n\nmodel-00002-of-00003.safetensors:  47%|█▉  | 2.35G/5.00G [01:03<00:53, 49.4MB/s]\u001b[A\u001b[A\n\nmodel-00002-of-00003.safetensors:  48%|█▉  | 2.38G/5.00G [01:04<01:05, 39.8MB/s]\u001b[A\u001b[A\n\n\nmodel-00001-of-00003.safetensors:  48%|█▉  | 2.35G/4.94G [01:05<01:38, 26.4MB/s]\u001b[A\u001b[A\u001b[A\nmodel-00003-of-00003.safetensors:  75%|███ | 3.42G/4.54G [01:05<00:19, 56.5MB/s]\u001b[A\n\nmodel-00002-of-00003.safetensors:  49%|█▉  | 2.44G/5.00G [01:05<00:57, 44.3MB/s]\u001b[A\u001b[A\n\nmodel-00002-of-00003.safetensors:  49%|█▉  | 2.46G/5.00G [01:06<00:54, 46.8MB/s]\u001b[A\u001b[A\n\nmodel-00002-of-00003.safetensors:  50%|█▉  | 2.48G/5.00G [01:06<01:00, 41.4MB/s]\u001b[A\u001b[A\n\n\nmodel-00001-of-00003.safetensors:  49%|█▉  | 2.42G/4.94G [01:07<01:33, 27.0MB/s]\u001b[A\u001b[A\u001b[A\n\nmodel-00002-of-00003.safetensors:  51%|██  | 2.54G/5.00G [01:08<01:05, 37.2MB/s]\u001b[A\u001b[A\nmodel-00003-of-00003.safetensors:  77%|███ | 3.49G/4.54G [01:08<00:27, 38.5MB/s]\u001b[A\nmodel-00003-of-00003.safetensors:  78%|███▏| 3.55G/4.54G [01:09<00:21, 46.9MB/s]\u001b[A\n\nmodel-00002-of-00003.safetensors:  52%|██  | 2.58G/5.00G [01:09<01:04, 37.7MB/s]\u001b[A\u001b[A\n\nmodel-00002-of-00003.safetensors:  52%|██  | 2.61G/5.00G [01:10<00:53, 44.5MB/s]\u001b[A\u001b[A\n\nmodel-00002-of-00003.safetensors:  53%|██  | 2.65G/5.00G [01:10<00:40, 58.5MB/s]\u001b[A\u001b[A\nmodel-00003-of-00003.safetensors:  80%|███▏| 3.62G/4.54G [01:10<00:17, 52.8MB/s]\u001b[A\n\nmodel-00002-of-00003.safetensors:  54%|██▏ | 2.72G/5.00G [01:11<00:36, 62.8MB/s]\u001b[A\u001b[A\n\nmodel-00002-of-00003.safetensors:  55%|██▏ | 2.77G/5.00G [01:11<00:26, 83.5MB/s]\u001b[A\u001b[A\n\nmodel-00002-of-00003.safetensors:  56%|██▏ | 2.80G/5.00G [01:11<00:22, 97.0MB/s]\u001b[A\u001b[A\n\n\nmodel-00001-of-00003.safetensors:  50%|██  | 2.49G/4.94G [01:11<01:47, 22.9MB/s]\u001b[A\u001b[A\u001b[A\n\n\nmodel-00001-of-00003.safetensors:  52%|██  | 2.56G/4.94G [01:11<01:14, 31.9MB/s]\u001b[A\u001b[A\u001b[A\n\nmodel-00002-of-00003.safetensors:  57%|██▎ | 2.86G/5.00G [01:12<00:28, 75.1MB/s]\u001b[A\u001b[A\nmodel-00003-of-00003.safetensors:  81%|███▏| 3.69G/4.54G [01:13<00:22, 37.4MB/s]\u001b[A\n\n\nmodel-00001-of-00003.safetensors:  53%|██  | 2.62G/4.94G [01:13<01:11, 32.4MB/s]\u001b[A\u001b[A\u001b[A\nmodel-00003-of-00003.safetensors:  83%|███▎| 3.76G/4.54G [01:14<00:17, 44.1MB/s]\u001b[A\n\nmodel-00002-of-00003.safetensors:  58%|██▎ | 2.90G/5.00G [01:14<00:46, 45.0MB/s]\u001b[A\u001b[A\n\n\nmodel-00001-of-00003.safetensors:  54%|██▏ | 2.69G/4.94G [01:14<00:57, 39.3MB/s]\u001b[A\u001b[A\u001b[A\n\n\nmodel-00001-of-00003.safetensors:  56%|██▏ | 2.76G/4.94G [01:15<00:43, 50.3MB/s]\u001b[A\u001b[A\u001b[A\n\nmodel-00002-of-00003.safetensors:  59%|██▎ | 2.96G/5.00G [01:17<00:59, 34.2MB/s]\u001b[A\u001b[A\nmodel-00003-of-00003.safetensors:  84%|███▎| 3.82G/4.54G [01:17<00:20, 34.8MB/s]\u001b[A\n\nmodel-00002-of-00003.safetensors:  59%|██▍ | 2.97G/5.00G [01:17<00:56, 35.8MB/s]\u001b[A\u001b[A\n\n\nmodel-00001-of-00003.safetensors:  57%|██▎ | 2.82G/4.94G [01:17<00:52, 40.2MB/s]\u001b[A\u001b[A\u001b[A\n\nmodel-00002-of-00003.safetensors:  61%|██▍ | 3.03G/5.00G [01:19<01:03, 30.8MB/s]\u001b[A\u001b[A\n\n\nmodel-00001-of-00003.safetensors:  58%|██▎ | 2.89G/4.94G [01:20<00:58, 34.8MB/s]\u001b[A\u001b[A\u001b[A\n\nmodel-00002-of-00003.safetensors:  62%|██▍ | 3.08G/5.00G [01:20<00:52, 36.8MB/s]\u001b[A\u001b[A\n\nmodel-00002-of-00003.safetensors:  63%|██▌ | 3.15G/5.00G [01:20<00:36, 51.4MB/s]\u001b[A\u001b[A\n\nmodel-00002-of-00003.safetensors:  64%|██▌ | 3.22G/5.00G [01:21<00:23, 76.3MB/s]\u001b[A\u001b[A\nmodel-00003-of-00003.safetensors:  86%|███▍| 3.89G/4.54G [01:21<00:24, 26.7MB/s]\u001b[A\n\nmodel-00002-of-00003.safetensors:  65%|██▌ | 3.26G/5.00G [01:21<00:20, 84.9MB/s]\u001b[A\u001b[A\n\nmodel-00002-of-00003.safetensors:  66%|██▋ | 3.31G/5.00G [01:21<00:18, 90.2MB/s]\u001b[A\u001b[A\n\n\nmodel-00001-of-00003.safetensors:  60%|██▍ | 2.96G/4.94G [01:21<00:54, 36.3MB/s]\u001b[A\u001b[A\u001b[A\n\n\nmodel-00001-of-00003.safetensors:  61%|██▍ | 3.03G/4.94G [01:22<00:40, 47.3MB/s]\u001b[A\u001b[A\u001b[A\nmodel-00003-of-00003.safetensors:  87%|███▍| 3.96G/4.54G [01:22<00:18, 30.7MB/s]\u001b[A\n\n\nmodel-00001-of-00003.safetensors:  63%|██▌ | 3.09G/4.94G [01:22<00:30, 59.8MB/s]\u001b[A\u001b[A\u001b[A\n\n\nmodel-00001-of-00003.safetensors:  64%|██▌ | 3.16G/4.94G [01:23<00:26, 67.6MB/s]\u001b[A\u001b[A\u001b[A\n\nmodel-00002-of-00003.safetensors:  67%|██▋ | 3.35G/5.00G [01:23<00:30, 53.9MB/s]\u001b[A\u001b[A\n\nmodel-00002-of-00003.safetensors:  68%|██▋ | 3.42G/5.00G [01:23<00:21, 72.8MB/s]\u001b[A\u001b[A\n\nmodel-00002-of-00003.safetensors:  70%|██▊ | 3.49G/5.00G [01:24<00:15, 98.9MB/s]\u001b[A\u001b[A\nmodel-00003-of-00003.safetensors:  89%|███▌| 4.02G/4.54G [01:25<00:18, 28.5MB/s]\u001b[A\n\nmodel-00002-of-00003.safetensors:  71%|██▊ | 3.55G/5.00G [01:25<00:19, 72.7MB/s]\u001b[A\u001b[A\n\nmodel-00002-of-00003.safetensors:  72%|██▉ | 3.62G/5.00G [01:26<00:16, 83.3MB/s]\u001b[A\u001b[A\n\nmodel-00002-of-00003.safetensors:  74%|██▉ | 3.69G/5.00G [01:26<00:15, 83.3MB/s]\u001b[A\u001b[A\n\nmodel-00002-of-00003.safetensors:  75%|███ | 3.76G/5.00G [01:28<00:18, 68.2MB/s]\u001b[A\u001b[A\n\n\nmodel-00001-of-00003.safetensors:  65%|██▌ | 3.23G/4.94G [01:28<00:58, 29.5MB/s]\u001b[A\u001b[A\u001b[A\n\n\nmodel-00001-of-00003.safetensors:  67%|██▋ | 3.29G/4.94G [01:29<00:43, 37.6MB/s]\u001b[A\u001b[A\u001b[A\n\n\nmodel-00001-of-00003.safetensors:  68%|██▋ | 3.36G/4.94G [01:29<00:34, 45.7MB/s]\u001b[A\u001b[A\u001b[A\n\nmodel-00002-of-00003.safetensors:  76%|███ | 3.82G/5.00G [01:31<00:27, 42.3MB/s]\u001b[A\u001b[A\n\n\nmodel-00001-of-00003.safetensors:  69%|██▊ | 3.43G/4.94G [01:31<00:32, 46.6MB/s]\u001b[A\u001b[A\u001b[A\n\nmodel-00002-of-00003.safetensors:  78%|███ | 3.89G/5.00G [01:31<00:20, 54.8MB/s]\u001b[A\u001b[A\nmodel-00003-of-00003.safetensors:  90%|███▌| 4.09G/4.54G [01:31<00:24, 18.5MB/s]\u001b[A\nmodel-00003-of-00003.safetensors:  92%|███▋| 4.16G/4.54G [01:32<00:15, 25.3MB/s]\u001b[A\n\nmodel-00002-of-00003.safetensors:  79%|███▏| 3.96G/5.00G [01:32<00:17, 60.0MB/s]\u001b[A\u001b[A\n\n\nmodel-00001-of-00003.safetensors:  71%|██▊ | 3.49G/4.94G [01:32<00:29, 48.4MB/s]\u001b[A\u001b[A\u001b[A\n\nmodel-00002-of-00003.safetensors:  80%|███▏| 4.02G/5.00G [01:33<00:15, 62.6MB/s]\u001b[A\u001b[A\n\nmodel-00002-of-00003.safetensors:  82%|███▎| 4.09G/5.00G [01:33<00:12, 75.2MB/s]\u001b[A\u001b[A\n\nmodel-00002-of-00003.safetensors:  83%|███▎| 4.14G/5.00G [01:34<00:09, 89.0MB/s]\u001b[A\u001b[A\n\n\nmodel-00001-of-00003.safetensors:  72%|██▉ | 3.56G/4.94G [01:34<00:30, 45.2MB/s]\u001b[A\u001b[A\u001b[A\nmodel-00003-of-00003.safetensors:  93%|███▋| 4.23G/4.54G [01:34<00:12, 26.0MB/s]\u001b[A\nmodel-00003-of-00003.safetensors:  95%|███▊| 4.29G/4.54G [01:34<00:06, 35.9MB/s]\u001b[A\n\n\nmodel-00001-of-00003.safetensors:  73%|██▉ | 3.63G/4.94G [01:35<00:25, 52.3MB/s]\u001b[A\u001b[A\u001b[A\n\nmodel-00002-of-00003.safetensors:  84%|███▎| 4.19G/5.00G [01:35<00:13, 61.2MB/s]\u001b[A\u001b[A\n\nmodel-00002-of-00003.safetensors:  85%|███▍| 4.24G/5.00G [01:35<00:09, 76.1MB/s]\u001b[A\u001b[A\n\n\nmodel-00001-of-00003.safetensors:  76%|███ | 3.76G/4.94G [01:37<00:20, 57.0MB/s]\u001b[A\u001b[A\u001b[A\n\n\nmodel-00001-of-00003.safetensors:  77%|███ | 3.83G/4.94G [01:38<00:20, 54.0MB/s]\u001b[A\u001b[A\u001b[A\nmodel-00003-of-00003.safetensors:  96%|███▊| 4.36G/4.54G [01:38<00:06, 27.3MB/s]\u001b[A\nmodel-00003-of-00003.safetensors:  97%|███▉| 4.41G/4.54G [01:39<00:04, 33.0MB/s]\u001b[A\n\n\nmodel-00001-of-00003.safetensors:  79%|███▏| 3.89G/4.94G [01:39<00:19, 53.0MB/s]\u001b[A\u001b[A\u001b[A\n\nmodel-00002-of-00003.safetensors:  86%|███▍| 4.29G/5.00G [01:40<00:26, 26.9MB/s]\u001b[A\u001b[A\n\n\nmodel-00001-of-00003.safetensors:  80%|███▏| 3.96G/4.94G [01:41<00:19, 50.3MB/s]\u001b[A\u001b[A\u001b[A\n\nmodel-00002-of-00003.safetensors:  87%|███▍| 4.35G/5.00G [01:42<00:21, 30.4MB/s]\u001b[A\u001b[A\n\n\nmodel-00001-of-00003.safetensors:  81%|███▎| 4.03G/4.94G [01:42<00:16, 54.6MB/s]\u001b[A\u001b[A\u001b[A\n\n\nmodel-00001-of-00003.safetensors:  83%|███▎| 4.09G/4.94G [01:42<00:12, 70.5MB/s]\u001b[A\u001b[A\u001b[A\n\n\nmodel-00001-of-00003.safetensors:  84%|███▎| 4.16G/4.94G [01:42<00:08, 94.9MB/s]\u001b[A\u001b[A\u001b[A\n\n\nmodel-00001-of-00003.safetensors:  85%|████▎| 4.21G/4.94G [01:43<00:07, 103MB/s]\u001b[A\u001b[A\u001b[A\n\nmodel-00002-of-00003.safetensors:  88%|███▌| 4.41G/5.00G [01:43<00:15, 37.2MB/s]\u001b[A\u001b[A\n\nmodel-00002-of-00003.safetensors:  90%|███▌| 4.49G/5.00G [01:43<00:10, 49.0MB/s]\u001b[A\u001b[A\n\nmodel-00002-of-00003.safetensors:  91%|███▋| 4.55G/5.00G [01:44<00:07, 59.4MB/s]\u001b[A\u001b[A\n\nmodel-00002-of-00003.safetensors:  92%|███▋| 4.62G/5.00G [01:45<00:05, 68.5MB/s]\u001b[A\u001b[A\n\n\nmodel-00001-of-00003.safetensors:  86%|███▍| 4.27G/4.94G [01:45<00:10, 61.1MB/s]\u001b[A\u001b[A\u001b[A\n\nmodel-00002-of-00003.safetensors:  94%|███▋| 4.69G/5.00G [01:45<00:03, 91.6MB/s]\u001b[A\u001b[A\n\n\nmodel-00001-of-00003.safetensors:  88%|███▌| 4.34G/4.94G [01:45<00:08, 75.2MB/s]\u001b[A\u001b[A\u001b[A\n\n\nmodel-00001-of-00003.safetensors:  89%|███▌| 4.41G/4.94G [01:45<00:05, 98.5MB/s]\u001b[A\u001b[A\u001b[A\n\nmodel-00002-of-00003.safetensors:  95%|███▊| 4.75G/5.00G [01:45<00:02, 97.9MB/s]\u001b[A\u001b[A\n\n\nmodel-00001-of-00003.safetensors:  91%|████▌| 4.47G/4.94G [01:45<00:03, 124MB/s]\u001b[A\u001b[A\u001b[A\n\nmodel-00002-of-00003.safetensors:  96%|████▊| 4.80G/5.00G [01:46<00:01, 110MB/s]\u001b[A\u001b[A\n\n\nmodel-00001-of-00003.safetensors:  92%|████▌| 4.54G/4.94G [01:46<00:03, 114MB/s]\u001b[A\u001b[A\u001b[A\n\n\nmodel-00001-of-00003.safetensors:  93%|████▋| 4.61G/4.94G [01:46<00:02, 141MB/s]\u001b[A\u001b[A\u001b[A\nmodel-00003-of-00003.safetensors:  99%|███▉| 4.47G/4.54G [01:47<00:03, 17.1MB/s]\u001b[A\n\n\nmodel-00001-of-00003.safetensors:  95%|████▋| 4.67G/4.94G [01:47<00:01, 146MB/s]\u001b[A\u001b[A\u001b[A\n\n\nmodel-00001-of-00003.safetensors:  96%|███▊| 4.74G/4.94G [01:48<00:02, 86.6MB/s]\u001b[A\u001b[A\u001b[A\n\n\nmodel-00001-of-00003.safetensors:  97%|████▊| 4.81G/4.94G [01:49<00:01, 106MB/s]\u001b[A\u001b[A\u001b[A\n\n\nmodel-00001-of-00003.safetensors:  99%|████▉| 4.88G/4.94G [01:49<00:00, 126MB/s]\u001b[A\u001b[A\u001b[A\n\nmodel-00002-of-00003.safetensors:  97%|███▉| 4.87G/5.00G [01:49<00:03, 40.1MB/s]\u001b[A\u001b[A\nmodel-00003-of-00003.safetensors: 100%|████| 4.54G/4.54G [01:51<00:00, 40.8MB/s]\u001b[A\n\n\nmodel-00002-of-00003.safetensors:  99%|███▉| 4.93G/5.00G [01:51<00:01, 39.7MB/s]\u001b[A\u001b[A\n\nmodel-00002-of-00003.safetensors: 100%|████| 5.00G/5.00G [01:51<00:00, 44.7MB/s]\u001b[A\u001b[A\n\n\n\nmodel-00001-of-00003.safetensors: 100%|████| 4.94G/4.94G [01:52<00:00, 44.0MB/s]\u001b[A\u001b[A\u001b[A\nFetching 3 files: 100%|███████████████████████████| 3/3 [01:52<00:00, 37.62s/it]\nThe following generation flags are not valid and may be ignored: ['output_hidden_states']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['output_hidden_states']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nLoading checkpoint shards: 100%|██████████████████| 3/3 [01:12<00:00, 24.00s/it]\ngeneration_config.json: 100%|███████████████████| 111/111 [00:00<00:00, 982kB/s]\ntokenizing: 3it [00:00, 297.34it/s]\ntokenizing: 9815it [00:05, 1842.98it/s]\ntokenizing: 1it [00:00, 1953.56it/s]\nEvaluating: 100%|█████████████████████████| 2454/2454 [1:34:50<00:00,  2.32s/it]\nDataset: mnli-m | Shot: 1 | Acc: 33.33\n","output_type":"stream"}],"execution_count":47},{"cell_type":"code","source":"!python src/run_dect.py \\\n--model auto \\\n--model_name_or_path mistralai/Mistral-7B-Instruct-v0.2 \\\n--dataset mnli-mm \\\n--max_epochs 30 \\\n--batch_size 4 \\\n--proto_dim 256 \\\n--lr 1e-4 \\\n--seed 42\n\n# For other datasets, just change the --dataset parameter","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-30T01:55:39.017914Z","iopub.execute_input":"2025-07-30T01:55:39.018321Z","iopub.status.idle":"2025-07-30T03:33:29.761536Z","shell.execute_reply.started":"2025-07-30T01:55:39.018282Z","shell.execute_reply":"2025-07-30T03:33:29.760287Z"}},"outputs":[{"name":"stdout","text":"DEBUG: Processor.labels for mnli-mm: ['contradiction', 'entailment', 'neutral']\nUsing full test set with size: 9832\nnormalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.\nThe following generation flags are not valid and may be ignored: ['output_hidden_states']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['output_hidden_states']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nLoading checkpoint shards: 100%|██████████████████| 3/3 [01:13<00:00, 24.59s/it]\ntokenizing: 3it [00:00, 657.28it/s]\ntokenizing: 9832it [00:05, 1646.34it/s]\ntokenizing: 1it [00:00, 1197.00it/s]\nEvaluating: 100%|█████████████████████████| 2458/2458 [1:35:01<00:00,  2.32s/it]\nDataset: mnli-mm | Shot: 1 | Acc: 31.99\n","output_type":"stream"}],"execution_count":61},{"cell_type":"markdown","source":"## snli","metadata":{}},{"cell_type":"code","source":"# For AGNews\n!python src/run_dect.py \\\n--model auto \\\n--model_name_or_path mistralai/Mistral-7B-Instruct-v0.2 \\\n--dataset snli \\\n--max_epochs 30 \\\n--batch_size 4 \\\n--proto_dim 256 \\\n--lr 1e-4 \\\n--seed 42\n\n# For other datasets, just change the --dataset parameter","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-30T05:21:37.864982Z","iopub.execute_input":"2025-07-30T05:21:37.865230Z","execution_failed":"2025-07-30T05:22:53.140Z"}},"outputs":[{"name":"stdout","text":"DEBUG: Processor.labels for snli: ['contradiction', 'entailment', 'neutral']\nUsing full test set with size: 9831\ntokenizer_config.json: 100%|███████████████| 2.10k/2.10k [00:00<00:00, 6.57MB/s]\ntokenizer.model: 100%|███████████████████████| 493k/493k [00:00<00:00, 1.08MB/s]\nspecial_tokens_map.json: 100%|█████████████████| 414/414 [00:00<00:00, 4.01MB/s]\ntokenizer.json: 100%|██████████████████████| 1.80M/1.80M [00:00<00:00, 9.34MB/s]\nnormalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.\nconfig.json: 100%|█████████████████████████████| 596/596 [00:00<00:00, 4.65MB/s]\nmodel.safetensors.index.json: 100%|█████████| 25.1k/25.1k [00:00<00:00, 104MB/s]\nFetching 3 files:   0%|                                   | 0/3 [00:00<?, ?it/s]\nmodel-00003-of-00003.safetensors:   0%|             | 0.00/4.54G [00:00<?, ?B/s]\u001b[A\n\nmodel-00001-of-00003.safetensors:   0%|             | 0.00/4.94G [00:00<?, ?B/s]\u001b[A\u001b[A\n\n\nmodel-00002-of-00003.safetensors:   0%|             | 0.00/5.00G [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\nmodel-00003-of-00003.safetensors:   0%|    | 816k/4.54G [00:01<1:53:06, 669kB/s]\u001b[A\n\nmodel-00001-of-00003.safetensors:   0%|    | 711k/4.94G [00:01<2:26:55, 561kB/s]\u001b[A\u001b[A\n\n\nmodel-00002-of-00003.safetensors:   0%|    | 754k/5.00G [00:01<2:24:06, 578kB/s]\u001b[A\u001b[A\u001b[A\n\nmodel-00001-of-00003.safetensors:   0%|    | 2.14M/4.94G [00:01<59:51, 1.38MB/s]\u001b[A\u001b[A\n\n\nmodel-00002-of-00003.safetensors:   0%|    | 994k/5.00G [00:01<2:33:27, 543kB/s]\u001b[A\u001b[A\u001b[A\nmodel-00003-of-00003.safetensors:   0%|    | 2.38M/4.54G [00:02<59:57, 1.26MB/s]\u001b[A\n\n\nmodel-00002-of-00003.safetensors:   0%|    | 12.1M/5.00G [00:03<19:56, 4.17MB/s]\u001b[A\u001b[A\u001b[A\nmodel-00003-of-00003.safetensors:   2%|    | 69.5M/4.54G [00:03<03:19, 22.4MB/s]\u001b[A\n\n\nmodel-00002-of-00003.safetensors:   1%|    | 30.1M/5.00G [00:05<11:57, 6.93MB/s]\u001b[A\u001b[A\u001b[A\nmodel-00003-of-00003.safetensors:   3%|▏    | 137M/4.54G [00:05<02:28, 29.7MB/s]\u001b[A\n\n\nmodel-00002-of-00003.safetensors:   1%|    | 39.2M/5.00G [00:06<10:14, 8.07MB/s]\u001b[A\u001b[A\u001b[A\n\nmodel-00001-of-00003.safetensors:   0%|    | 20.5M/4.94G [00:06<23:31, 3.49MB/s]\u001b[A\u001b[A\n\n\nmodel-00002-of-00003.safetensors:   2%|     | 111M/5.00G [00:06<02:42, 30.0MB/s]\u001b[A\u001b[A\u001b[A\n\nmodel-00001-of-00003.safetensors:   2%|    | 87.6M/4.94G [00:07<04:53, 16.6MB/s]\u001b[A\u001b[A\n\n\nmodel-00002-of-00003.safetensors:   3%|▏    | 130M/5.00G [00:07<02:54, 27.9MB/s]\u001b[A\u001b[A\u001b[A\nmodel-00003-of-00003.safetensors:   4%|▏    | 204M/4.54G [00:07<02:18, 31.2MB/s]\u001b[A\n\n\nmodel-00002-of-00003.safetensors:   3%|▏    | 163M/5.00G [00:08<02:16, 35.4MB/s]\u001b[A\u001b[A\u001b[A\n\nmodel-00001-of-00003.safetensors:   3%|▏    | 157M/4.94G [00:09<03:19, 23.9MB/s]\u001b[A\u001b[A\nmodel-00003-of-00003.safetensors:   6%|▎    | 271M/4.54G [00:09<02:05, 34.0MB/s]\u001b[A\n\n\nmodel-00002-of-00003.safetensors:   4%|▏    | 182M/5.00G [00:09<02:58, 27.0MB/s]\u001b[A\u001b[A\u001b[A\n\nmodel-00001-of-00003.safetensors:   6%|▎    | 291M/4.94G [00:10<01:37, 47.9MB/s]\u001b[A\u001b[A\nmodel-00003-of-00003.safetensors:   7%|▎    | 338M/4.54G [00:10<01:46, 39.4MB/s]\u001b[A\n\n\nmodel-00002-of-00003.safetensors:   4%|▏    | 198M/5.00G [00:10<03:41, 21.6MB/s]\u001b[A\u001b[A\u001b[A\n\nmodel-00001-of-00003.safetensors:   7%|▎    | 358M/4.94G [00:10<01:20, 57.0MB/s]\u001b[A\u001b[A\n\n\nmodel-00002-of-00003.safetensors:   5%|▎    | 257M/5.00G [00:10<01:54, 41.3MB/s]\u001b[A\u001b[A\u001b[A\n\n\nmodel-00002-of-00003.safetensors:   6%|▎    | 290M/5.00G [00:11<02:00, 39.0MB/s]\u001b[A\u001b[A\u001b[A\n\nmodel-00001-of-00003.safetensors:   8%|▍    | 400M/4.94G [00:11<01:29, 51.0MB/s]\u001b[A\u001b[A\nmodel-00003-of-00003.safetensors:   9%|▍    | 405M/4.54G [00:12<01:45, 39.3MB/s]\u001b[A\n\n\nmodel-00002-of-00003.safetensors:   7%|▎    | 339M/5.00G [00:12<01:40, 46.2MB/s]\u001b[A\u001b[A\u001b[A\nmodel-00003-of-00003.safetensors:  10%|▌    | 472M/4.54G [00:14<01:45, 38.5MB/s]\u001b[A\n\n\nmodel-00002-of-00003.safetensors:   8%|▍    | 410M/5.00G [00:14<01:37, 46.9MB/s]\u001b[A\u001b[A\u001b[A\n\nmodel-00001-of-00003.safetensors:   9%|▍    | 467M/4.94G [00:14<01:54, 39.0MB/s]\u001b[A\u001b[A\nmodel-00003-of-00003.safetensors:  12%|▌    | 539M/4.54G [00:14<01:22, 48.4MB/s]\u001b[A\n\n\nmodel-00002-of-00003.safetensors:  10%|▍    | 477M/5.00G [00:15<01:22, 54.5MB/s]\u001b[A\u001b[A\u001b[A\n\nmodel-00001-of-00003.safetensors:  11%|▌    | 534M/4.94G [00:15<01:34, 46.6MB/s]\u001b[A\u001b[A\nmodel-00003-of-00003.safetensors:  13%|▋    | 606M/4.54G [00:15<01:12, 54.5MB/s]\u001b[A\n\n\nmodel-00002-of-00003.safetensors:  11%|▌    | 544M/5.00G [00:15<01:14, 60.1MB/s]\u001b[A\u001b[A\u001b[A\n\n\nmodel-00002-of-00003.safetensors:  13%|▋    | 636M/5.00G [00:16<00:52, 83.6MB/s]\u001b[A\u001b[A\u001b[A\n\nmodel-00001-of-00003.safetensors:  12%|▌    | 601M/4.94G [00:16<01:31, 47.5MB/s]\u001b[A\u001b[A\n\n\nmodel-00002-of-00003.safetensors:  14%|▋    | 703M/5.00G [00:17<00:54, 79.0MB/s]\u001b[A\u001b[A\u001b[A\n\nmodel-00001-of-00003.safetensors:  14%|▋    | 668M/4.94G [00:17<01:15, 56.3MB/s]\u001b[A\u001b[A\nmodel-00003-of-00003.safetensors:  15%|▋    | 675M/4.54G [00:17<01:24, 45.9MB/s]\u001b[A\n\n\nmodel-00002-of-00003.safetensors:  16%|▉     | 794M/5.00G [00:17<00:41, 102MB/s]\u001b[A\u001b[A\u001b[A\nmodel-00003-of-00003.safetensors:  16%|▊    | 742M/4.54G [00:19<01:32, 41.1MB/s]\u001b[A\n\n\nmodel-00002-of-00003.safetensors:  17%|▊    | 845M/5.00G [00:19<01:11, 58.3MB/s]\u001b[A\u001b[A\u001b[A\nmodel-00003-of-00003.safetensors:  18%|▉    | 809M/4.54G [00:20<01:25, 43.8MB/s]\u001b[A\n\n\nmodel-00002-of-00003.safetensors:  18%|▉    | 897M/5.00G [00:20<01:12, 56.7MB/s]\u001b[A\u001b[A\u001b[A\n\nmodel-00001-of-00003.safetensors:  16%|▊    | 778M/4.94G [00:21<01:44, 40.0MB/s]\u001b[A\u001b[A\nmodel-00003-of-00003.safetensors:  21%|█    | 943M/4.54G [00:21<00:48, 74.4MB/s]\u001b[A\n\n\nmodel-00002-of-00003.safetensors:  18%|▉    | 911M/5.00G [00:22<01:40, 40.5MB/s]\u001b[A\u001b[A\u001b[A\n\nmodel-00001-of-00003.safetensors:  16%|▊    | 809M/4.94G [00:22<01:52, 36.8MB/s]\u001b[A\u001b[A\nmodel-00003-of-00003.safetensors:  22%|▉   | 1.01G/4.54G [00:23<00:57, 61.1MB/s]\u001b[A\n\n\nmodel-00002-of-00003.safetensors:  19%|▉    | 969M/5.00G [00:23<01:29, 44.9MB/s]\u001b[A\u001b[A\u001b[A\n\nmodel-00001-of-00003.safetensors:  18%|▉    | 876M/4.94G [00:23<01:36, 42.4MB/s]\u001b[A\u001b[A\n\n\nmodel-00002-of-00003.safetensors:  20%|▉    | 993M/5.00G [00:24<01:40, 39.8MB/s]\u001b[A\u001b[A\u001b[A\n\nmodel-00001-of-00003.safetensors:  19%|▉    | 943M/4.94G [00:24<01:29, 44.6MB/s]\u001b[A\u001b[A\n\n\nmodel-00002-of-00003.safetensors:  20%|▊   | 1.02G/5.00G [00:25<01:42, 38.9MB/s]\u001b[A\u001b[A\u001b[A\n\nmodel-00001-of-00003.safetensors:  20%|▊   | 1.01G/4.94G [00:25<01:15, 51.8MB/s]\u001b[A\u001b[A\n\n\nmodel-00002-of-00003.safetensors:  22%|▊   | 1.09G/5.00G [00:26<01:28, 44.2MB/s]\u001b[A\u001b[A\u001b[A\n\nmodel-00001-of-00003.safetensors:  23%|▉   | 1.14G/4.94G [00:26<00:48, 78.2MB/s]\u001b[A\u001b[A\n\nmodel-00001-of-00003.safetensors:  25%|▉   | 1.21G/4.94G [00:27<00:55, 67.0MB/s]\u001b[A\u001b[A\nmodel-00003-of-00003.safetensors:  24%|▉   | 1.08G/4.54G [00:28<01:50, 31.5MB/s]\u001b[A\n\nmodel-00001-of-00003.safetensors:  27%|█   | 1.35G/4.94G [00:28<00:42, 83.7MB/s]\u001b[A\u001b[A\n\n\nmodel-00002-of-00003.safetensors:  24%|▉   | 1.18G/5.00G [00:29<01:39, 38.4MB/s]\u001b[A\u001b[A\u001b[A\n\n\nmodel-00002-of-00003.safetensors:  24%|▉   | 1.22G/5.00G [00:29<01:20, 46.8MB/s]\u001b[A\u001b[A\u001b[A\nmodel-00003-of-00003.safetensors:  25%|█   | 1.14G/4.54G [00:30<01:47, 31.6MB/s]\u001b[A\n\n\nmodel-00002-of-00003.safetensors:  26%|█   | 1.28G/5.00G [00:30<01:11, 52.0MB/s]\u001b[A\u001b[A\u001b[A\n\nmodel-00001-of-00003.safetensors:  31%|█▎  | 1.55G/4.94G [00:31<00:40, 84.6MB/s]\u001b[A\u001b[A\nmodel-00003-of-00003.safetensors:  27%|█   | 1.21G/4.54G [00:32<01:45, 31.7MB/s]\u001b[A\n\n\nmodel-00002-of-00003.safetensors:  26%|█   | 1.29G/5.00G [00:32<02:03, 29.9MB/s]\u001b[A\u001b[A\u001b[A\n\nmodel-00001-of-00003.safetensors:  33%|█▎  | 1.62G/4.94G [00:32<00:44, 74.0MB/s]\u001b[A\u001b[A\nmodel-00003-of-00003.safetensors:  28%|█▏  | 1.28G/4.54G [00:32<01:22, 39.8MB/s]\u001b[A\n\nmodel-00001-of-00003.safetensors:  34%|█▎  | 1.68G/4.94G [00:33<00:43, 74.4MB/s]\u001b[A\u001b[A\n\nmodel-00001-of-00003.safetensors:  37%|█▍  | 1.82G/4.94G [00:34<00:35, 88.4MB/s]\u001b[A\u001b[A\nmodel-00003-of-00003.safetensors:  30%|█▏  | 1.34G/4.54G [00:35<01:27, 36.6MB/s]\u001b[A\n\n\nmodel-00002-of-00003.safetensors:  27%|█   | 1.34G/5.00G [00:35<02:49, 21.6MB/s]\u001b[A\u001b[A\u001b[A\nmodel-00003-of-00003.safetensors:  33%|█▎  | 1.48G/4.54G [00:36<00:58, 52.3MB/s]\u001b[A\n\nmodel-00001-of-00003.safetensors:  39%|█▌  | 1.95G/4.94G [00:36<00:39, 76.5MB/s]\u001b[A\u001b[A\n\nmodel-00001-of-00003.safetensors:  41%|█▋  | 2.02G/4.94G [00:37<00:39, 73.6MB/s]\u001b[A\u001b[A\n\n\nmodel-00002-of-00003.safetensors:  28%|█   | 1.38G/5.00G [00:37<02:51, 21.2MB/s]\u001b[A\u001b[A\u001b[A\nmodel-00003-of-00003.safetensors:  34%|█▎  | 1.55G/4.54G [00:38<01:07, 44.1MB/s]\u001b[A\n\nmodel-00001-of-00003.safetensors:  42%|█▋  | 2.09G/4.94G [00:38<00:38, 74.5MB/s]\u001b[A\u001b[A\n\n\nmodel-00002-of-00003.safetensors:  28%|█   | 1.40G/5.00G [00:38<02:48, 21.3MB/s]\u001b[A\u001b[A\u001b[A\nmodel-00003-of-00003.safetensors:  36%|█▍  | 1.61G/4.54G [00:39<00:54, 53.5MB/s]\u001b[A\n\nmodel-00001-of-00003.safetensors:  44%|█▋  | 2.15G/4.94G [00:39<00:31, 87.3MB/s]\u001b[A\u001b[A\n\n\nmodel-00002-of-00003.safetensors:  29%|█▏  | 1.44G/5.00G [00:39<02:14, 26.5MB/s]\u001b[A\u001b[A\u001b[A\n\nmodel-00001-of-00003.safetensors:  45%|█▊  | 2.22G/4.94G [00:40<00:33, 81.8MB/s]\u001b[A\u001b[A\nmodel-00003-of-00003.safetensors:  37%|█▍  | 1.68G/4.54G [00:40<00:54, 52.7MB/s]\u001b[A\nmodel-00003-of-00003.safetensors:  38%|█▌  | 1.75G/4.54G [00:41<00:52, 53.0MB/s]\u001b[A\n\n\nmodel-00002-of-00003.safetensors:  29%|█▏  | 1.46G/5.00G [00:42<03:24, 17.3MB/s]\u001b[A\u001b[A\u001b[A\n\nmodel-00001-of-00003.safetensors:  46%|█▊  | 2.29G/4.94G [00:43<01:02, 42.8MB/s]\u001b[A\u001b[A\n\n\nmodel-00002-of-00003.safetensors:  30%|█▏  | 1.51G/5.00G [00:43<02:35, 22.4MB/s]\u001b[A\u001b[A\u001b[A\n\nmodel-00001-of-00003.safetensors:  49%|█▉  | 2.42G/4.94G [00:44<00:37, 68.0MB/s]\u001b[A\u001b[A\n\n\nmodel-00002-of-00003.safetensors:  31%|█▏  | 1.55G/5.00G [00:44<02:12, 26.0MB/s]\u001b[A\u001b[A\u001b[A\nmodel-00003-of-00003.safetensors:  40%|█▌  | 1.81G/4.54G [00:45<01:17, 35.0MB/s]\u001b[A\n\n\nmodel-00002-of-00003.safetensors:  32%|█▎  | 1.62G/5.00G [00:46<01:46, 31.6MB/s]\u001b[A\u001b[A\u001b[A\nmodel-00003-of-00003.safetensors:  41%|█▋  | 1.88G/4.54G [00:46<01:10, 37.7MB/s]\u001b[A\n\n\nmodel-00002-of-00003.safetensors:  34%|█▎  | 1.68G/5.00G [00:47<01:21, 41.0MB/s]\u001b[A\u001b[A\u001b[A\n\n\nmodel-00002-of-00003.safetensors:  35%|█▍  | 1.75G/5.00G [00:47<00:53, 60.7MB/s]\u001b[A\u001b[A\u001b[A\nmodel-00003-of-00003.safetensors:  43%|█▋  | 1.95G/4.54G [00:47<01:01, 41.9MB/s]\u001b[A\n\nmodel-00001-of-00003.safetensors:  50%|██  | 2.49G/4.94G [00:47<01:00, 40.9MB/s]\u001b[A\u001b[A\n\n\nmodel-00002-of-00003.safetensors:  36%|█▍  | 1.80G/5.00G [00:47<00:50, 63.3MB/s]\u001b[A\u001b[A\u001b[A\n\nmodel-00001-of-00003.safetensors:  52%|██  | 2.55G/4.94G [00:48<00:45, 52.4MB/s]\u001b[A\u001b[A","output_type":"stream"}],"execution_count":null},{"cell_type":"markdown","source":"## rte","metadata":{}},{"cell_type":"code","source":"!python src/run_dect.py \\\n--model auto \\\n--model_name_or_path mistralai/Mistral-7B-Instruct-v0.2 \\\n--dataset rte \\\n--max_epochs 30 \\\n--batch_size 4 \\\n--proto_dim 256 \\\n--lr 1e-4 \\\n--seed 42\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-30T01:48:42.011675Z","iopub.execute_input":"2025-07-30T01:48:42.011844Z","iopub.status.idle":"2025-07-30T01:55:39.016437Z","shell.execute_reply.started":"2025-07-30T01:48:42.011831Z","shell.execute_reply":"2025-07-30T01:55:39.015368Z"}},"outputs":[{"name":"stdout","text":"DEBUG: Processor.labels for rte: ['not_entailment', 'entailment']\nUsing full test set with size: 277\ntokenizer_config.json: 100%|███████████████| 2.10k/2.10k [00:00<00:00, 15.5MB/s]\ntokenizer.model: 100%|███████████████████████| 493k/493k [00:00<00:00, 1.06MB/s]\nspecial_tokens_map.json: 100%|█████████████████| 414/414 [00:00<00:00, 2.73MB/s]\ntokenizer.json: 100%|██████████████████████| 1.80M/1.80M [00:00<00:00, 23.1MB/s]\nnormalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.\nconfig.json: 100%|█████████████████████████████| 596/596 [00:00<00:00, 5.13MB/s]\nmodel.safetensors.index.json: 100%|████████| 25.1k/25.1k [00:00<00:00, 86.2MB/s]\nFetching 3 files:   0%|                                   | 0/3 [00:00<?, ?it/s]\nmodel-00001-of-00003.safetensors:   0%|             | 0.00/4.94G [00:00<?, ?B/s]\u001b[A\n\nmodel-00003-of-00003.safetensors:   0%|             | 0.00/4.54G [00:00<?, ?B/s]\u001b[A\u001b[A\n\n\nmodel-00002-of-00003.safetensors:   0%|             | 0.00/5.00G [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n\nmodel-00003-of-00003.safetensors:   0%|    | 816k/4.54G [00:01<1:33:24, 810kB/s]\u001b[A\u001b[A\n\n\nmodel-00002-of-00003.safetensors:   0%|    | 754k/5.00G [00:01<2:09:38, 643kB/s]\u001b[A\u001b[A\u001b[A\n\n\nmodel-00002-of-00003.safetensors:   0%|    | 994k/5.00G [00:01<2:07:45, 652kB/s]\u001b[A\u001b[A\u001b[A\n\n\nmodel-00002-of-00003.safetensors:   0%|    | 2.34M/5.00G [00:01<41:02, 2.03MB/s]\u001b[A\u001b[A\u001b[A\n\n\nmodel-00002-of-00003.safetensors:   0%|    | 11.7M/5.00G [00:03<15:45, 5.27MB/s]\u001b[A\u001b[A\u001b[A\n\n\nmodel-00002-of-00003.safetensors:   0%|    | 21.8M/5.00G [00:03<09:36, 8.64MB/s]\u001b[A\u001b[A\u001b[A\n\n\nmodel-00002-of-00003.safetensors:   1%|    | 39.7M/5.00G [00:03<04:34, 18.0MB/s]\u001b[A\u001b[A\u001b[A\n\nmodel-00003-of-00003.safetensors:   0%|   | 2.38M/4.54G [00:04<2:12:57, 569kB/s]\u001b[A\u001b[A\n\n\nmodel-00002-of-00003.safetensors:   1%|    | 58.0M/5.00G [00:04<02:49, 29.2MB/s]\u001b[A\u001b[A\u001b[A\n\n\nmodel-00002-of-00003.safetensors:   1%|    | 73.3M/5.00G [00:04<03:10, 25.8MB/s]\u001b[A\u001b[A\u001b[A\nmodel-00001-of-00003.safetensors:   0%|   | 711k/4.94G [00:05<10:39:38, 129kB/s]\u001b[A\n\n\nmodel-00002-of-00003.safetensors:   2%|    | 85.2M/5.00G [00:05<04:15, 19.2MB/s]\u001b[A\u001b[A\u001b[A\nmodel-00001-of-00003.safetensors:   1%|    | 42.5M/4.94G [00:06<09:01, 9.05MB/s]\u001b[A\n\n\nmodel-00002-of-00003.safetensors:   2%|     | 103M/5.00G [00:07<05:01, 16.3MB/s]\u001b[A\u001b[A\u001b[A\n\n\nmodel-00002-of-00003.safetensors:   3%|▏    | 154M/5.00G [00:07<02:17, 35.2MB/s]\u001b[A\u001b[A\u001b[A\n\n\nmodel-00002-of-00003.safetensors:   4%|▏    | 202M/5.00G [00:08<02:19, 34.4MB/s]\u001b[A\u001b[A\u001b[A\n\n\nmodel-00002-of-00003.safetensors:   5%|▎    | 253M/5.00G [00:10<02:38, 30.0MB/s]\u001b[A\u001b[A\u001b[A\nmodel-00001-of-00003.safetensors:   2%|     | 109M/4.94G [00:11<06:53, 11.7MB/s]\u001b[A\n\n\nmodel-00002-of-00003.safetensors:   6%|▎    | 286M/5.00G [00:11<02:15, 34.7MB/s]\u001b[A\u001b[A\u001b[A\nmodel-00001-of-00003.safetensors:   6%|▎    | 304M/4.94G [00:11<01:53, 41.0MB/s]\u001b[A\n\n\nmodel-00002-of-00003.safetensors:   7%|▎    | 354M/5.00G [00:12<01:38, 47.1MB/s]\u001b[A\u001b[A\u001b[A\n\n\nmodel-00002-of-00003.safetensors:   8%|▍    | 420M/5.00G [00:12<01:13, 62.0MB/s]\u001b[A\u001b[A\u001b[A\nmodel-00001-of-00003.safetensors:   8%|▍    | 372M/4.94G [00:13<01:42, 44.8MB/s]\u001b[A\n\n\nmodel-00002-of-00003.safetensors:  10%|▍    | 492M/5.00G [00:13<00:52, 85.8MB/s]\u001b[A\u001b[A\u001b[A\nmodel-00001-of-00003.safetensors:   9%|▍    | 439M/4.94G [00:13<01:26, 51.9MB/s]\u001b[A\n\nmodel-00003-of-00003.safetensors:   0%|   | 4.59M/4.54G [00:14<2:12:53, 569kB/s]\u001b[A\u001b[A\n\nmodel-00003-of-00003.safetensors:   2%|    | 71.7M/4.54G [00:14<13:17, 5.60MB/s]\u001b[A\u001b[A\n\nmodel-00003-of-00003.safetensors:   2%|    | 74.4M/4.54G [00:14<13:38, 5.46MB/s]\u001b[A\u001b[A\n\n\nmodel-00002-of-00003.safetensors:  11%|▌    | 566M/5.00G [00:15<01:14, 59.3MB/s]\u001b[A\u001b[A\u001b[A\n\nmodel-00003-of-00003.safetensors:   5%|▏    | 208M/4.54G [00:15<03:10, 22.8MB/s]\u001b[A\u001b[A\n\n\nmodel-00002-of-00003.safetensors:  12%|▌    | 614M/5.00G [00:16<01:20, 54.2MB/s]\u001b[A\u001b[A\u001b[A\n\nmodel-00003-of-00003.safetensors:   8%|▍    | 342M/4.54G [00:16<01:45, 39.9MB/s]\u001b[A\u001b[A\n\n\nmodel-00002-of-00003.safetensors:  13%|▋    | 637M/5.00G [00:16<01:24, 51.8MB/s]\u001b[A\u001b[A\u001b[A\n\nmodel-00003-of-00003.safetensors:  10%|▌    | 476M/4.54G [00:17<01:05, 62.3MB/s]\u001b[A\u001b[A\n\n\nmodel-00002-of-00003.safetensors:  13%|▋    | 647M/5.00G [00:17<01:42, 42.6MB/s]\u001b[A\u001b[A\u001b[A\n\nmodel-00003-of-00003.safetensors:  13%|▋    | 610M/4.54G [00:17<00:47, 83.4MB/s]\u001b[A\u001b[A\n\n\nmodel-00002-of-00003.safetensors:  16%|▊    | 781M/5.00G [00:17<00:47, 88.1MB/s]\u001b[A\u001b[A\u001b[A\n\nmodel-00003-of-00003.safetensors:  15%|▋    | 677M/4.54G [00:18<00:40, 96.0MB/s]\u001b[A\u001b[A\nmodel-00001-of-00003.safetensors:  10%|▍    | 470M/4.94G [00:18<03:01, 24.6MB/s]\u001b[A\n\nmodel-00003-of-00003.safetensors:  18%|█     | 810M/4.54G [00:18<00:29, 126MB/s]\u001b[A\u001b[A\nmodel-00001-of-00003.safetensors:  11%|▌    | 539M/4.94G [00:19<02:24, 30.4MB/s]\u001b[A\n\nmodel-00003-of-00003.safetensors:  21%|█▏    | 944M/4.54G [00:19<00:29, 120MB/s]\u001b[A\u001b[A\n\n\nmodel-00002-of-00003.safetensors:  16%|▊    | 815M/5.00G [00:20<01:29, 46.7MB/s]\u001b[A\u001b[A\u001b[A\n\nmodel-00003-of-00003.safetensors:  22%|█    | 1.01G/4.54G [00:20<00:32, 108MB/s]\u001b[A\u001b[A\n\nmodel-00003-of-00003.safetensors:  24%|█▏   | 1.08G/4.54G [00:21<00:28, 124MB/s]\u001b[A\u001b[A\n\nmodel-00003-of-00003.safetensors:  25%|█▎   | 1.14G/4.54G [00:21<00:23, 144MB/s]\u001b[A\u001b[A\n\n\nmodel-00002-of-00003.safetensors:  17%|▊    | 857M/5.00G [00:21<01:35, 43.4MB/s]\u001b[A\u001b[A\u001b[A\n\nmodel-00003-of-00003.safetensors:  27%|█▎   | 1.21G/4.54G [00:22<00:26, 127MB/s]\u001b[A\u001b[A\n\nmodel-00003-of-00003.safetensors:  28%|█▍   | 1.28G/4.54G [00:22<00:21, 149MB/s]\u001b[A\u001b[A\n\n\nmodel-00002-of-00003.safetensors:  18%|▉    | 924M/5.00G [00:23<01:39, 40.8MB/s]\u001b[A\u001b[A\u001b[A\nmodel-00001-of-00003.safetensors:  12%|▌    | 606M/4.94G [00:23<02:52, 25.2MB/s]\u001b[A\n\n\nmodel-00002-of-00003.safetensors:  19%|▉    | 967M/5.00G [00:24<01:53, 35.5MB/s]\u001b[A\u001b[A\u001b[A\n\n\nmodel-00002-of-00003.safetensors:  21%|▊   | 1.04G/5.00G [00:25<01:26, 45.9MB/s]\u001b[A\u001b[A\u001b[A\n\n\nmodel-00002-of-00003.safetensors:  23%|▉   | 1.14G/5.00G [00:25<00:51, 74.7MB/s]\u001b[A\u001b[A\u001b[A\nmodel-00001-of-00003.safetensors:  14%|▋    | 673M/4.94G [00:26<02:51, 24.9MB/s]\u001b[A\n\nmodel-00003-of-00003.safetensors:  30%|█▏  | 1.35G/4.54G [00:26<01:15, 42.4MB/s]\u001b[A\u001b[A\n\n\nmodel-00002-of-00003.safetensors:  24%|▉   | 1.21G/5.00G [00:26<00:49, 76.1MB/s]\u001b[A\u001b[A\u001b[A\n\nmodel-00003-of-00003.safetensors:  31%|█▏  | 1.41G/4.54G [00:27<01:02, 50.3MB/s]\u001b[A\u001b[A\n\n\nmodel-00002-of-00003.safetensors:  25%|█   | 1.25G/5.00G [00:27<00:59, 63.5MB/s]\u001b[A\u001b[A\u001b[A\n\nmodel-00003-of-00003.safetensors:  34%|█▎  | 1.55G/4.54G [00:28<00:45, 66.0MB/s]\u001b[A\u001b[A\n\nmodel-00003-of-00003.safetensors:  36%|█▍  | 1.61G/4.54G [00:30<00:50, 57.7MB/s]\u001b[A\u001b[A\n\n\nmodel-00002-of-00003.safetensors:  26%|█   | 1.28G/5.00G [00:30<01:43, 35.9MB/s]\u001b[A\u001b[A\u001b[A\n\nmodel-00003-of-00003.safetensors:  37%|█▍  | 1.68G/4.54G [00:30<00:42, 67.3MB/s]\u001b[A\u001b[A\n\n\nmodel-00002-of-00003.safetensors:  27%|█   | 1.35G/5.00G [00:31<01:17, 46.9MB/s]\u001b[A\u001b[A\u001b[A\n\n\nmodel-00002-of-00003.safetensors:  28%|█   | 1.39G/5.00G [00:31<01:16, 46.9MB/s]\u001b[A\u001b[A\u001b[A\n\n\nmodel-00002-of-00003.safetensors:  30%|█▏  | 1.52G/5.00G [00:32<00:42, 81.7MB/s]\u001b[A\u001b[A\u001b[A\n\nmodel-00003-of-00003.safetensors:  39%|█▌  | 1.75G/4.54G [00:34<01:10, 39.3MB/s]\u001b[A\u001b[A\n\n\nmodel-00002-of-00003.safetensors:  33%|█▎  | 1.66G/5.00G [00:34<00:45, 73.7MB/s]\u001b[A\u001b[A\u001b[A\n\n\nmodel-00002-of-00003.safetensors:  34%|█▎  | 1.70G/5.00G [00:35<00:44, 74.5MB/s]\u001b[A\u001b[A\u001b[A\n\nmodel-00003-of-00003.safetensors:  40%|█▌  | 1.82G/4.54G [00:35<00:57, 47.2MB/s]\u001b[A\u001b[A\nmodel-00001-of-00003.safetensors:  15%|▋    | 740M/4.94G [00:35<04:50, 14.5MB/s]\u001b[A\n\n\nmodel-00002-of-00003.safetensors:  35%|█▍  | 1.74G/5.00G [00:35<00:42, 76.8MB/s]\u001b[A\u001b[A\u001b[A\n\nmodel-00003-of-00003.safetensors:  41%|█▋  | 1.88G/4.54G [00:36<00:48, 54.4MB/s]\u001b[A\u001b[A\n\nmodel-00003-of-00003.safetensors:  43%|█▋  | 1.95G/4.54G [00:36<00:38, 67.3MB/s]\u001b[A\u001b[A\n\nmodel-00003-of-00003.safetensors:  44%|█▊  | 2.02G/4.54G [00:37<00:35, 70.3MB/s]\u001b[A\u001b[A\n\n\nmodel-00002-of-00003.safetensors:  37%|█▍  | 1.83G/5.00G [00:37<00:47, 66.1MB/s]\u001b[A\u001b[A\u001b[A\n\nmodel-00003-of-00003.safetensors:  46%|█▊  | 2.08G/4.54G [00:37<00:31, 77.6MB/s]\u001b[A\u001b[A\n\n\nmodel-00002-of-00003.safetensors:  37%|█▍  | 1.87G/5.00G [00:38<00:59, 53.0MB/s]\u001b[A\u001b[A\u001b[A\n\nmodel-00003-of-00003.safetensors:  47%|█▉  | 2.15G/4.54G [00:38<00:32, 73.9MB/s]\u001b[A\u001b[A\n\nmodel-00003-of-00003.safetensors:  49%|█▉  | 2.22G/4.54G [00:40<00:36, 63.8MB/s]\u001b[A\u001b[A\n\n\nmodel-00002-of-00003.safetensors:  38%|█▌  | 1.92G/5.00G [00:40<01:15, 40.8MB/s]\u001b[A\u001b[A\u001b[A\n\nmodel-00003-of-00003.safetensors:  52%|██  | 2.35G/4.54G [00:41<00:26, 82.9MB/s]\u001b[A\u001b[A\n\n\nmodel-00002-of-00003.safetensors:  40%|█▌  | 1.99G/5.00G [00:41<00:59, 51.0MB/s]\u001b[A\u001b[A\u001b[A\n\nmodel-00003-of-00003.safetensors:  53%|██▏ | 2.42G/4.54G [00:42<00:28, 74.1MB/s]\u001b[A\u001b[A\n\nmodel-00003-of-00003.safetensors:  55%|██▏ | 2.49G/4.54G [00:43<00:26, 76.2MB/s]\u001b[A\u001b[A\nmodel-00001-of-00003.safetensors:  16%|▊    | 810M/4.94G [00:43<05:46, 11.9MB/s]\u001b[A\n\nmodel-00003-of-00003.safetensors:  56%|██▏ | 2.55G/4.54G [00:43<00:22, 87.3MB/s]\u001b[A\u001b[A\n\nmodel-00003-of-00003.safetensors:  58%|██▎ | 2.62G/4.54G [00:44<00:23, 81.4MB/s]\u001b[A\u001b[A\n\n\nmodel-00002-of-00003.safetensors:  41%|█▋  | 2.05G/5.00G [00:44<01:29, 33.0MB/s]\u001b[A\u001b[A\u001b[A\n\nmodel-00003-of-00003.safetensors:  59%|██▎ | 2.69G/4.54G [00:45<00:21, 87.2MB/s]\u001b[A\u001b[A\n\nmodel-00003-of-00003.safetensors:  61%|███  | 2.75G/4.54G [00:45<00:17, 104MB/s]\u001b[A\u001b[A\n\nmodel-00003-of-00003.safetensors:  62%|███  | 2.82G/4.54G [00:45<00:12, 133MB/s]\u001b[A\u001b[A\n\n\nmodel-00002-of-00003.safetensors:  41%|█▋  | 2.07G/5.00G [00:46<01:38, 29.7MB/s]\u001b[A\u001b[A\u001b[A\nmodel-00001-of-00003.safetensors:  18%|▉    | 877M/4.94G [00:46<04:54, 13.8MB/s]\u001b[A\n\n\nmodel-00002-of-00003.safetensors:  43%|█▋  | 2.14G/5.00G [00:46<01:08, 41.6MB/s]\u001b[A\u001b[A\u001b[A\n\nmodel-00003-of-00003.safetensors:  64%|██▌ | 2.89G/4.54G [00:47<00:16, 97.9MB/s]\u001b[A\u001b[A\n\n\nmodel-00002-of-00003.safetensors:  44%|█▊  | 2.21G/5.00G [00:47<00:51, 54.1MB/s]\u001b[A\u001b[A\u001b[A\n\n\nmodel-00002-of-00003.safetensors:  46%|█▊  | 2.28G/5.00G [00:47<00:38, 71.5MB/s]\u001b[A\u001b[A\u001b[A\n\n\nmodel-00002-of-00003.safetensors:  47%|█▊  | 2.33G/5.00G [00:48<00:40, 66.7MB/s]\u001b[A\u001b[A\u001b[A\n\nmodel-00003-of-00003.safetensors:  65%|██▌ | 2.96G/4.54G [00:49<00:27, 58.4MB/s]\u001b[A\u001b[A\n\n\nmodel-00002-of-00003.safetensors:  49%|█▉  | 2.45G/5.00G [00:49<00:36, 69.8MB/s]\u001b[A\u001b[A\u001b[A\n\nmodel-00003-of-00003.safetensors:  67%|██▋ | 3.02G/4.54G [00:49<00:22, 66.5MB/s]\u001b[A\u001b[A\n\n\nmodel-00002-of-00003.safetensors:  49%|█▉  | 2.46G/5.00G [00:50<00:38, 66.6MB/s]\u001b[A\u001b[A\u001b[A\nmodel-00001-of-00003.safetensors:  19%|▉    | 944M/4.94G [00:51<04:49, 13.8MB/s]\u001b[A\n\n\nmodel-00002-of-00003.safetensors:  50%|██  | 2.52G/5.00G [00:51<00:43, 57.0MB/s]\u001b[A\u001b[A\u001b[A\n\n\nmodel-00002-of-00003.safetensors:  52%|██  | 2.59G/5.00G [00:51<00:30, 79.6MB/s]\u001b[A\u001b[A\u001b[A\n\n\nmodel-00002-of-00003.safetensors:  53%|██▋  | 2.65G/5.00G [00:51<00:21, 107MB/s]\u001b[A\u001b[A\u001b[A\n\n\nmodel-00002-of-00003.safetensors:  54%|██▋  | 2.72G/5.00G [00:52<00:22, 100MB/s]\u001b[A\u001b[A\u001b[A\n\nmodel-00003-of-00003.safetensors:  68%|██▋ | 3.09G/4.54G [00:53<00:40, 35.4MB/s]\u001b[A\u001b[A\n\n\nmodel-00002-of-00003.safetensors:  55%|██▏ | 2.76G/5.00G [00:54<00:33, 66.8MB/s]\u001b[A\u001b[A\u001b[A\n\n\nmodel-00002-of-00003.safetensors:  56%|██▏ | 2.79G/5.00G [00:54<00:33, 65.5MB/s]\u001b[A\u001b[A\u001b[A\n\n\nmodel-00002-of-00003.safetensors:  57%|██▎ | 2.86G/5.00G [00:55<00:28, 75.1MB/s]\u001b[A\u001b[A\u001b[A\n\n\nmodel-00002-of-00003.safetensors:  58%|██▎ | 2.88G/5.00G [00:56<00:35, 59.0MB/s]\u001b[A\u001b[A\u001b[A\nmodel-00001-of-00003.safetensors:  20%|▊   | 1.01G/4.94G [00:56<04:56, 13.2MB/s]\u001b[A\nmodel-00001-of-00003.safetensors:  22%|▊   | 1.08G/4.94G [00:57<03:31, 18.3MB/s]\u001b[A\n\n\nmodel-00002-of-00003.safetensors:  58%|██▎ | 2.92G/5.00G [00:57<00:44, 47.2MB/s]\u001b[A\u001b[A\u001b[A\n\n\nmodel-00002-of-00003.safetensors:  59%|██▎ | 2.97G/5.00G [00:58<00:39, 51.8MB/s]\u001b[A\u001b[A\u001b[A\nmodel-00001-of-00003.safetensors:  22%|▉   | 1.10G/4.94G [00:58<03:41, 17.4MB/s]\u001b[A\n\nmodel-00003-of-00003.safetensors:  70%|██▊ | 3.16G/4.54G [00:58<00:58, 23.8MB/s]\u001b[A\u001b[A\n\n\nmodel-00002-of-00003.safetensors:  61%|██▍ | 3.03G/5.00G [00:59<00:37, 51.9MB/s]\u001b[A\u001b[A\u001b[A\nmodel-00001-of-00003.safetensors:  24%|▉   | 1.16G/4.94G [00:59<02:34, 24.4MB/s]\u001b[A\n\n\nmodel-00002-of-00003.safetensors:  62%|██▍ | 3.12G/5.00G [01:00<00:26, 70.3MB/s]\u001b[A\u001b[A\u001b[A\n\n\nmodel-00002-of-00003.safetensors:  64%|██▌ | 3.18G/5.00G [01:00<00:19, 91.8MB/s]\u001b[A\u001b[A\u001b[A\n\nmodel-00003-of-00003.safetensors:  71%|██▊ | 3.22G/4.54G [01:00<00:47, 27.8MB/s]\u001b[A\u001b[A\n\nmodel-00003-of-00003.safetensors:  72%|██▉ | 3.29G/4.54G [01:00<00:32, 38.6MB/s]\u001b[A\u001b[A\n\nmodel-00003-of-00003.safetensors:  74%|██▉ | 3.36G/4.54G [01:00<00:24, 49.3MB/s]\u001b[A\u001b[A\n\n\nmodel-00002-of-00003.safetensors:  65%|██▌ | 3.25G/5.00G [01:01<00:20, 84.9MB/s]\u001b[A\u001b[A\u001b[A\nmodel-00001-of-00003.safetensors:  25%|▉   | 1.23G/4.94G [01:01<02:14, 27.5MB/s]\u001b[A\n\n\nmodel-00002-of-00003.safetensors:  66%|██▋ | 3.29G/5.00G [01:01<00:20, 85.5MB/s]\u001b[A\u001b[A\u001b[A\n\n\nmodel-00002-of-00003.safetensors:  66%|██▋ | 3.31G/5.00G [01:02<00:22, 75.7MB/s]\u001b[A\u001b[A\u001b[A\n\n\nmodel-00002-of-00003.safetensors:  67%|███▎ | 3.37G/5.00G [01:02<00:14, 110MB/s]\u001b[A\u001b[A\u001b[A\nmodel-00001-of-00003.safetensors:  26%|█   | 1.30G/4.94G [01:02<01:53, 32.1MB/s]\u001b[A\n\n\nmodel-00002-of-00003.safetensors:  69%|███▍ | 3.44G/5.00G [01:02<00:15, 101MB/s]\u001b[A\u001b[A\u001b[A\n\n\nmodel-00002-of-00003.safetensors:  70%|██▊ | 3.49G/5.00G [01:04<00:21, 69.7MB/s]\u001b[A\u001b[A\u001b[A\n\nmodel-00003-of-00003.safetensors:  75%|███ | 3.42G/4.54G [01:04<00:33, 33.1MB/s]\u001b[A\u001b[A\n\n\nmodel-00002-of-00003.safetensors:  71%|██▊ | 3.55G/5.00G [01:04<00:20, 71.8MB/s]\u001b[A\u001b[A\u001b[A\n\n\nmodel-00002-of-00003.safetensors:  72%|██▉ | 3.61G/5.00G [01:05<00:16, 86.4MB/s]\u001b[A\u001b[A\u001b[A\nmodel-00001-of-00003.safetensors:  28%|█   | 1.36G/4.94G [01:07<02:29, 24.0MB/s]\u001b[A\n\nmodel-00003-of-00003.safetensors:  77%|███ | 3.49G/4.54G [01:07<00:37, 28.2MB/s]\u001b[A\u001b[A\n\n\nmodel-00002-of-00003.safetensors:  74%|██▉ | 3.70G/5.00G [01:08<00:25, 51.1MB/s]\u001b[A\u001b[A\u001b[A\nmodel-00001-of-00003.safetensors:  29%|█▏  | 1.43G/4.94G [01:08<02:03, 28.4MB/s]\u001b[A\n\nmodel-00003-of-00003.safetensors:  78%|███▏| 3.56G/4.54G [01:09<00:32, 30.5MB/s]\u001b[A\u001b[A\nmodel-00001-of-00003.safetensors:  30%|█▏  | 1.48G/4.94G [01:09<01:55, 30.0MB/s]\u001b[A\n\nmodel-00003-of-00003.safetensors:  81%|███▏| 3.67G/4.54G [01:10<00:18, 48.2MB/s]\u001b[A\u001b[A\n\n\nmodel-00002-of-00003.safetensors:  75%|███ | 3.77G/5.00G [01:10<00:27, 44.4MB/s]\u001b[A\u001b[A\u001b[A\n\n\nmodel-00002-of-00003.safetensors:  77%|███ | 3.84G/5.00G [01:10<00:19, 58.6MB/s]\u001b[A\u001b[A\u001b[A\n\nmodel-00003-of-00003.safetensors:  82%|███▎| 3.74G/4.54G [01:10<00:14, 54.7MB/s]\u001b[A\u001b[A\n\n\nmodel-00002-of-00003.safetensors:  77%|███ | 3.87G/5.00G [01:10<00:17, 63.5MB/s]\u001b[A\u001b[A\u001b[A\nmodel-00001-of-00003.safetensors:  31%|█▎  | 1.55G/4.94G [01:11<01:45, 32.3MB/s]\u001b[A\nmodel-00001-of-00003.safetensors:  33%|█▎  | 1.61G/4.94G [01:11<01:15, 43.9MB/s]\u001b[A\n\n\nmodel-00002-of-00003.safetensors:  78%|███▏| 3.91G/5.00G [01:11<00:20, 53.2MB/s]\u001b[A\u001b[A\u001b[A\n\nmodel-00003-of-00003.safetensors:  84%|███▎| 3.81G/4.54G [01:12<00:13, 54.8MB/s]\u001b[A\u001b[A\n\n\nmodel-00002-of-00003.safetensors:  79%|███▏| 3.94G/5.00G [01:12<00:20, 51.5MB/s]\u001b[A\u001b[A\u001b[A\n\n\nmodel-00002-of-00003.safetensors:  80%|███▏| 4.01G/5.00G [01:13<00:15, 63.8MB/s]\u001b[A\u001b[A\u001b[A\nmodel-00001-of-00003.safetensors:  34%|█▎  | 1.68G/4.94G [01:13<01:18, 41.6MB/s]\u001b[A\n\n\nmodel-00002-of-00003.safetensors:  81%|███▏| 4.05G/5.00G [01:14<00:14, 63.9MB/s]\u001b[A\u001b[A\u001b[A\n\n\nmodel-00002-of-00003.safetensors:  82%|███▎| 4.12G/5.00G [01:16<00:19, 46.1MB/s]\u001b[A\u001b[A\u001b[A\nmodel-00001-of-00003.safetensors:  35%|█▍  | 1.75G/4.94G [01:16<01:32, 34.6MB/s]\u001b[A\n\nmodel-00003-of-00003.safetensors:  85%|███▍| 3.87G/4.54G [01:17<00:23, 28.9MB/s]\u001b[A\u001b[A\nmodel-00001-of-00003.safetensors:  37%|█▍  | 1.82G/4.94G [01:17<01:22, 38.0MB/s]\u001b[A\n\n\nmodel-00002-of-00003.safetensors:  83%|███▎| 4.16G/5.00G [01:18<00:22, 36.5MB/s]\u001b[A\u001b[A\u001b[A\nmodel-00001-of-00003.safetensors:  38%|█▌  | 1.88G/4.94G [01:18<01:12, 42.1MB/s]\u001b[A\n\nmodel-00003-of-00003.safetensors:  87%|███▍| 3.94G/4.54G [01:19<00:20, 29.7MB/s]\u001b[A\u001b[A\nmodel-00001-of-00003.safetensors:  39%|█▌  | 1.95G/4.94G [01:19<00:56, 52.6MB/s]\u001b[A\n\n\nmodel-00002-of-00003.safetensors:  85%|███▍| 4.23G/5.00G [01:19<00:19, 40.2MB/s]\u001b[A\u001b[A\u001b[A\nmodel-00001-of-00003.safetensors:  41%|█▋  | 2.02G/4.94G [01:19<00:45, 64.2MB/s]\u001b[A\nmodel-00001-of-00003.safetensors:  42%|█▋  | 2.08G/4.94G [01:20<00:34, 83.6MB/s]\u001b[A\n\nmodel-00003-of-00003.safetensors:  88%|███▌| 4.01G/4.54G [01:20<00:15, 33.8MB/s]\u001b[A\u001b[A\n\nmodel-00003-of-00003.safetensors:  91%|███▋| 4.14G/4.54G [01:21<00:07, 53.9MB/s]\u001b[A\u001b[A\nmodel-00001-of-00003.safetensors:  44%|█▋  | 2.15G/4.94G [01:21<00:42, 65.0MB/s]\u001b[A\n\n\nmodel-00002-of-00003.safetensors:  85%|███▍| 4.27G/5.00G [01:21<00:23, 31.4MB/s]\u001b[A\u001b[A\u001b[A\n\nmodel-00003-of-00003.safetensors:  93%|███▋| 4.21G/4.54G [01:22<00:05, 56.0MB/s]\u001b[A\u001b[A\nmodel-00001-of-00003.safetensors:  45%|█▊  | 2.22G/4.94G [01:23<00:47, 57.9MB/s]\u001b[A\n\nmodel-00003-of-00003.safetensors:  94%|███▊| 4.27G/4.54G [01:23<00:05, 52.3MB/s]\u001b[A\u001b[A\nmodel-00001-of-00003.safetensors:  46%|█▊  | 2.28G/4.94G [01:24<00:42, 62.1MB/s]\u001b[A\nmodel-00001-of-00003.safetensors:  48%|█▉  | 2.35G/4.94G [01:25<00:40, 64.0MB/s]\u001b[A\nmodel-00001-of-00003.safetensors:  49%|█▉  | 2.41G/4.94G [01:26<00:39, 63.8MB/s]\u001b[A\nmodel-00001-of-00003.safetensors:  50%|██  | 2.48G/4.94G [01:27<00:37, 65.7MB/s]\u001b[A\n\n\nmodel-00002-of-00003.safetensors:  87%|███▍| 4.34G/5.00G [01:27<00:32, 20.2MB/s]\u001b[A\u001b[A\u001b[A\n\n\nmodel-00002-of-00003.safetensors:  88%|███▌| 4.41G/5.00G [01:28<00:22, 26.7MB/s]\u001b[A\u001b[A\u001b[A\nmodel-00001-of-00003.safetensors:  52%|██  | 2.55G/4.94G [01:29<00:48, 49.6MB/s]\u001b[A\nmodel-00001-of-00003.safetensors:  53%|██  | 2.62G/4.94G [01:29<00:35, 65.2MB/s]\u001b[A\n\nmodel-00003-of-00003.safetensors:  96%|███▊| 4.34G/4.54G [01:29<00:07, 26.8MB/s]\u001b[A\u001b[A\nmodel-00001-of-00003.safetensors:  54%|██▏ | 2.68G/4.94G [01:29<00:29, 76.0MB/s]\u001b[A\nmodel-00001-of-00003.safetensors:  56%|██▏ | 2.75G/4.94G [01:30<00:22, 96.6MB/s]\u001b[A\nmodel-00001-of-00003.safetensors:  57%|██▊  | 2.82G/4.94G [01:30<00:17, 125MB/s]\u001b[A\n\n\nmodel-00002-of-00003.safetensors:  89%|███▌| 4.47G/5.00G [01:30<00:19, 27.0MB/s]\u001b[A\u001b[A\u001b[A\nmodel-00001-of-00003.safetensors:  58%|██▉  | 2.88G/4.94G [01:30<00:16, 128MB/s]\u001b[A\nmodel-00001-of-00003.safetensors:  60%|██▉  | 2.95G/4.94G [01:31<00:13, 151MB/s]\u001b[A\nmodel-00001-of-00003.safetensors:  61%|███  | 3.02G/4.94G [01:31<00:10, 180MB/s]\u001b[A\nmodel-00001-of-00003.safetensors:  62%|███  | 3.08G/4.94G [01:31<00:09, 195MB/s]\u001b[A\nmodel-00001-of-00003.safetensors:  64%|███▏ | 3.15G/4.94G [01:32<00:09, 190MB/s]\u001b[A\nmodel-00001-of-00003.safetensors:  65%|███▎ | 3.22G/4.94G [01:32<00:10, 171MB/s]\u001b[A\n\n\nmodel-00002-of-00003.safetensors:  91%|███▋| 4.54G/5.00G [01:32<00:16, 28.0MB/s]\u001b[A\u001b[A\u001b[A\n\nmodel-00003-of-00003.safetensors:  97%|███▉| 4.41G/4.54G [01:33<00:05, 23.9MB/s]\u001b[A\u001b[A\n\nmodel-00003-of-00003.safetensors:  99%|███▉| 4.47G/4.54G [01:33<00:02, 30.7MB/s]\u001b[A\u001b[A\nmodel-00001-of-00003.safetensors:  66%|██▋ | 3.29G/4.94G [01:34<00:20, 79.7MB/s]\u001b[A\n\n\nmodel-00002-of-00003.safetensors:  92%|███▋| 4.61G/5.00G [01:34<00:13, 28.9MB/s]\u001b[A\u001b[A\u001b[A\n\nmodel-00003-of-00003.safetensors: 100%|████| 4.54G/4.54G [01:35<00:00, 47.5MB/s]\u001b[A\u001b[A\n\nmodel-00001-of-00003.safetensors:  68%|██▋ | 3.35G/4.94G [01:37<00:36, 43.3MB/s]\u001b[A\nmodel-00001-of-00003.safetensors:  69%|██▊ | 3.42G/4.94G [01:38<00:29, 52.5MB/s]\u001b[A\nmodel-00001-of-00003.safetensors:  71%|██▊ | 3.49G/4.94G [01:39<00:28, 51.2MB/s]\u001b[A\n\n\nmodel-00002-of-00003.safetensors:  93%|███▋| 4.66G/5.00G [01:39<00:15, 21.0MB/s]\u001b[A\u001b[A\u001b[A\n\n\nmodel-00002-of-00003.safetensors:  95%|███▊| 4.73G/5.00G [01:39<00:08, 29.9MB/s]\u001b[A\u001b[A\u001b[A\nmodel-00001-of-00003.safetensors:  72%|██▉ | 3.55G/4.94G [01:40<00:23, 59.9MB/s]\u001b[A\nmodel-00001-of-00003.safetensors:  73%|██▉ | 3.62G/4.94G [01:40<00:18, 72.3MB/s]\u001b[A\n\n\nmodel-00002-of-00003.safetensors:  96%|███▊| 4.80G/5.00G [01:40<00:05, 36.4MB/s]\u001b[A\u001b[A\u001b[A\nmodel-00001-of-00003.safetensors:  75%|██▉ | 3.69G/4.94G [01:40<00:12, 97.5MB/s]\u001b[A\n\n\nmodel-00002-of-00003.safetensors:  97%|███▉| 4.87G/5.00G [01:41<00:02, 47.2MB/s]\u001b[A\u001b[A\u001b[A\n\n\nmodel-00002-of-00003.safetensors:  99%|███▉| 4.93G/5.00G [01:41<00:01, 60.3MB/s]\u001b[A\u001b[A\u001b[A\nmodel-00001-of-00003.safetensors:  76%|███ | 3.76G/4.94G [01:41<00:12, 92.2MB/s]\u001b[A\n\n\nmodel-00002-of-00003.safetensors: 100%|████| 5.00G/5.00G [01:41<00:00, 49.1MB/s]\u001b[A\u001b[A\u001b[A\n\nmodel-00001-of-00003.safetensors:  77%|███▊ | 3.82G/4.94G [01:42<00:10, 103MB/s]\u001b[A\nmodel-00001-of-00003.safetensors:  79%|███▉ | 3.89G/4.94G [01:42<00:09, 114MB/s]\u001b[A\nmodel-00001-of-00003.safetensors:  81%|████ | 4.02G/4.94G [01:42<00:04, 191MB/s]\u001b[A\nmodel-00001-of-00003.safetensors:  83%|████▏| 4.09G/4.94G [01:43<00:04, 183MB/s]\u001b[A\nmodel-00001-of-00003.safetensors:  84%|████▏| 4.16G/4.94G [01:43<00:05, 157MB/s]\u001b[A\nmodel-00001-of-00003.safetensors:  87%|████▎| 4.29G/4.94G [01:43<00:02, 243MB/s]\u001b[A\nmodel-00001-of-00003.safetensors:  88%|████▍| 4.34G/4.94G [01:44<00:02, 239MB/s]\u001b[A\nmodel-00001-of-00003.safetensors:  89%|████▍| 4.41G/4.94G [01:45<00:05, 106MB/s]\u001b[A\nmodel-00001-of-00003.safetensors:  91%|███▌| 4.47G/4.94G [01:47<00:05, 80.6MB/s]\u001b[A\nmodel-00001-of-00003.safetensors:  92%|███▋| 4.54G/4.94G [01:48<00:05, 68.0MB/s]\u001b[A\nmodel-00001-of-00003.safetensors:  93%|███▋| 4.61G/4.94G [01:49<00:05, 63.3MB/s]\u001b[A\nmodel-00001-of-00003.safetensors:  95%|███▊| 4.67G/4.94G [01:50<00:04, 64.5MB/s]\u001b[A\nmodel-00001-of-00003.safetensors:  96%|███▊| 4.74G/4.94G [01:51<00:02, 76.9MB/s]\u001b[A\nmodel-00001-of-00003.safetensors:  97%|███▉| 4.81G/4.94G [01:51<00:01, 95.8MB/s]\u001b[A\nmodel-00001-of-00003.safetensors: 100%|████| 4.94G/4.94G [01:51<00:00, 44.3MB/s]\u001b[A\nFetching 3 files: 100%|███████████████████████████| 3/3 [01:51<00:00, 37.26s/it]\nThe following generation flags are not valid and may be ignored: ['output_hidden_states']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['output_hidden_states']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nLoading checkpoint shards: 100%|██████████████████| 3/3 [01:13<00:00, 24.46s/it]\ngeneration_config.json: 100%|███████████████████| 111/111 [00:00<00:00, 875kB/s]\ntokenizing: 2it [00:00, 175.30it/s]\ntokenizing: 277it [00:00, 1247.18it/s]\ntokenizing: 1it [00:00, 1104.35it/s]\nEvaluating: 100%|███████████████████████████████| 70/70 [02:40<00:00,  2.30s/it]\nDataset: rte | Shot: 1 | Acc: 48.01\n","output_type":"stream"}],"execution_count":60},{"cell_type":"markdown","source":"## yahoo","metadata":{}},{"cell_type":"code","source":"# For AGNews\n!python src/run_dect.py \\\n--model auto \\\n--model_name_or_path mistralai/Mistral-7B-Instruct-v0.2 \\\n--dataset yahoo \\\n--max_epochs 30 \\\n--batch_size 4 \\\n--proto_dim 256 \\\n--lr 1e-4 \\\n--seed 42\n\n# For other datasets, just change the --dataset parameter","metadata":{"trusted":true,"execution":{"execution_failed":"2025-07-30T05:22:53.141Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## yelp","metadata":{}},{"cell_type":"code","source":"!python src/run_dect.py \\\n--model auto \\\n--model_name_or_path mistralai/Mistral-7B-Instruct-v0.2 \\\n--dataset yelp \\\n--max_epochs 30 \\\n--batch_size 4 \\\n--proto_dim 256 \\\n--lr 1e-4 \\\n--seed 42\n\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-07-30T05:22:53.141Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## fewnerd","metadata":{}},{"cell_type":"code","source":"!python src/run_dect.py \\\n--model auto \\\n--model_name_or_path mistralai/Mistral-7B-Instruct-v0.2 \\\n--dataset fewnerd \\\n--max_epochs 30 \\\n--batch_size 4 \\\n--proto_dim 256 \\\n--lr 1e-4 \\\n--seed 42\n\n# For other datasets, just change the --dataset parameter","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}